{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/000300.SH/000300.SH.csv ===============\n",
      "Factors Shape: (2671, 10) (2671, 38)\n",
      "----- start:  0\n",
      "---- Trend Distribution Check --------\n",
      "0.0     960\n",
      "1.0     558\n",
      "2.0    1153\n",
      "dtype: int64\n",
      "input data shape:  (2671, 48)\n",
      "input label shape:  (2671,)\n",
      "------------- test -----------\n",
      "Training set size:  2124  Test set size:  544\n",
      "Batch Size:  20 Train Batch Num:  106 Test Batch Num:  27\n",
      "Train interval:  0 2124\n",
      "Test interval:  2120 2664\n",
      "Namespace(batch_size=20, data_dir='./input_data', dropout=0.9, fine_grained=False, input_dim=48, learning_rate=0.001, log_dir='./logs', max_steps=100, n_epoch=50, n_layers=3, n_neurons=150, no_retrace=False, retrace=0.618, time_steps=4, train_ratio=0.8, use_weight=0.0)\n",
      "label:  Tensor(\"input/y-input:0\", shape=(?,), dtype=int32)\n",
      "logits:  Tensor(\"fully_connected/BiasAdd:0\", shape=(?, 3), dtype=float32)\n",
      "---- Epoch  1\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.51415094431\n",
      "   BEST Test Accuracy: 0.8  AVERAGE Test Accuracy: 0.394444447011\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  2\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.563207547092\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.43148148391\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  3\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.57971698239\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.438888890876\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  4\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.605188679302\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.442592595462\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  5\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.589150945674\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.442592593255\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  6\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.609433962665\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.448148148479\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  7\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.602830189157\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.448148148479\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  8\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.614150943867\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.468518518739\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  9\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.607075469674\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.448148149031\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  10\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.625471697312\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.468518518739\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  11\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.616981130972\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.457407409394\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  12\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.6132075471\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.466666666446\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  13\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.635849055887\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.438888890324\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  14\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.650471699758\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.446296296738\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  15\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.638679245487\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.438888890324\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  16\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.6415094334\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.440740742479\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  17\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.646226415762\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.437037039134\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  18\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.637735848438\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.442592592703\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  19\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.636320754113\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.455555556273\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  20\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.640566039303\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.437037039134\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  21\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.66745283054\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.446296297842\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  22\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.654245284368\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.44629629729\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  23\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.673584903538\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.446296297842\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  24\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.668396225234\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.461111112877\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  25\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.662735848354\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.45370370398\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  26\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.67452830143\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.431481482806\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  27\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.66556603697\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.459259261688\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  28\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.682547168174\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.46481481636\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  29\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.68301886654\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.468518519015\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  30\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.691037735427\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.43888888922\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  31\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.686792452415\n",
      "   BEST Test Accuracy: 0.85  AVERAGE Test Accuracy: 0.438888888668\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  32\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.687264150571\n",
      "   BEST Test Accuracy: 0.85  AVERAGE Test Accuracy: 0.440740742617\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.693867923832\n",
      "   BEST Test Accuracy: 0.85  AVERAGE Test Accuracy: 0.450000000497\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  34\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.704245282478\n",
      "   BEST Test Accuracy: 0.85  AVERAGE Test Accuracy: 0.461111111084\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  35\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.712264151365\n",
      "   BEST Test Accuracy: 0.85  AVERAGE Test Accuracy: 0.459259259756\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  36\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.70141509467\n",
      "   BEST Test Accuracy: 0.85  AVERAGE Test Accuracy: 0.448148146548\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  37\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.705660378279\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.487037034498\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  38\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.708018869408\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.457407404151\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  39\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.716981133938\n",
      "   BEST Test Accuracy: 0.8  AVERAGE Test Accuracy: 0.444444442237\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  40\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.719811321816\n",
      "   BEST Test Accuracy: 0.8  AVERAGE Test Accuracy: 0.442592594083\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  41\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.719339622255\n",
      "   BEST Test Accuracy: 0.8  AVERAGE Test Accuracy: 0.448148148203\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  42\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.728773585468\n",
      "   BEST Test Accuracy: 0.8  AVERAGE Test Accuracy: 0.459259260032\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  43\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.729245282358\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.451851851686\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  44\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.727358491213\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.475925925153\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  45\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.723584906462\n",
      "   BEST Test Accuracy: 0.9  AVERAGE Test Accuracy: 0.446296297566\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  46\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.733018868058\n",
      "   BEST Test Accuracy: 0.8  AVERAGE Test Accuracy: 0.444444445548\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  47\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.72452830228\n",
      "   BEST Test Accuracy: 0.85  AVERAGE Test Accuracy: 0.440740742617\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  48\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.738207546326\n",
      "   BEST Test Accuracy: 0.85  AVERAGE Test Accuracy: 0.466666668653\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  49\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.728301887383\n",
      "   BEST Test Accuracy: 0.8  AVERAGE Test Accuracy: 0.446296296462\n",
      " ========= ========= ========= =========\n",
      "---- Epoch  50\n",
      " ========= EPOCH 19 STATISTICS============\n",
      "   BEST Train Accuracy: 1.0  AVERAGE Train Accuracy: 0.731132075472\n",
      "   BEST Test Accuracy: 0.85  AVERAGE Test Accuracy: 0.459259258652\n",
      " ========= ========= ========= =========\n",
      "-- AVG ROR:  4284.8696\n",
      "==== FINAL TRAINING ACC:  0.530890091742\n",
      "==== FINAL TESTING ACC:  0.466487318212\n",
      " ========= ========= ========= =========\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "#from data.dataset.BatchGenerator import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "'''\n",
    "读入一支股票指定年份的ohlcv数据\n",
    "输入:baseDir,stockCode为字符, startYear,yearNum为整数，\n",
    "输出:dataframe\n",
    "'''\n",
    "def readWSDFile(baseDir, stockCode, startYear, yearNum=1):\n",
    "    # 解析日期\n",
    "    filename = baseDir+stockCode+'/'+stockCode+'.csv'\n",
    "    print (filename, \"===============\")\n",
    "    dateparse = lambda x: pd.datetime.strptime(x, '%Y/%m/%d').date()\n",
    "    df = pd.read_csv(filename, index_col=0, sep=',', header=None,\n",
    "                            skiprows=1, names=['date','pre_close','open','high','low','close','change','chg_range',\n",
    "                                               'volume','amount','turn'],\n",
    "                           parse_dates=True, date_parser=dateparse)\n",
    "    return df['2005-01-04':'2015-12-31']\n",
    "\n",
    "'''\n",
    "读入一支股票指定年份的技术指标\n",
    "输入:baseDir,stockCode为字符, startYear,yearNum为整数，\n",
    "输出:dataframe\n",
    "'''\n",
    "def readWSDIndexFile(baseDir, stockCode, startYear, yearNum=1):\n",
    "    # 解析日期\n",
    "    dateparse = lambda x: pd.datetime.strptime(x, '%Y/%m/%d').date()\n",
    "\n",
    "    df = 0\n",
    "    for i in range(yearNum):\n",
    "        tempDF = pd.read_csv(baseDir+'I'+stockCode+'/wsd_'+stockCode+'_'+str(startYear+i)+'.csv', index_col=0, sep=',', parse_dates=True, date_parser=dateparse\n",
    "                             # , usecols=usecols\n",
    "                             )\n",
    "        if i==0: df = tempDF\n",
    "        else: df = df.append(tempDF)\n",
    "    return df\n",
    "\n",
    "# 整理好多因子输入，以dataframe返回数据+标签\n",
    "from sklearn import preprocessing\n",
    "def data_prepare(retrace = 0.618):\n",
    "    # prepare data\n",
    "    baseDir = '../data/'\n",
    "    stockCodes = ['000300.SH']\n",
    "    i = 0\n",
    "    startYear = 2005\n",
    "    number =11\n",
    "    df = readWSDFile(baseDir, stockCodes[i], startYear, number)\n",
    "    dfi = readWSDIndexFile(baseDir, stockCodes[i], startYear, number)\n",
    "    allDF = pd.concat([df, dfi], axis=1)\n",
    "    sample_num = np.shape(df)[0]\n",
    "    labelDF = pd.Series(np.zeros(sample_num))\n",
    "    print (\"Factors Shape:\", np.shape(df), np.shape(dfi))\n",
    "    \n",
    "    # 求出 trend\n",
    "    price = df['close']\n",
    "    start = 0\n",
    "    while price[start] > price[start+1]:\n",
    "        labelDF[start] = 1 #flat\n",
    "        start +=1\n",
    "    print(\"----- start: \",start)\n",
    "    #find peak, find trough, calculate retracement and label trend accordingly\n",
    "    i = start\n",
    "    while i < sample_num - 1:\n",
    "        cursor = i\n",
    "        while cursor < sample_num - 1 and price[cursor] <= price[cursor+1]:\n",
    "            cursor += 1\n",
    "        peak = cursor\n",
    "        while cursor < sample_num - 1 and price[cursor] >= price[cursor+1]:\n",
    "            cursor += 1\n",
    "        trough = cursor\n",
    "        retracement = (price[peak] - price[trough]) / (price[peak] - price[i])\n",
    "        mark = 1 # flat\n",
    "        if retracement < retrace:\n",
    "            mark = 2 # UP\n",
    "        elif retracement > 1 + retrace:\n",
    "            mark = 0 # DOWN\n",
    "        for k in range(i, cursor+1):\n",
    "            labelDF[k] = mark\n",
    "        i = cursor\n",
    "\n",
    "    print(\"---- Trend Distribution Check --------\")\n",
    "    print(labelDF.value_counts().sort_index())\n",
    "    \n",
    "    # make a deep copy of Price Difference before normalizing\n",
    "    priceDF = allDF['change'].copy(deep=True)\n",
    "    # normalize\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    input_data = scaler.fit_transform(allDF)\n",
    "    \n",
    "    print (\"input data shape: \", np.shape(allDF)) #  days *  factors\n",
    "    print (\"input label shape: \", np.shape(labelDF))\n",
    "   \n",
    "    return allDF, labelDF, priceDF # train/test data, labels and prices for yield calucluation\n",
    "\n",
    "def _data_prepare(file_name, retrace = 0.618, no_retrace = False, col=1):\n",
    "    print(col, \"===\")\n",
    "    dataset = pd.read_csv(file_name,index_col=0, sep=',', usecols=[0, col], skiprows=1, names=['date','close'],parse_dates=True)\n",
    "    print(file_name)\n",
    "    # calculate momentum: Mt = (CLOSE(t) -CLOSE(t-1))/CLOSE(t-1)\n",
    "    dataset['mmt'] = 0.0\n",
    "    dataset['diff'] = 0.0\n",
    "    for i in range(1, len(dataset)):\n",
    "        dataset['mmt'][i] = (dataset['close'][i] - dataset['close'][i-1]) / dataset['close'][i-1]\n",
    "        dataset['diff'][i-1] = dataset['close'][i] - dataset['close'][i-1]\n",
    "    \n",
    "    pd.set_option('mode.chained_assignment',None)\n",
    "    # classify by counts\n",
    "    dataset['label'] = 0\n",
    "    mmt_series = dataset['mmt']\n",
    "    for i in range(len(dataset)):\n",
    "        mmt = mmt_series[i]\n",
    "        if mmt < -0.01:\n",
    "            dataset['label'][i] = 0\n",
    "        elif mmt <= 0.01:\n",
    "            dataset['label'][i] = 1\n",
    "        else:\n",
    "            dataset['label'][i] = 2\n",
    "    dataset['trend']=0\n",
    "    price = dataset['close']\n",
    "    size = len(price)\n",
    "    start = 0\n",
    "    while price[start] > price[start+1]:\n",
    "        start +=1\n",
    "    print(\"----- start: \",start)\n",
    "    #find peak, find trough, calculate retracement and label trend accordingly\n",
    "    i = start\n",
    "    while i < size - 1:\n",
    "        cursor = i\n",
    "        while cursor < size - 1 and price[cursor] <= price[cursor+1]:\n",
    "            cursor += 1\n",
    "        peak = cursor\n",
    "        while cursor < size - 1 and price[cursor] >= price[cursor+1]:\n",
    "            cursor += 1\n",
    "        trough = cursor\n",
    "        retracement = (price[peak] - price[trough]) / (price[peak] - price[i])\n",
    "        mark = 1 # flat\n",
    "        if retracement < retrace:\n",
    "            mark = 2 # UP\n",
    "        elif retracement > 1 + retrace:\n",
    "            mark = 0 # DOWN\n",
    "        for k in range(i, cursor+1):\n",
    "            dataset['trend'][k] = mark\n",
    "        i = cursor\n",
    "\n",
    "    print(\"---- Trend Distribution Check --------\")\n",
    "    print(dataset['trend'].value_counts().sort_index())\n",
    "    print (time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) , \" ------ Complete Data Preparation\")\n",
    "    #dataset['label'].plot(lw=2.0)\n",
    "    #dataset.plot(lw=2.0)\n",
    "    #plt.show()\n",
    "    print(\"total data: \", len(dataset))\n",
    "    if no_retrace == True:\n",
    "        #print('whole data for SVM')\n",
    "        return dataset['label']\n",
    "    else:\n",
    "        return dataset #dataset['trend'] # only need 1 series => 为了计算收益率，需要每日收盘价\n",
    "\n",
    "###############\n",
    "# NOTICE:\n",
    "# We use rolling data here due to the small size of data, label is 1 step further into the future\n",
    "# eg:\n",
    "# batch0: X=[0,1,2,3] y=[4]\n",
    "# batch1: X=[1,2,3,4] y=[5]\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, file_name, batch_size, train_ratio, time_steps, input_size, column = 1, retrace = 0.618, fold_i=0, use_weight=False, no_retrace=False, fine_grained=False):\n",
    "        self.train_ratio = train_ratio\n",
    "        self.batch_size = batch_size\n",
    "        self.time_steps = time_steps\n",
    "        self.input_size = input_size\n",
    "        self.no_retrace = no_retrace\n",
    "        self.fine_grained = fine_grained\n",
    "        #self.dataset = _data_prepare(file_name, retrace, no_retrace=no_retrace, col = column)\n",
    "        self.dataset, self.labels, self.price_diff = data_prepare(retrace)\n",
    "         # 为计算收益率，后一天的价差，预测涨则加\n",
    "        self.segment_num = (len(self.dataset) - time_steps - 1) // batch_size # rollingly use data\n",
    "        self.train_size = int(self.segment_num * train_ratio)\n",
    "        self.test_size = self.segment_num - self.train_size\n",
    "        \n",
    "        self.train_dataset = self.dataset.iloc[0 : self.train_size * batch_size + time_steps]\n",
    "        self.test_dataset = self.dataset.iloc[self.train_size * batch_size : (self.train_size + self.test_size) * batch_size + time_steps]\n",
    "        self.train_labels = self.labels[0 : self.train_size * batch_size + time_steps]\n",
    "        self.test_labels = self.labels[self.train_size * batch_size : (self.train_size + self.test_size) * batch_size + time_steps]\n",
    "        self.train_diff = self.price_diff[0 : self.train_size * batch_size + time_steps]\n",
    "        self.test_diff = self.price_diff[self.train_size * batch_size : (self.train_size + self.test_size) * batch_size + time_steps]\n",
    "        \n",
    "        self.train_cursor = 0\n",
    "        self.test_cursor = 0\n",
    "        self.use_weight = use_weight\n",
    "\n",
    "        #print (self.train_dataset)\n",
    "        print (\"------------- test -----------\")\n",
    "        #print (self.test_dataset)\n",
    "        print(\"Training set size: \", len(self.train_dataset),\" Test set size: \", len(self.test_dataset))\n",
    "        print(\"Batch Size: \", batch_size, \"Train Batch Num: \", self.train_size,\"Test Batch Num: \", self.test_size)\n",
    "        print(\"Train interval: \",0, self.train_size * batch_size + time_steps)\n",
    "        print(\"Test interval: \",self.train_size * batch_size, (self.train_size + self.test_size) * batch_size + time_steps)\n",
    "\n",
    "    def reset(self):\n",
    "        self.train_cursor = 0\n",
    "        self.test_cursor = 0\n",
    "    def _next_batch(self, b_index, isTraining = True):\n",
    "        X_batch, y_batch, z_batch = [],[], []\n",
    "        if isTraining == True:\n",
    "            datasource = self.train_dataset\n",
    "            labels = self.train_labels\n",
    "            diff = self.train_diff\n",
    "        else:\n",
    "            datasource = self.test_dataset\n",
    "            labels = self.test_labels\n",
    "            diff = self.test_diff\n",
    "        for i in range(self.batch_size):\n",
    "            '''data_series_x = datasource['trend'].values[b_index + i : b_index + i + self.time_steps]\n",
    "            X_batch.append(weight_assign(data_series_x, self.use_weight, self.fine_grained))#time-weighted processing\n",
    "            y_batch.append(datasource['trend'].values[b_index + i + self.time_steps])\n",
    "            z_batch.append(datasource['diff'].values[b_index + i + self.time_steps - 1])'''\n",
    "            data_series_x = datasource.values[b_index + i : b_index + i + self.time_steps]\n",
    "            X_batch.append(data_series_x)\n",
    "            y_batch.append(labels.values[b_index + i + self.time_steps])\n",
    "            z_batch.append(diff.values[b_index + i + self.time_steps]) #涨跌幅 (应该往后推一天，即与预测的值的差值)\n",
    "            #input size changed to the total factors' number\n",
    "            self.input_size = np.shape(datasource)[1]\n",
    "        return np.array(X_batch).reshape(self.batch_size, self.time_steps, self.input_size),np.array(y_batch), np.array(z_batch)\n",
    "\n",
    "    def next_batch(self, is_training=True):\n",
    "        if is_training:\n",
    "            xs, ys, zs = self._next_batch(self.train_cursor * self.batch_size, is_training) #rolling use\n",
    "            self.train_cursor = (self.train_cursor + 1) % (self.train_size)\n",
    "        else:\n",
    "            xs, ys, zs = self._next_batch(self.test_cursor * self.batch_size, is_training)\n",
    "            self.test_cursor = (self.test_cursor + 1) % self.test_size\n",
    "        dic = {}\n",
    "        dic['X'] = xs\n",
    "        dic['y'] = ys\n",
    "        dic['z'] = zs\n",
    "        return dic\n",
    "\n",
    "\n",
    "# yapf: disable\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "FLAGS = None\n",
    "\n",
    "\"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "def variable_summaries(var):    \n",
    "    with tf.name_scope('summaries2'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "\n",
    "''' time recorder '''\n",
    "def elapsed(sec):\n",
    "    if sec<60:\n",
    "        return str(sec) + \" sec\"\n",
    "    elif sec<(60*60):\n",
    "        return str(sec/60) + \" min\"\n",
    "    else:\n",
    "        return str(sec/(60*60)) + \" hr\"\n",
    "\n",
    "\n",
    "############################\n",
    "###### contruction phase ###\n",
    "############################\n",
    "def train(dataset, FLAGS):\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # how long will a span cover, e.g. 20 days (4 tradable weeks)\n",
    "    TIME_SPAN = FLAGS.time_steps # 4weeks, 1month\n",
    "    TRAIN_RATIO = 0.9#0.8\n",
    "    BATCH_SIZE = FLAGS.batch_size\n",
    "    n_neurons = FLAGS.n_neurons #250\n",
    "    n_steps = TIME_SPAN\n",
    "    n_input = FLAGS.input_dim #5, 1 week's data as feature\n",
    "    n_output = 3#5 #5 class\n",
    "    n_layers = FLAGS.n_layers #5#10 #5 #3\n",
    "    learning_rate = FLAGS.learning_rate #0.0005 # # 0.02 # 0.005\n",
    "    print(FLAGS)\n",
    "    \n",
    "    with tf.name_scope('input'):\n",
    "        X = tf.placeholder(tf.float32, [None, n_steps, n_input], name='X-input')\n",
    "        y = tf.placeholder(tf.int32, [None], name='y-input') # every instance relates to 1 label\n",
    "        #z = tf.placeholder(tf.float32, [None], name='price-input')\n",
    "    \n",
    "    with tf.variable_scope(\"lstm_layers\", initializer=tf.contrib.layers.variance_scaling_initializer()):\n",
    "        # define network, 3 LSTM layer now, use tf.tanh as activation function, use peephole\n",
    "        #lstm_cell = tf.contrib.rnn.LSTMCell(num_units=n_neurons, use_peepholes=False)\n",
    "        #lstm_cell = tf.contrib.rnn.LSTMCell(num_units=n_neurons, activation=tf.nn.relu, use_peepholes=False)\n",
    "        stacked_cells = []\n",
    "        for i in range(n_layers):\n",
    "            stacked_cells.append(tf.contrib.rnn.LSTMCell(num_units=n_neurons))\n",
    "        cells = tf.contrib.rnn.MultiRNNCell(stacked_cells)\n",
    "        rnn_outputs, states = tf.nn.dynamic_rnn(cells, X, dtype=tf.float32)\n",
    "\n",
    "    tf.summary.histogram('lstm_outputs',rnn_outputs) # new_h, every output\n",
    "    #tf.summary.histogram('lstm_states', states)  # state is just the tuple(new_c, new_h)\n",
    "\n",
    "    states = states[-1][1] #retrieve the last layer's state tuple and only need last output/hypothesis states, omit memory cell\n",
    "    #print(\"Final STATES: \", states)\n",
    "    tf.summary.histogram('lstm_cell_states', states)\n",
    "    fc_layer = fully_connected(states, n_output, activation_fn=None)\n",
    "    print (\"label: \",y)\n",
    "    print (\"logits: \", fc_layer)\n",
    "    #print (\"price: \",z)\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=fc_layer)\n",
    "        #print(\"Cross-entropy\", xentropy)   #[batch_size, n_steps], float32\n",
    "        # define loss function & optimize method\n",
    "        with tf.name_scope('total_loss'):\n",
    "            loss = tf.reduce_mean(xentropy)\n",
    "            #print(\"LOSS:\", loss)\n",
    "    tf.summary.scalar('xentropy_mean', loss)\n",
    "\n",
    "    with tf.name_scope('training'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    # measurement\n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct = tf.nn.in_top_k(fc_layer, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    tf.summary.scalar('accuracy_mean',accuracy)\n",
    "\n",
    "    # rate of return, get all classes: if up or flat, add difference else add 0 \n",
    "    with tf.name_scope(\"ROR\"):\n",
    "        classes = tf.argmax(fc_layer, axis=1)\n",
    "        \n",
    "    #########################################\n",
    "    ############# Training Phase    #########\n",
    "    #########################################\n",
    "    n_epoch = FLAGS.n_epoch\n",
    "    \n",
    "    # yapf: enable\n",
    "    ######### Start Training ########################\n",
    "    start_time = time.time()\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        # Merge all the summaries and write them out to log dir\n",
    "        merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(FLAGS.log_dir + '/train', sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(FLAGS.log_dir + '/test')    \n",
    "    \n",
    "        avg_train, avg_test = [0], [0]\n",
    "        sum_train, sum_test =0., 0. #for all iterations\n",
    "        train_res, test_res = [], []\n",
    "        return_train = 0.\n",
    "        \n",
    "        init.run()\n",
    "        for j in range(n_epoch):\n",
    "            dataset.reset()\n",
    "            best_train_acc, sum_train_acc = 0., 0.\n",
    "            best_test_acc, sum_test_acc = 0., 0.\n",
    "            test_threshold = dataset.train_size\n",
    "            return_train_epoch = 0.\n",
    "            print(\"---- Epoch \",j+1)\n",
    "            for i in range(dataset.segment_num):    \n",
    "                ### Testing after training\n",
    "                if i > test_threshold:\n",
    "                    # Record summaries and test-set accuracy\n",
    "                    data = dataset.next_batch(is_training=False)\n",
    "                    summary, acc_test = sess.run([merged, accuracy], feed_dict={X: data['X'], y: data['y']})\n",
    "                    test_writer.add_summary(summary, i+j*FLAGS.max_steps)\n",
    "                    #print('Test Accuracy %s: %s' % (i, acc_test))\n",
    "                    if acc_test > best_test_acc:\n",
    "                        best_test_acc = acc_test\n",
    "                    sum_test_acc += acc_test\n",
    "                    sum_test += acc_test\n",
    "                    avg_test.append(sum_test/(len(avg_test)+1))\n",
    "                    #test_res.append(acc_test)\n",
    "                # Record train set summaries and train\n",
    "                if i % 100 == 99: # Record execution stats    \n",
    "                    data = dataset.next_batch()\n",
    "                    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                    run_metadata = tf.RunMetadata()\n",
    "                    summary, acc_train, _ = sess.run([merged, accuracy, training_op],\n",
    "                    #summary, _, acc, fp, fl, cr, out, st = sess.run([merged, training_op, accuracy,final_predict,final_label,correct,rnn_outputs, states],\n",
    "                                          feed_dict={X: data['X'], y: data['y']},\n",
    "                                          options=run_options,\n",
    "                                          run_metadata=run_metadata)\n",
    "                    #train_writer.add_run_metadata(run_metadata, 'step%03d' % (i+j*FLAGS.max_steps))\n",
    "                    train_writer.add_summary(summary, i+j*FLAGS.max_steps)\n",
    "                    #print('Adding run metadata for %d, Acc: %s' % (i, acc_train))\n",
    "                    sum_train_acc += acc_train\n",
    "                    # add average training accuracy\n",
    "                    sum_train += acc_train\n",
    "                    avg_train.append(sum_train/(len(avg_train)+1))\n",
    "                    #train_res.append(acc_train)\n",
    "                    \n",
    "                else:\n",
    "                    data = dataset.next_batch()\n",
    "                    summary, _, acc_train, clz = sess.run([merged, training_op, accuracy, classes],\n",
    "                                                          feed_dict={X: data['X'], y: data['y']})\n",
    "                    #print (\"classes: \", clz)\n",
    "                    z = data['z']\n",
    "                    for j in range(len(z)):\n",
    "                        if clz[j] > 0:\n",
    "                            return_train_epoch += z[j] #add difference\n",
    "                    \n",
    "                    train_writer.add_summary(summary, i+j*FLAGS.max_steps)\n",
    "                    if acc_train > best_train_acc:\n",
    "                        best_train_acc = acc_train\n",
    "                    sum_train_acc += acc_train\n",
    "                    # add average training accuracy\n",
    "                    sum_train += acc_train\n",
    "                    avg_train.append(sum_train/(len(avg_train)+1))\n",
    "                    #train_res.append(acc_train)\n",
    "            return_train += return_train_epoch   \n",
    "            print(\" ========= EPOCH %d STATISTICS============\" % (j))\n",
    "            print(\"   BEST Train Accuracy:\", best_train_acc, \" AVERAGE Train Accuracy:\", sum_train_acc/dataset.train_size)\n",
    "            print(\"   BEST Test Accuracy:\", best_test_acc, \" AVERAGE Test Accuracy:\", sum_test_acc/dataset.test_size)\n",
    "            print(\" ========= ========= ========= =========\")\n",
    "        train_writer.close()\n",
    "        test_writer.close()\n",
    "        #x_test = [i for i in range(0, FLAGS.max_steps*n_epoch, test_freq-1)]\n",
    "        size = len(avg_train)\n",
    "        \n",
    "        print(\"-- AVG ROR: \", return_train / n_epoch)\n",
    "        print(\"==== FINAL TRAINING ACC: \", avg_train[-1])\n",
    "        print(\"==== FINAL TESTING ACC: \", avg_test[-1])\n",
    "        print(\" ========= ========= ========= =========\")\n",
    "        #plt.title(\"Average Accuracy of the Model\", fontsize=34)\n",
    "        #plt.plot(avg_train, markersize=12, linewidth=3, label=\"Train\")\n",
    "        #plt.plot(x_test[: len(avg_test)], avg_test, markersize=12, linewidth=3, label=\"Test\")\n",
    "        #plt.legend(loc=\"upper left\")\n",
    "        #plt.xlabel(\"Steps\", fontsize=16)\n",
    "        #plt.ylabel(\"Average Accuracy\", fontsize=16)\n",
    "        #plt.show()\n",
    "        return avg_train[-1], avg_test[-1]\n",
    "        \n",
    "def main(_):\n",
    "    if tf.gfile.Exists(FLAGS.log_dir):\n",
    "        tf.gfile.DeleteRecursively(FLAGS.log_dir)\n",
    "    tf.gfile.MakeDirs(FLAGS.log_dir)\n",
    "    #filename = '../data/dataset/close_weekly-2007-2017.csv'\n",
    "    filename = '../data/dataset/close_2002-2017.csv'\n",
    "    #filename = '../data/dataset/Dow Jones Industrial Average.csv'\n",
    "    #filename = '../data/dataset/S&P 500.csv'\n",
    "    dataset = BatchGenerator(filename,  FLAGS.batch_size, FLAGS.train_ratio,FLAGS.time_steps, FLAGS.input_dim, column = 1, retrace = FLAGS.retrace, fold_i=0, use_weight=FLAGS.use_weight)\n",
    "    train(dataset, FLAGS)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--max_steps', type=int, default=100,\n",
    "                          help='Number of steps to run trainer.')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001,\n",
    "                          help='Initial learning rate')\n",
    "    parser.add_argument('--batch_size', type=int, default=20,\n",
    "                          help='number of instances in a batch')\n",
    "    parser.add_argument('--time_steps', type=int, default=4,\n",
    "                          help='Number of time steps.')\n",
    "    parser.add_argument('--input_dim', type=int, default=48, #dimension of factors\n",
    "                          help='Dimension of inputs.')\n",
    "    parser.add_argument('--n_neurons', type=int, default=150,\n",
    "                          help='Number of neurons.')\n",
    "    parser.add_argument('--n_layers', type=int, default=3,\n",
    "                          help='Number of lstm layers.')\n",
    "    parser.add_argument('--train_ratio', type=float, default=0.8,\n",
    "                          help='Keep probability for training dropout.')                      \n",
    "    parser.add_argument('--dropout', type=float, default=0.9,\n",
    "                          help='Keep probability for training dropout.')\n",
    "    parser.add_argument('--retrace', type=float, default=0.618,\n",
    "                          help='Retracement of wave.')\n",
    "    parser.add_argument('--use_weight', type=float, default=0.0,\n",
    "                          help='Whether use time-weighted function.')               \n",
    "    parser.add_argument('--n_epoch', type=int, default=50,\n",
    "                          help='Epoch Number.')\n",
    "    parser.add_argument('--no_retrace', type=bool, default=False,\n",
    "                          help='Whether use retracement.')\n",
    "    parser.add_argument('--fine_grained', type=bool, default=False,\n",
    "                          help='Use fine-grained weight function.')\n",
    "                          \n",
    "    parser.add_argument(\n",
    "          '--data_dir',\n",
    "          type=str,\n",
    "          default='./input_data',\n",
    "          help='Directory for storing input data')\n",
    "    parser.add_argument(\n",
    "          '--log_dir',\n",
    "          type=str,\n",
    "          default='./logs',\n",
    "          help='Summaries log directory')\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ===\n",
      "../data/dataset/close_weekly-2007-2017.csv\n",
      "----- start:  0\n",
      "---- Trend Distribution Check --------\n",
      "0    201\n",
      "1    123\n",
      "2    210\n",
      "Name: trend, dtype: int64\n",
      "2017-12-03 18:07:25  ------ Complete Data Preparation\n",
      "total data:  534\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>mmt</th>\n",
       "      <th>diff</th>\n",
       "      <th>label</th>\n",
       "      <th>trend</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-01-05</th>\n",
       "      <td>2678.526500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>77.233300</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-12</th>\n",
       "      <td>2755.759800</td>\n",
       "      <td>0.028834</td>\n",
       "      <td>41.001800</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-19</th>\n",
       "      <td>2796.761600</td>\n",
       "      <td>0.014879</td>\n",
       "      <td>122.715600</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-26</th>\n",
       "      <td>2919.477200</td>\n",
       "      <td>0.043878</td>\n",
       "      <td>-95.316400</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-02-02</th>\n",
       "      <td>2824.160800</td>\n",
       "      <td>-0.032648</td>\n",
       "      <td>-129.655400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-02-09</th>\n",
       "      <td>2694.505400</td>\n",
       "      <td>-0.045909</td>\n",
       "      <td>212.618800</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-02-16</th>\n",
       "      <td>2907.124200</td>\n",
       "      <td>0.078908</td>\n",
       "      <td>-42.688400</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-03-02</th>\n",
       "      <td>2864.435800</td>\n",
       "      <td>-0.014684</td>\n",
       "      <td>13.164200</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-03-09</th>\n",
       "      <td>2877.600000</td>\n",
       "      <td>0.004596</td>\n",
       "      <td>64.044200</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-03-16</th>\n",
       "      <td>2941.644200</td>\n",
       "      <td>0.022256</td>\n",
       "      <td>108.261400</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-03-23</th>\n",
       "      <td>3049.905600</td>\n",
       "      <td>0.036803</td>\n",
       "      <td>113.329400</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-03-30</th>\n",
       "      <td>3163.235000</td>\n",
       "      <td>0.037158</td>\n",
       "      <td>132.397200</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-04-06</th>\n",
       "      <td>3295.632200</td>\n",
       "      <td>0.041855</td>\n",
       "      <td>181.920000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-04-13</th>\n",
       "      <td>3477.552200</td>\n",
       "      <td>0.055200</td>\n",
       "      <td>93.233200</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-04-20</th>\n",
       "      <td>3570.785400</td>\n",
       "      <td>0.026810</td>\n",
       "      <td>172.876000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-04-27</th>\n",
       "      <td>3743.661400</td>\n",
       "      <td>0.048414</td>\n",
       "      <td>97.610600</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-04-30</th>\n",
       "      <td>3841.272000</td>\n",
       "      <td>0.026074</td>\n",
       "      <td>167.346750</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-05-11</th>\n",
       "      <td>4008.618750</td>\n",
       "      <td>0.043565</td>\n",
       "      <td>-6.585950</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-05-18</th>\n",
       "      <td>4002.032800</td>\n",
       "      <td>-0.001643</td>\n",
       "      <td>135.411600</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-05-25</th>\n",
       "      <td>4137.444400</td>\n",
       "      <td>0.033836</td>\n",
       "      <td>16.659400</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-06-01</th>\n",
       "      <td>4154.103800</td>\n",
       "      <td>0.004026</td>\n",
       "      <td>-350.552400</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-06-08</th>\n",
       "      <td>3803.551400</td>\n",
       "      <td>-0.084387</td>\n",
       "      <td>294.923800</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-06-15</th>\n",
       "      <td>4098.475200</td>\n",
       "      <td>0.077539</td>\n",
       "      <td>106.817400</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-06-22</th>\n",
       "      <td>4205.292600</td>\n",
       "      <td>0.026063</td>\n",
       "      <td>-259.701000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-06-29</th>\n",
       "      <td>3945.591600</td>\n",
       "      <td>-0.061756</td>\n",
       "      <td>-155.711000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-07-06</th>\n",
       "      <td>3789.880600</td>\n",
       "      <td>-0.039465</td>\n",
       "      <td>96.588600</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-07-13</th>\n",
       "      <td>3886.469200</td>\n",
       "      <td>0.025486</td>\n",
       "      <td>37.523000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-07-20</th>\n",
       "      <td>3923.992200</td>\n",
       "      <td>0.009655</td>\n",
       "      <td>363.902000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-07-27</th>\n",
       "      <td>4287.894200</td>\n",
       "      <td>0.092738</td>\n",
       "      <td>148.279400</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-08-03</th>\n",
       "      <td>4436.173600</td>\n",
       "      <td>0.034581</td>\n",
       "      <td>253.018200</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-18</th>\n",
       "      <td>3204.744600</td>\n",
       "      <td>0.015643</td>\n",
       "      <td>37.517600</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-25</th>\n",
       "      <td>3242.262200</td>\n",
       "      <td>0.011707</td>\n",
       "      <td>23.159800</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-02</th>\n",
       "      <td>3265.422000</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>-50.452600</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-09</th>\n",
       "      <td>3214.969400</td>\n",
       "      <td>-0.015451</td>\n",
       "      <td>-77.130000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-16</th>\n",
       "      <td>3137.839400</td>\n",
       "      <td>-0.023991</td>\n",
       "      <td>-16.218880</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-23</th>\n",
       "      <td>3121.620520</td>\n",
       "      <td>-0.005169</td>\n",
       "      <td>-13.779960</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30</th>\n",
       "      <td>3107.840560</td>\n",
       "      <td>-0.004414</td>\n",
       "      <td>45.771115</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-06</th>\n",
       "      <td>3153.611675</td>\n",
       "      <td>0.014728</td>\n",
       "      <td>-13.268875</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-13</th>\n",
       "      <td>3140.342800</td>\n",
       "      <td>-0.004208</td>\n",
       "      <td>-30.412200</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-20</th>\n",
       "      <td>3109.930600</td>\n",
       "      <td>-0.009684</td>\n",
       "      <td>37.081600</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-26</th>\n",
       "      <td>3147.012200</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>-6.842200</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-03</th>\n",
       "      <td>3140.170000</td>\n",
       "      <td>-0.002174</td>\n",
       "      <td>31.216360</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-10</th>\n",
       "      <td>3171.386360</td>\n",
       "      <td>0.009941</td>\n",
       "      <td>44.503060</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-17</th>\n",
       "      <td>3215.889420</td>\n",
       "      <td>0.014033</td>\n",
       "      <td>35.973200</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-24</th>\n",
       "      <td>3251.862620</td>\n",
       "      <td>0.011186</td>\n",
       "      <td>-18.729280</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-03</th>\n",
       "      <td>3233.133340</td>\n",
       "      <td>-0.005760</td>\n",
       "      <td>-3.844860</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-10</th>\n",
       "      <td>3229.288480</td>\n",
       "      <td>-0.001189</td>\n",
       "      <td>15.610400</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-17</th>\n",
       "      <td>3244.898880</td>\n",
       "      <td>0.004834</td>\n",
       "      <td>10.227800</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-24</th>\n",
       "      <td>3255.126680</td>\n",
       "      <td>0.003152</td>\n",
       "      <td>-16.332960</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-31</th>\n",
       "      <td>3238.793720</td>\n",
       "      <td>-0.005018</td>\n",
       "      <td>40.514980</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-07</th>\n",
       "      <td>3279.308700</td>\n",
       "      <td>0.012509</td>\n",
       "      <td>-8.465600</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-14</th>\n",
       "      <td>3270.843100</td>\n",
       "      <td>-0.002582</td>\n",
       "      <td>-83.879340</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-21</th>\n",
       "      <td>3186.963760</td>\n",
       "      <td>-0.025645</td>\n",
       "      <td>-44.605560</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-28</th>\n",
       "      <td>3142.358200</td>\n",
       "      <td>-0.013996</td>\n",
       "      <td>-14.992050</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-05</th>\n",
       "      <td>3127.366150</td>\n",
       "      <td>-0.004771</td>\n",
       "      <td>-55.978490</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-12</th>\n",
       "      <td>3071.387660</td>\n",
       "      <td>-0.017900</td>\n",
       "      <td>26.293240</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-19</th>\n",
       "      <td>3097.680900</td>\n",
       "      <td>0.008561</td>\n",
       "      <td>-13.763240</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-26</th>\n",
       "      <td>3083.917660</td>\n",
       "      <td>-0.004443</td>\n",
       "      <td>33.260140</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-02</th>\n",
       "      <td>3117.177800</td>\n",
       "      <td>0.010785</td>\n",
       "      <td>41.222600</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-09</th>\n",
       "      <td>3158.400400</td>\n",
       "      <td>0.013224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>534 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  close       mmt        diff  label  trend\n",
       "date                                                       \n",
       "2007-01-05  2678.526500  0.000000   77.233300      1      1\n",
       "2007-01-12  2755.759800  0.028834   41.001800      2      1\n",
       "2007-01-19  2796.761600  0.014879  122.715600      2      1\n",
       "2007-01-26  2919.477200  0.043878  -95.316400      2      1\n",
       "2007-02-02  2824.160800 -0.032648 -129.655400      0      1\n",
       "2007-02-09  2694.505400 -0.045909  212.618800      0      2\n",
       "2007-02-16  2907.124200  0.078908  -42.688400      2      2\n",
       "2007-03-02  2864.435800 -0.014684   13.164200      0      2\n",
       "2007-03-09  2877.600000  0.004596   64.044200      1      2\n",
       "2007-03-16  2941.644200  0.022256  108.261400      2      2\n",
       "2007-03-23  3049.905600  0.036803  113.329400      2      2\n",
       "2007-03-30  3163.235000  0.037158  132.397200      2      2\n",
       "2007-04-06  3295.632200  0.041855  181.920000      2      2\n",
       "2007-04-13  3477.552200  0.055200   93.233200      2      2\n",
       "2007-04-20  3570.785400  0.026810  172.876000      2      2\n",
       "2007-04-27  3743.661400  0.048414   97.610600      2      2\n",
       "2007-04-30  3841.272000  0.026074  167.346750      2      2\n",
       "2007-05-11  4008.618750  0.043565   -6.585950      2      2\n",
       "2007-05-18  4002.032800 -0.001643  135.411600      1      0\n",
       "2007-05-25  4137.444400  0.033836   16.659400      2      0\n",
       "2007-06-01  4154.103800  0.004026 -350.552400      1      0\n",
       "2007-06-08  3803.551400 -0.084387  294.923800      0      1\n",
       "2007-06-15  4098.475200  0.077539  106.817400      2      1\n",
       "2007-06-22  4205.292600  0.026063 -259.701000      2      1\n",
       "2007-06-29  3945.591600 -0.061756 -155.711000      0      1\n",
       "2007-07-06  3789.880600 -0.039465   96.588600      0      2\n",
       "2007-07-13  3886.469200  0.025486   37.523000      2      2\n",
       "2007-07-20  3923.992200  0.009655  363.902000      1      2\n",
       "2007-07-27  4287.894200  0.092738  148.279400      2      2\n",
       "2007-08-03  4436.173600  0.034581  253.018200      2      2\n",
       "...                 ...       ...         ...    ...    ...\n",
       "2016-11-18  3204.744600  0.015643   37.517600      2      1\n",
       "2016-11-25  3242.262200  0.011707   23.159800      2      1\n",
       "2016-12-02  3265.422000  0.007143  -50.452600      1      1\n",
       "2016-12-09  3214.969400 -0.015451  -77.130000      0      1\n",
       "2016-12-16  3137.839400 -0.023991  -16.218880      0      1\n",
       "2016-12-23  3121.620520 -0.005169  -13.779960      1      1\n",
       "2016-12-30  3107.840560 -0.004414   45.771115      1      1\n",
       "2017-01-06  3153.611675  0.014728  -13.268875      2      1\n",
       "2017-01-13  3140.342800 -0.004208  -30.412200      1      1\n",
       "2017-01-20  3109.930600 -0.009684   37.081600      1      2\n",
       "2017-01-26  3147.012200  0.011924   -6.842200      2      2\n",
       "2017-02-03  3140.170000 -0.002174   31.216360      1      2\n",
       "2017-02-10  3171.386360  0.009941   44.503060      1      2\n",
       "2017-02-17  3215.889420  0.014033   35.973200      2      2\n",
       "2017-02-24  3251.862620  0.011186  -18.729280      2      2\n",
       "2017-03-03  3233.133340 -0.005760   -3.844860      1      2\n",
       "2017-03-10  3229.288480 -0.001189   15.610400      1      1\n",
       "2017-03-17  3244.898880  0.004834   10.227800      1      1\n",
       "2017-03-24  3255.126680  0.003152  -16.332960      1      1\n",
       "2017-03-31  3238.793720 -0.005018   40.514980      1      0\n",
       "2017-04-07  3279.308700  0.012509   -8.465600      2      0\n",
       "2017-04-14  3270.843100 -0.002582  -83.879340      1      0\n",
       "2017-04-21  3186.963760 -0.025645  -44.605560      0      0\n",
       "2017-04-28  3142.358200 -0.013996  -14.992050      0      0\n",
       "2017-05-05  3127.366150 -0.004771  -55.978490      1      0\n",
       "2017-05-12  3071.387660 -0.017900   26.293240      0      2\n",
       "2017-05-19  3097.680900  0.008561  -13.763240      1      2\n",
       "2017-05-26  3083.917660 -0.004443   33.260140      1      2\n",
       "2017-06-02  3117.177800  0.010785   41.222600      2      2\n",
       "2017-06-09  3158.400400  0.013224    0.000000      2      2\n",
       "\n",
       "[534 rows x 5 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_data_prepare('../data/dataset/close_weekly-2007-2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/000300.SH/000300.SH.csv ===============\n",
      "Factors Shape: (2671, 10) (2671, 38)\n",
      "----- start:  0\n",
      "---- Trend Distribution Check --------\n",
      "0.0     960\n",
      "1.0     558\n",
      "2.0    1153\n",
      "dtype: int64\n",
      "input data shape:  (2671, 48)\n",
      "input label shape:  (2671,)\n",
      "------------- test -----------\n",
      "Training set size:  2384  Test set size:  284\n",
      "Batch Size:  20 Train Batch Num:  119 Test Batch Num:  14\n",
      "Train interval:  0 2384\n",
      "Test interval:  2380 2664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'X': array([[[  1.00000000e+03,   9.94770000e+02,   9.94770000e+02, ...,\n",
       "            0.00000000e+00,  -6.29077355e+06,   0.00000000e+00],\n",
       "         [  9.82790000e+02,   9.81580000e+02,   9.97320000e+02, ...,\n",
       "            0.00000000e+00,  -1.80735899e+06,   9.60371624e-01],\n",
       "         [  9.92560000e+02,   9.93330000e+02,   9.93790000e+02, ...,\n",
       "            0.00000000e+00,  -6.55305011e+06,   8.65405808e-01],\n",
       "         [  9.83170000e+02,   9.83040000e+02,   9.95710000e+02, ...,\n",
       "            0.00000000e+00,  -6.13392264e+06,   1.05168467e+00]],\n",
       " \n",
       "        [[  9.82790000e+02,   9.81580000e+02,   9.97320000e+02, ...,\n",
       "            0.00000000e+00,  -1.80735899e+06,   9.60371624e-01],\n",
       "         [  9.92560000e+02,   9.93330000e+02,   9.93790000e+02, ...,\n",
       "            0.00000000e+00,  -6.55305011e+06,   8.65405808e-01],\n",
       "         [  9.83170000e+02,   9.83040000e+02,   9.95710000e+02, ...,\n",
       "            0.00000000e+00,  -6.13392264e+06,   1.05168467e+00],\n",
       "         [  9.83960000e+02,   9.83760000e+02,   9.93960000e+02, ...,\n",
       "            0.00000000e+00,  -1.99798813e+06,   8.23892679e-01]],\n",
       " \n",
       "        [[  9.92560000e+02,   9.93330000e+02,   9.93790000e+02, ...,\n",
       "            0.00000000e+00,  -6.55305011e+06,   8.65405808e-01],\n",
       "         [  9.83170000e+02,   9.83040000e+02,   9.95710000e+02, ...,\n",
       "            0.00000000e+00,  -6.13392264e+06,   1.05168467e+00],\n",
       "         [  9.83960000e+02,   9.83760000e+02,   9.93960000e+02, ...,\n",
       "            0.00000000e+00,  -1.99798813e+06,   8.23892679e-01],\n",
       "         [  9.93880000e+02,   9.94190000e+02,   9.99550000e+02, ...,\n",
       "            0.00000000e+00,   3.83377534e+04,   8.62431606e-01]],\n",
       " \n",
       "        ..., \n",
       "        [[  9.89930000e+02,   9.87340000e+02,   9.87700000e+02, ...,\n",
       "            3.02775277e+06,  -1.59343411e+07,   6.42346518e-01],\n",
       "         [  9.74630000e+02,   9.74630000e+02,   9.75620000e+02, ...,\n",
       "            3.15707779e+06,  -1.89271432e+07,   5.39483036e-01],\n",
       "         [  9.69210000e+02,   9.65790000e+02,   9.65790000e+02, ...,\n",
       "            3.18692726e+06,  -2.47831609e+07,   7.61416520e-01],\n",
       "         [  9.54880000e+02,   9.53330000e+02,   9.65480000e+02, ...,\n",
       "            3.18120345e+06,  -2.32534759e+07,   1.00423240e+00]],\n",
       " \n",
       "        [[  9.74630000e+02,   9.74630000e+02,   9.75620000e+02, ...,\n",
       "            3.15707779e+06,  -1.89271432e+07,   5.39483036e-01],\n",
       "         [  9.69210000e+02,   9.65790000e+02,   9.65790000e+02, ...,\n",
       "            3.18692726e+06,  -2.47831609e+07,   7.61416520e-01],\n",
       "         [  9.54880000e+02,   9.53330000e+02,   9.65480000e+02, ...,\n",
       "            3.18120345e+06,  -2.32534759e+07,   1.00423240e+00],\n",
       "         [  9.55950000e+02,   9.56700000e+02,   1.00693000e+03, ...,\n",
       "            3.98857079e+06,  -6.20258935e+06,   2.46641347e+00]],\n",
       " \n",
       "        [[  9.69210000e+02,   9.65790000e+02,   9.65790000e+02, ...,\n",
       "            3.18692726e+06,  -2.47831609e+07,   7.61416520e-01],\n",
       "         [  9.54880000e+02,   9.53330000e+02,   9.65480000e+02, ...,\n",
       "            3.18120345e+06,  -2.32534759e+07,   1.00423240e+00],\n",
       "         [  9.55950000e+02,   9.56700000e+02,   1.00693000e+03, ...,\n",
       "            3.98857079e+06,  -6.20258935e+06,   2.46641347e+00],\n",
       "         [  1.00691000e+03,   1.00556000e+03,   1.01419000e+03, ...,\n",
       "            4.49367227e+06,  -1.57160975e+07,   1.93009684e+00]]]),\n",
       " 'y': array([ 2.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  2.,  2.,  2.,  2.,  2.]),\n",
       " 'z': array([  9.92,   3.26,  -0.39,   0.13,  -8.57, -20.85,   7.24,  -7.48,\n",
       "        -10.97,  26.36,  15.53,  -0.36,  -7.85, -15.3 ,  -5.42, -14.33,\n",
       "          1.07,  50.96, -13.7 ,  23.64])}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = BatchGenerator('../data/dataset/close_weekly-2007-2017.csv',batch_size=20, train_ratio=0.9, time_steps=4, input_size=1)\n",
    "dataset.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/000300.SH/000300.SH.csv ===============\n",
      "Factors Shape: (2671, 10) (2671, 38)\n",
      "----- start:  0\n",
      "input data shape:  (2671, 48)\n",
      "input label shape:  (2671,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(            pre_close     open     high      low    close  change  chg_range  \\\n",
       " 2005-01-04    1000.00   994.77   994.77   980.66   982.79  -17.21    -1.7206   \n",
       " 2005-01-05     982.79   981.58   997.32   979.88   992.56    9.77     0.9941   \n",
       " 2005-01-06     992.56   993.33   993.79   980.33   983.17   -9.39    -0.9460   \n",
       " 2005-01-07     983.17   983.04   995.71   979.81   983.96    0.78     0.0797   \n",
       " 2005-01-10     983.96   983.76   993.96   979.79   993.88    9.92     1.0083   \n",
       " 2005-01-11     993.88   994.19   999.55   991.09   997.14    3.26     0.3276   \n",
       " 2005-01-12     997.14   996.65   996.98   989.26   996.75   -0.39    -0.0388   \n",
       " 2005-01-13     996.75   996.08   999.47   992.70   996.88    0.13     0.0129   \n",
       " 2005-01-14     996.88   996.62  1006.46   987.23   988.31   -8.57    -0.8598   \n",
       " 2005-01-17     988.31   979.11   981.52   965.08   967.45  -20.85    -2.1101   \n",
       " 2005-01-18     967.45   967.37   974.87   960.29   974.69    7.24     0.7480   \n",
       " 2005-01-19     974.69   974.33   974.33   965.26   967.21   -7.48    -0.7673   \n",
       " 2005-01-20     967.21   963.21   963.21   952.23   956.25  -10.97    -1.1337   \n",
       " 2005-01-21     956.25   954.46   984.27   943.44   982.60   26.36     2.7565   \n",
       " 2005-01-24     982.60  1001.85  1001.85   986.24   998.13   15.53     1.5804   \n",
       " 2005-01-25     998.13   995.64   997.95   985.24   997.78   -0.36    -0.0359   \n",
       " 2005-01-26     997.78   995.78   999.47   988.48   989.93   -7.85    -0.7864   \n",
       " 2005-01-27     989.93   987.34   987.70   973.77   974.63  -15.30    -1.5453   \n",
       " 2005-01-28     974.63   974.63   975.62   965.21   969.21   -5.42    -0.5563   \n",
       " 2005-01-31     969.21   965.79   965.79   953.14   954.88  -14.33    -1.4785   \n",
       " 2005-02-01     954.88   953.33   965.48   952.74   955.95    1.07     0.1123   \n",
       " 2005-02-02     955.95   956.70  1006.93   956.70  1006.91   50.96     5.3310   \n",
       " 2005-02-03    1006.91  1005.56  1014.19   992.16   993.22  -13.70    -1.3604   \n",
       " 2005-02-04     993.22   992.25  1021.03   989.94  1016.86   23.64     2.3805   \n",
       " 2005-02-16    1016.86  1023.32  1033.25  1018.49  1023.58    6.73     0.6614   \n",
       " 2005-02-17    1023.58  1024.43  1024.43  1010.48  1020.61   -2.98    -0.2909   \n",
       " 2005-02-18    1020.61  1020.12  1021.20  1005.63  1006.06  -14.55    -1.4257   \n",
       " 2005-02-21    1006.06  1006.69  1025.66  1006.69  1025.63   19.58     1.9460   \n",
       " 2005-02-22    1025.63  1026.84  1048.68  1024.53  1046.74   21.11     2.0583   \n",
       " 2005-02-23    1046.74  1046.81  1049.61  1036.62  1043.94   -2.80    -0.2675   \n",
       " ...               ...      ...      ...      ...      ...     ...        ...   \n",
       " 2015-11-20    3774.97  3778.88  3793.55  3757.03  3774.38   -0.59    -0.0155   \n",
       " 2015-11-23    3774.38  3774.44  3802.85  3743.41  3753.34  -21.05    -0.5577   \n",
       " 2015-11-24    3753.34  3745.76  3754.17  3702.74  3753.89    0.55     0.0148   \n",
       " 2015-11-25    3753.89  3747.76  3781.86  3735.72  3781.61   27.72     0.7384   \n",
       " 2015-11-26    3781.61  3795.25  3805.84  3756.82  3759.43  -22.18    -0.5865   \n",
       " 2015-11-27    3759.43  3739.10  3742.74  3534.39  3556.99 -202.44    -5.3848   \n",
       " 2015-11-30    3556.99  3554.89  3587.97  3455.07  3566.41    9.42     0.2648   \n",
       " 2015-12-01    3566.41  3562.32  3617.31  3536.12  3591.70   25.28     0.7089   \n",
       " 2015-12-02    3591.70  3587.76  3725.85  3573.40  3721.96  130.26     3.6267   \n",
       " 2015-12-03    3721.96  3709.55  3758.45  3693.14  3749.30   27.34     0.7347   \n",
       " 2015-12-04    3749.30  3719.73  3726.12  3667.29  3677.59  -71.71    -1.9125   \n",
       " 2015-12-07    3677.59  3681.58  3699.34  3658.58  3687.61   10.02     0.2723   \n",
       " 2015-12-08    3687.61  3668.84  3668.84  3619.45  3623.02  -64.59    -1.7515   \n",
       " 2015-12-09    3623.02  3613.42  3655.16  3611.49  3635.94   12.92     0.3566   \n",
       " 2015-12-10    3635.94  3634.37  3678.32  3615.99  3623.08  -12.85    -0.3535   \n",
       " 2015-12-11    3623.08  3605.37  3630.74  3578.15  3608.06  -15.03    -0.4147   \n",
       " 2015-12-14    3608.06  3573.88  3712.50  3571.00  3711.32  103.26     2.8620   \n",
       " 2015-12-15    3711.32  3707.73  3717.36  3679.23  3694.39  -16.93    -0.4563   \n",
       " 2015-12-16    3694.39  3703.95  3713.09  3677.45  3685.44   -8.95    -0.2422   \n",
       " 2015-12-17    3685.44  3712.78  3768.84  3709.85  3755.89   70.45     1.9115   \n",
       " 2015-12-18    3755.89  3754.96  3810.22  3750.93  3767.91   12.03     0.3202   \n",
       " 2015-12-21    3767.91  3762.10  3877.82  3759.99  3865.97   98.05     2.6023   \n",
       " 2015-12-22    3865.97  3872.10  3880.47  3841.59  3876.73   10.77     0.2785   \n",
       " 2015-12-23    3876.73  3877.87  3926.69  3862.97  3866.38  -10.35    -0.2670   \n",
       " 2015-12-24    3866.38  3858.14  3867.68  3787.08  3829.40  -36.98    -0.9564   \n",
       " 2015-12-25    3829.40  3832.09  3848.03  3813.20  3838.20    8.80     0.2298   \n",
       " 2015-12-28    3838.20  3847.53  3853.39  3727.63  3727.63 -110.57    -2.8808   \n",
       " 2015-12-29    3727.63  3723.05  3762.05  3710.48  3761.88   34.24     0.9186   \n",
       " 2015-12-30    3761.88  3762.91  3765.66  3726.28  3765.18    3.30     0.0877   \n",
       " 2015-12-31    3765.18  3760.90  3772.62  3727.33  3731.01  -34.17    -0.9075   \n",
       " \n",
       "                   volume        amount    turn    ...             STD  \\\n",
       " 2005-01-04  7.412869e+06  4.431977e+09  0.0000    ...       22.812847   \n",
       " 2005-01-05  7.119109e+06  4.529208e+09  0.0000    ...       23.197167   \n",
       " 2005-01-06  6.288029e+06  3.921015e+09  0.0000    ...       23.777688   \n",
       " 2005-01-07  7.298694e+06  4.737469e+09  0.0000    ...       24.186179   \n",
       " 2005-01-10  5.791698e+06  3.762933e+09  0.0000    ...       23.877122   \n",
       " 2005-01-11  5.849080e+06  3.704077e+09  0.0000    ...       23.050237   \n",
       " 2005-01-12  5.014525e+06  3.093300e+09  0.0000    ...       21.807968   \n",
       " 2005-01-13  6.044066e+06  3.842173e+09  0.0000    ...       21.105061   \n",
       " 2005-01-14  7.297842e+06  4.162921e+09  0.0000    ...       20.427222   \n",
       " 2005-01-17  7.288189e+06  4.249808e+09  0.0000    ...       19.774578   \n",
       " 2005-01-18  7.311848e+06  4.117944e+09  0.0000    ...       19.446461   \n",
       " 2005-01-19  6.338091e+06  3.427951e+09  0.0000    ...       19.679296   \n",
       " 2005-01-20  7.727107e+06  4.399351e+09  0.0000    ...       20.559039   \n",
       " 2005-01-21  1.445006e+07  8.152086e+09  0.0000    ...       19.317575   \n",
       " 2005-01-24  1.435946e+07  8.360161e+09  0.0000    ...       17.951160   \n",
       " 2005-01-25  9.862529e+06  6.157022e+09  0.0000    ...       17.139325   \n",
       " 2005-01-26  7.663290e+06  4.719440e+09  0.0000    ...       16.928236   \n",
       " 2005-01-27  6.945365e+06  4.094399e+09  0.0000    ...       17.113617   \n",
       " 2005-01-28  5.748807e+06  3.280950e+09  0.0000    ...       15.638829   \n",
       " 2005-01-31  6.788706e+06  3.863574e+09  0.0000    ...       16.271270   \n",
       " 2005-02-01  7.433067e+06  4.275707e+09  0.0000    ...       16.416175   \n",
       " 2005-02-02  1.705734e+07  1.020290e+10  0.0000    ...       16.105350   \n",
       " 2005-02-03  1.697454e+07  1.005731e+10  0.0000    ...       15.402216   \n",
       " 2005-02-04  1.573621e+07  9.549871e+09  0.0000    ...       16.116489   \n",
       " 2005-02-16  1.213129e+07  7.438168e+09  0.0000    ...       17.369140   \n",
       " 2005-02-17  8.968298e+06  5.513063e+09  0.0000    ...       18.413310   \n",
       " 2005-02-18  8.139939e+06  4.817732e+09  0.0000    ...       18.743159   \n",
       " 2005-02-21  9.886094e+06  6.052395e+09  0.0000    ...       20.082367   \n",
       " 2005-02-22  1.793784e+07  1.092278e+10  0.0000    ...       22.865361   \n",
       " 2005-02-23  1.869693e+07  1.123194e+10  0.0000    ...       24.899684   \n",
       " ...                  ...           ...     ...    ...             ...   \n",
       " 2015-11-20  1.869517e+08  2.420000e+11  0.7808    ...      126.715591   \n",
       " 2015-11-23  1.756268e+08  2.350000e+11  0.7335    ...      126.026570   \n",
       " 2015-11-24  1.433833e+08  1.920000e+11  0.5988    ...      124.784606   \n",
       " 2015-11-25  1.519449e+08  2.150000e+11  0.6345    ...      125.351574   \n",
       " 2015-11-26  1.735255e+08  2.400000e+11  0.7246    ...      119.627958   \n",
       " 2015-11-27  2.089394e+08  2.700000e+11  0.8725    ...      118.100835   \n",
       " 2015-11-30  1.732012e+08  2.200000e+11  0.7233    ...      118.286054   \n",
       " 2015-12-01  1.365366e+08  1.770000e+11  0.5702    ...      118.209888   \n",
       " 2015-12-02  2.135389e+08  2.610000e+11  0.8917    ...      116.868351   \n",
       " 2015-12-03  1.977621e+08  2.370000e+11  0.8252    ...      112.437380   \n",
       " 2015-12-04  1.511995e+08  1.960000e+11  0.6309    ...      107.511498   \n",
       " 2015-12-07  1.138832e+08  1.610000e+11  0.4752    ...      101.989888   \n",
       " 2015-12-08  1.257811e+08  1.630000e+11  0.5249    ...       92.382334   \n",
       " 2015-12-09  1.133254e+08  1.540000e+11  0.4729    ...       79.410694   \n",
       " 2015-12-10  1.202156e+08  1.690000e+11  0.5016    ...       79.669235   \n",
       " 2015-12-11  1.103225e+08  1.450000e+11  0.4603    ...       82.677802   \n",
       " 2015-12-14  1.517691e+08  2.030000e+11  0.6213    ...       81.371846   \n",
       " 2015-12-15  1.264817e+08  1.870000e+11  0.5238    ...       77.603665   \n",
       " 2015-12-16  1.113427e+08  1.600000e+11  0.4611    ...       73.775739   \n",
       " 2015-12-17  1.779241e+08  2.430000e+11  0.7369    ...       70.001032   \n",
       " 2015-12-18  1.734660e+08  2.320000e+11  0.7184    ...       68.791605   \n",
       " 2015-12-21  2.174353e+08  2.900000e+11  0.9005    ...       75.329817   \n",
       " 2015-12-22  1.803037e+08  2.440000e+11  0.7467    ...       81.524570   \n",
       " 2015-12-23  2.070171e+08  2.950000e+11  0.8573    ...       86.464205   \n",
       " 2015-12-24  1.471814e+08  2.040000e+11  0.6095    ...       89.241282   \n",
       " 2015-12-25  1.196240e+08  1.630000e+11  0.4953    ...       91.566089   \n",
       " 2015-12-28  1.539884e+08  2.100000e+11  0.6375    ...       90.964683   \n",
       " 2015-12-29  1.018856e+08  1.400000e+11  0.4216    ...       91.101267   \n",
       " 2015-12-30  1.056300e+08  1.560000e+11  0.4365    ...       91.288827   \n",
       " 2015-12-31  1.018736e+08  1.450000e+11  0.4209    ...       90.498015   \n",
       " \n",
       "                     TAPI      TRIX       VHF           VMA         VMACD  \\\n",
       " 2005-01-04  3.566197e+06 -0.233165  0.493217  1.482574e+06  5.913400e+05   \n",
       " 2005-01-05  3.617760e+06 -0.241114  0.465351  2.906396e+06  1.024468e+06   \n",
       " 2005-01-06  3.163563e+06 -0.249905  0.388171  4.164001e+06  1.285841e+06   \n",
       " 2005-01-07  3.805973e+06 -0.257524  0.413976  5.623740e+06  1.556590e+06   \n",
       " 2005-01-10  3.004575e+06 -0.259565  0.383931  6.782080e+06  1.630760e+06   \n",
       " 2005-01-11  2.945677e+06 -0.256008  0.386629  6.469322e+06  1.674864e+06   \n",
       " 2005-01-12  2.461010e+06 -0.248488  0.389942  6.048405e+06  1.623758e+06   \n",
       " 2005-01-13  3.058291e+06 -0.238089  0.394196  5.999613e+06  1.647341e+06   \n",
       " 2005-01-14  3.342048e+06 -0.228788  0.377732  5.999442e+06  1.747061e+06   \n",
       " 2005-01-17  3.493035e+06 -0.227378  0.434472  6.298740e+06  1.804510e+06   \n",
       " 2005-01-18  3.360342e+06 -0.227491  0.422042  6.591294e+06  1.830843e+06   \n",
       " 2005-01-19  2.814160e+06 -0.230413  0.354595  6.856007e+06  1.752931e+06   \n",
       " 2005-01-20  3.652750e+06 -0.238009  0.388222  7.192615e+06  1.782718e+06   \n",
       " 2005-01-21  6.603681e+06 -0.237664  0.354110  8.623059e+06  2.322042e+06   \n",
       " 2005-01-24  6.657362e+06 -0.226043  0.336078  1.003731e+07  2.710901e+06   \n",
       " 2005-01-25  4.908990e+06 -0.207577  0.324547  1.054745e+07  2.625940e+06   \n",
       " 2005-01-26  3.800000e+06 -0.188188  0.320223  1.081249e+07  2.354011e+06   \n",
       " 2005-01-27  3.339937e+06 -0.174336  0.313738  1.065614e+07  2.056865e+06   \n",
       " 2005-01-28  2.703323e+06 -0.166085  0.324471  8.915891e+06  1.705166e+06   \n",
       " 2005-01-31  3.241734e+06 -0.166403  0.313092  7.401740e+06  1.493141e+06   \n",
       " 2005-02-01  3.596262e+06 -0.170946  0.269853  6.915847e+06  1.361411e+06   \n",
       " 2005-02-02  8.146031e+06 -0.158528  0.236238  8.794657e+06  2.010438e+06   \n",
       " 2005-02-03  8.095672e+06 -0.140997  0.210694  1.080049e+07  2.489418e+06   \n",
       " 2005-02-04  7.525485e+06 -0.112266  0.211597  1.279797e+07  2.737534e+06   \n",
       " 2005-02-16  5.816158e+06 -0.075648  0.230640  1.386649e+07  2.613158e+06   \n",
       " 2005-02-17  4.320129e+06 -0.037378  0.232349  1.417354e+07  2.233614e+06   \n",
       " 2005-02-18  3.826728e+06 -0.005769  0.222555  1.239006e+07  1.844717e+06   \n",
       " 2005-02-21  4.711942e+06  0.026995  0.218175  1.097237e+07  1.658297e+06   \n",
       " 2005-02-22  8.342602e+06  0.066012  0.279902  1.141269e+07  2.135648e+06   \n",
       " 2005-02-23  8.577438e+06  0.105009  0.285975  1.272582e+07  2.545857e+06   \n",
       " ...                  ...       ...       ...           ...           ...   \n",
       " 2015-11-20  6.673878e+07  0.400144  0.369099  1.874994e+08  1.231566e+06   \n",
       " 2015-11-23  6.499175e+07  0.379773  0.323544  1.902866e+08 -1.311661e+06   \n",
       " 2015-11-24  5.323261e+07  0.357408  0.347589  1.692567e+08 -5.861403e+06   \n",
       " 2015-11-25  5.897181e+07  0.336757  0.354000  1.625077e+08 -8.676238e+06   \n",
       " 2015-11-26  6.595895e+07  0.314993  0.346776  1.662864e+08 -9.061196e+06   \n",
       " 2015-11-27  7.850244e+07  0.272589  0.302330  1.706840e+08 -6.434499e+06   \n",
       " 2015-11-30  6.388169e+07  0.220531  0.327425  1.701988e+08 -7.154128e+06   \n",
       " 2015-12-01  5.109463e+07  0.167763  0.335032  1.688295e+08 -1.056122e+07   \n",
       " 2015-12-02  7.377599e+07  0.130271  0.311752  1.811483e+08 -6.967611e+06   \n",
       " 2015-12-03  6.602807e+07  0.106608  0.309354  1.859956e+08 -5.331242e+06   \n",
       " 2015-12-04  5.573221e+07  0.085311  0.292896  1.744476e+08 -7.702828e+06   \n",
       " 2015-12-07  4.558196e+07  0.067484  0.306785  1.625840e+08 -1.244993e+07   \n",
       " 2015-12-08  4.706454e+07  0.046264  0.293296  1.604329e+08 -1.507817e+07   \n",
       " 2015-12-09  4.444138e+07  0.025456  0.290535  1.403903e+08 -1.795911e+07   \n",
       " 2015-12-10  4.879827e+07  0.004859  0.301097  1.248810e+08 -1.946195e+07   \n",
       " 2015-12-11  4.229006e+07 -0.015804  0.226773  1.167056e+08 -2.120680e+07   \n",
       " 2015-12-14  5.774231e+07 -0.024944  0.238167  1.242827e+08 -1.902589e+07   \n",
       " 2015-12-15  5.321271e+07 -0.028105  0.250927  1.244229e+08 -1.911761e+07   \n",
       " 2015-12-16  4.553747e+07 -0.028221  0.269662  1.240263e+08 -2.017927e+07   \n",
       " 2015-12-17  6.774969e+07 -0.019419  0.257526  1.355680e+08 -1.546976e+07   \n",
       " 2015-12-18  6.490187e+07 -0.004508  0.256355  1.481967e+08 -1.195932e+07   \n",
       " 2015-12-21  7.959026e+07  0.022863  0.262547  1.613300e+08 -5.565154e+06   \n",
       " 2015-12-22  6.676148e+07  0.057004  0.278211  1.720943e+08 -3.454129e+06   \n",
       " 2015-12-23  8.105158e+07  0.091800  0.287914  1.912292e+08  3.701617e+05   \n",
       " 2015-12-24  5.638322e+07  0.120737  0.283047  1.850807e+08 -1.411040e+06   \n",
       " 2015-12-25  4.501609e+07  0.144663  0.282282  1.743123e+08 -4.988797e+06   \n",
       " 2015-12-28  5.943383e+07  0.152578  0.266348  1.616229e+08 -4.993710e+06   \n",
       " 2015-12-29  3.939828e+07  0.153124  0.272047  1.459393e+08 -9.097006e+06   \n",
       " 2015-12-30  4.359068e+07  0.148950  0.271420  1.256619e+08 -1.190947e+07   \n",
       " 2015-12-31  4.098227e+07  0.138301  0.268430  1.166003e+08 -1.427690e+07   \n",
       " \n",
       "                  VOSC          VSTD          WVAD  vol_ratio  \n",
       " 2005-01-04  53.846154  0.000000e+00 -6.290774e+06   0.000000  \n",
       " 2005-01-05  53.846154  0.000000e+00 -1.807359e+06   0.960372  \n",
       " 2005-01-06  53.846154  0.000000e+00 -6.553050e+06   0.865406  \n",
       " 2005-01-07  53.846154  0.000000e+00 -6.133923e+06   1.051685  \n",
       " 2005-01-10  53.846154  0.000000e+00 -1.997988e+06   0.823893  \n",
       " 2005-01-11  53.846154  0.000000e+00  3.833775e+04   0.862432  \n",
       " 2005-01-12  53.846154  0.000000e+00  1.013523e+05   0.775124  \n",
       " 2005-01-13  53.846154  0.000000e+00  8.138351e+05   0.999283  \n",
       " 2005-01-14  53.846154  0.000000e+00 -2.340214e+06   1.216386  \n",
       " 2005-01-17  53.846154  8.485827e+05 -7.506688e+06   1.214811  \n",
       " 2005-01-18  53.846154  8.375722e+05 -3.837975e+06   1.160843  \n",
       " 2005-01-19  53.846154  8.126095e+05 -8.812765e+06   0.961585  \n",
       " 2005-01-20  49.535459  9.027323e+05 -1.371511e+07   1.127056  \n",
       " 2005-01-21  46.110137  2.654367e+06 -3.756656e+06   2.009013  \n",
       " 2005-01-24  43.706693  3.390158e+06 -7.178465e+06   1.665240  \n",
       " 2005-01-25  40.512893  3.322007e+06 -5.519972e+06   0.982587  \n",
       " 2005-01-26  38.069951  3.105623e+06 -9.597213e+06   0.726554  \n",
       " 2005-01-27  35.550945  3.027753e+06 -1.593434e+07   0.642347  \n",
       " 2005-01-28  33.393255  3.157078e+06 -1.892714e+07   0.539483  \n",
       " 2005-01-31  30.802142  3.186927e+06 -2.478316e+07   0.761417  \n",
       " 2005-02-01  27.527821  3.181203e+06 -2.325348e+07   1.004232  \n",
       " 2005-02-02  26.818060  3.988571e+06 -6.202589e+06   2.466413  \n",
       " 2005-02-03  26.189244  4.493672e+06 -1.571610e+07   1.930097  \n",
       " 2005-02-04  25.939874  4.628415e+06 -3.259147e+06   1.456991  \n",
       " 2005-02-16  24.210462  4.492709e+06  3.245307e+06   0.947907  \n",
       " 2005-02-17  17.814313  4.518598e+06 -3.695809e+06   0.646760  \n",
       " 2005-02-18  13.401875  4.487231e+06 -6.305239e+06   0.574305  \n",
       " 2005-02-21  12.384111  4.314712e+06  3.149737e+06   0.797905  \n",
       " 2005-02-22  15.094668  4.422425e+06  1.379947e+07   1.634820  \n",
       " 2005-02-23  18.336600  4.434995e+06  7.633229e+06   1.638257  \n",
       " ...               ...           ...           ...        ...  \n",
       " 2015-11-20  13.996750  6.165229e+07  8.216669e+08   0.979794  \n",
       " 2015-11-23   7.502237  4.138565e+07  6.269017e+08   0.936679  \n",
       " 2015-11-24   3.034246  3.431956e+07  8.146404e+08   0.753512  \n",
       " 2015-11-25  -4.892793  3.295987e+07  7.774805e+08   0.897719  \n",
       " 2015-11-26  -7.953872  3.068805e+07  5.473668e+08   1.067798  \n",
       " 2015-11-27  -9.604782  3.122390e+07  4.519887e+08   1.256503  \n",
       " 2015-11-30 -11.312350  3.071925e+07  4.278572e+08   1.014748  \n",
       " 2015-12-01 -13.372445  2.232434e+07  5.847217e+08   0.802218  \n",
       " 2015-12-02 -11.102242  2.605678e+07  7.884263e+08   1.264820  \n",
       " 2015-12-03 -14.374304  2.646057e+07  9.018230e+08   1.091714  \n",
       " 2015-12-04 -16.822584  2.724139e+07  8.186027e+08   0.812920  \n",
       " 2015-12-07 -18.596385  3.288346e+07  8.751639e+08   0.652821  \n",
       " 2015-12-08 -22.188556  3.467426e+07  5.530635e+08   0.773637  \n",
       " 2015-12-09 -26.477965  3.821422e+07  4.172968e+08   0.706372  \n",
       " 2015-12-10 -25.944684  3.991744e+07  1.303193e+08   0.856296  \n",
       " 2015-12-11 -21.572303  3.732911e+07 -2.852916e+07   0.883421  \n",
       " 2015-12-14 -19.293765  3.616392e+07  2.086394e+07   1.300445  \n",
       " 2015-12-15 -18.836091  3.651506e+07 -4.296016e+07   1.017693  \n",
       " 2015-12-16 -18.397095  2.761895e+07  3.823575e+07   0.894873  \n",
       " 2015-12-17 -14.074603  2.265326e+07  1.988975e+08   1.434567  \n",
       " 2015-12-18 -15.590745  2.581736e+07  8.074442e+07   1.279549  \n",
       " 2015-12-21 -14.627803  3.621549e+07  3.500103e+08   1.467207  \n",
       " 2015-12-22 -13.189514  3.745067e+07  4.986582e+08   1.117608  \n",
       " 2015-12-23  -6.327104  3.940868e+07  3.353196e+08   1.202928  \n",
       " 2015-12-24  -4.120678  3.743756e+07  3.058910e+08   0.769660  \n",
       " 2015-12-25  -2.888196  3.615064e+07  3.892412e+08   0.646334  \n",
       " 2015-12-28  -0.226189  3.609270e+07  2.197649e+08   0.883405  \n",
       " 2015-12-29   1.142798  3.942658e+07  1.850054e+08   0.630390  \n",
       " 2015-12-30  -0.404015  4.022737e+07  3.178579e+08   0.723794  \n",
       " 2015-12-31  -0.492892  4.321365e+07  4.332240e+08   0.810696  \n",
       " \n",
       " [2671 rows x 48 columns], 0       1\n",
       " 1       1\n",
       " 2       2\n",
       " 3       2\n",
       " 4       2\n",
       " 5       2\n",
       " 6       0\n",
       " 7       0\n",
       " 8       0\n",
       " 9       0\n",
       " 10      0\n",
       " 11      0\n",
       " 12      1\n",
       " 13      1\n",
       " 14      1\n",
       " 15      1\n",
       " 16      1\n",
       " 17      1\n",
       " 18      1\n",
       " 19      2\n",
       " 20      2\n",
       " 21      2\n",
       " 22      2\n",
       " 23      2\n",
       " 24      2\n",
       " 25      2\n",
       " 26      2\n",
       " 27      2\n",
       " 28      2\n",
       " 29      0\n",
       "        ..\n",
       " 2641    2\n",
       " 2642    0\n",
       " 2643    0\n",
       " 2644    0\n",
       " 2645    0\n",
       " 2646    2\n",
       " 2647    2\n",
       " 2648    2\n",
       " 2649    2\n",
       " 2650    2\n",
       " 2651    0\n",
       " 2652    0\n",
       " 2653    0\n",
       " 2654    0\n",
       " 2655    0\n",
       " 2656    2\n",
       " 2657    2\n",
       " 2658    2\n",
       " 2659    2\n",
       " 2660    2\n",
       " 2661    2\n",
       " 2662    2\n",
       " 2663    2\n",
       " 2664    2\n",
       " 2665    0\n",
       " 2666    0\n",
       " 2667    1\n",
       " 2668    1\n",
       " 2669    1\n",
       " 2670    1\n",
       " Length: 2671, dtype: int32, 2005-01-04    -17.21\n",
       " 2005-01-05      9.77\n",
       " 2005-01-06     -9.39\n",
       " 2005-01-07      0.78\n",
       " 2005-01-10      9.92\n",
       " 2005-01-11      3.26\n",
       " 2005-01-12     -0.39\n",
       " 2005-01-13      0.13\n",
       " 2005-01-14     -8.57\n",
       " 2005-01-17    -20.85\n",
       " 2005-01-18      7.24\n",
       " 2005-01-19     -7.48\n",
       " 2005-01-20    -10.97\n",
       " 2005-01-21     26.36\n",
       " 2005-01-24     15.53\n",
       " 2005-01-25     -0.36\n",
       " 2005-01-26     -7.85\n",
       " 2005-01-27    -15.30\n",
       " 2005-01-28     -5.42\n",
       " 2005-01-31    -14.33\n",
       " 2005-02-01      1.07\n",
       " 2005-02-02     50.96\n",
       " 2005-02-03    -13.70\n",
       " 2005-02-04     23.64\n",
       " 2005-02-16      6.73\n",
       " 2005-02-17     -2.98\n",
       " 2005-02-18    -14.55\n",
       " 2005-02-21     19.58\n",
       " 2005-02-22     21.11\n",
       " 2005-02-23     -2.80\n",
       "                ...  \n",
       " 2015-11-20     -0.59\n",
       " 2015-11-23    -21.05\n",
       " 2015-11-24      0.55\n",
       " 2015-11-25     27.72\n",
       " 2015-11-26    -22.18\n",
       " 2015-11-27   -202.44\n",
       " 2015-11-30      9.42\n",
       " 2015-12-01     25.28\n",
       " 2015-12-02    130.26\n",
       " 2015-12-03     27.34\n",
       " 2015-12-04    -71.71\n",
       " 2015-12-07     10.02\n",
       " 2015-12-08    -64.59\n",
       " 2015-12-09     12.92\n",
       " 2015-12-10    -12.85\n",
       " 2015-12-11    -15.03\n",
       " 2015-12-14    103.26\n",
       " 2015-12-15    -16.93\n",
       " 2015-12-16     -8.95\n",
       " 2015-12-17     70.45\n",
       " 2015-12-18     12.03\n",
       " 2015-12-21     98.05\n",
       " 2015-12-22     10.77\n",
       " 2015-12-23    -10.35\n",
       " 2015-12-24    -36.98\n",
       " 2015-12-25      8.80\n",
       " 2015-12-28   -110.57\n",
       " 2015-12-29     34.24\n",
       " 2015-12-30      3.30\n",
       " 2015-12-31    -34.17\n",
       " Name: change, Length: 2671, dtype: float64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "def data_prepare(retrace = 0.618):\n",
    "    # prepare data\n",
    "    baseDir = '../data/'\n",
    "    stockCodes = ['000300.SH']\n",
    "    i = 0\n",
    "    startYear = 2005\n",
    "    number =11\n",
    "    df = readWSDFile(baseDir, stockCodes[i], startYear, number)\n",
    "    dfi = readWSDIndexFile(baseDir, stockCodes[i], startYear, number)\n",
    "    allDF = pd.concat([df, dfi], axis=1)\n",
    "    sample_num = np.shape(df)[0]\n",
    "    labelDF = pd.Series(np.zeros(sample_num), dtype=np.int32)  \n",
    "    print (\"Factors Shape:\", np.shape(df), np.shape(dfi))\n",
    "    \n",
    "    # 求出 trend\n",
    "    price = df['close']\n",
    "    start = 0\n",
    "    while price[start] > price[start+1]:\n",
    "        labelDF[start] = 1 #flat\n",
    "        start +=1\n",
    "    print(\"----- start: \",start)\n",
    "    #find peak, find trough, calculate retracement and label trend accordingly\n",
    "    i = start\n",
    "    while i < sample_num - 1:\n",
    "        cursor = i\n",
    "        while cursor < sample_num - 1 and price[cursor] <= price[cursor+1]:\n",
    "            cursor += 1\n",
    "        peak = cursor\n",
    "        while cursor < sample_num - 1 and price[cursor] >= price[cursor+1]:\n",
    "            cursor += 1\n",
    "        trough = cursor\n",
    "        retracement = (price[peak] - price[trough]) / (price[peak] - price[i])\n",
    "        mark = 1 # flat\n",
    "        if retracement < retrace:\n",
    "            mark = 2 # UP\n",
    "        elif retracement > 1 + retrace:\n",
    "            mark = 0 # DOWN\n",
    "        for k in range(i, cursor+1):\n",
    "            labelDF[k] = mark\n",
    "        i = cursor\n",
    "\n",
    "    # make a deep copy of Price Difference before normalizing\n",
    "    priceDF = allDF['change'].copy(deep=True)\n",
    "    # normalize\n",
    "    #scaler = preprocessing.MinMaxScaler()\n",
    "    #input_data = scaler.fit_transform(allDF)\n",
    "    \n",
    "    print (\"input data shape: \", np.shape(allDF)) #  days *  factors\n",
    "    print (\"input label shape: \", np.shape(labelDF))\n",
    "   \n",
    "    return allDF, labelDF, priceDF\n",
    "data_prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Pandas DEEP copy\n",
    "d = {'one' : pd.Series([1., 2., 3.], index=['a', 'b', 'c']),'two' : pd.Series([1., 2., 3., 4.], index=['a', 'b', 'c', 'd'])}\n",
    "s = d['one']\n",
    "s2 = d['one'].copy(deep=True)\n",
    "#print(s)\n",
    "d['one'][1] = 2.33333\n",
    "#print(s)\n",
    "#print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "日志\n",
    "12.04 LSTM 输入重塑，读入技术指标及行情数据，data_prepare单元测试通过\n",
    "12.05 next_batch输出符合预期, trend是timestep之后的标签，price_diff是timestep之后与未来一天的价差。预测准确度在65%，收益率40%\n",
    "    \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
