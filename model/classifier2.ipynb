{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/000300.SH/000300.SH.csv ===============\n",
      "Factors Shape: (2671, 10) (2671, 10)\n",
      "----- start:  0\n",
      "---- Trend Distribution Check --------\n",
      "0.0     960\n",
      "1.0     558\n",
      "2.0    1153\n",
      "dtype: int64\n",
      "input data shape:  (2671, 10)\n",
      "input label shape:  (2671,)\n",
      "training size:  2380\n",
      "testing size:  280\n",
      "Train on 2380 samples, validate on 280 samples\n",
      "Epoch 1/75\n",
      "2380/2380 [==============================] - 13s - loss: 1.1025 - acc: 0.4475 - val_loss: 1.0006 - val_acc: 0.6964\n",
      "Epoch 2/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.8288 - acc: 0.6655 - val_loss: 0.6034 - val_acc: 0.7714\n",
      "Epoch 3/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.6132 - acc: 0.8080 - val_loss: 0.5399 - val_acc: 0.8679\n",
      "Epoch 4/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.5562 - acc: 0.8408 - val_loss: 0.5123 - val_acc: 0.8679\n",
      "Epoch 5/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.5371 - acc: 0.8429 - val_loss: 0.5197 - val_acc: 0.8679\n",
      "Epoch 6/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.5307 - acc: 0.8420 - val_loss: 0.5191 - val_acc: 0.8393\n",
      "Epoch 7/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.5256 - acc: 0.8412 - val_loss: 0.5200 - val_acc: 0.8393\n",
      "Epoch 8/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.5214 - acc: 0.8416 - val_loss: 0.5167 - val_acc: 0.8429\n",
      "Epoch 9/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.5149 - acc: 0.8420 - val_loss: 0.5097 - val_acc: 0.8500\n",
      "Epoch 10/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.5175 - acc: 0.8391 - val_loss: 0.5005 - val_acc: 0.8607\n",
      "Epoch 11/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.5074 - acc: 0.8429 - val_loss: 0.5122 - val_acc: 0.8571\n",
      "Epoch 12/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.4979 - acc: 0.8445 - val_loss: 0.5031 - val_acc: 0.8607\n",
      "Epoch 13/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.4952 - acc: 0.8437 - val_loss: 0.4947 - val_acc: 0.8679\n",
      "Epoch 14/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.4808 - acc: 0.8441 - val_loss: 0.5100 - val_acc: 0.8643\n",
      "Epoch 15/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.4855 - acc: 0.8403 - val_loss: 0.4993 - val_acc: 0.8607\n",
      "Epoch 16/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.4696 - acc: 0.8424 - val_loss: 0.5196 - val_acc: 0.8500\n",
      "Epoch 17/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.4601 - acc: 0.8458 - val_loss: 0.4825 - val_acc: 0.8500\n",
      "Epoch 18/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.4446 - acc: 0.8529 - val_loss: 0.5039 - val_acc: 0.8500\n",
      "Epoch 19/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.4361 - acc: 0.8500 - val_loss: 0.5304 - val_acc: 0.8286\n",
      "Epoch 20/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.4432 - acc: 0.8416 - val_loss: 0.5046 - val_acc: 0.8536\n",
      "Epoch 21/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.3974 - acc: 0.8546 - val_loss: 0.5234 - val_acc: 0.8571\n",
      "Epoch 22/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.3796 - acc: 0.8580 - val_loss: 0.5473 - val_acc: 0.8393\n",
      "Epoch 23/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.3883 - acc: 0.8538 - val_loss: 0.5592 - val_acc: 0.8429\n",
      "Epoch 24/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.3632 - acc: 0.8668 - val_loss: 0.5672 - val_acc: 0.8429\n",
      "Epoch 25/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.3295 - acc: 0.8807 - val_loss: 0.6072 - val_acc: 0.8357\n",
      "Epoch 26/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.3067 - acc: 0.8882 - val_loss: 0.5535 - val_acc: 0.8286\n",
      "Epoch 27/75\n",
      "2380/2380 [==============================] - 13s - loss: 0.3191 - acc: 0.8773 - val_loss: 0.5737 - val_acc: 0.8393\n",
      "Epoch 28/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.2819 - acc: 0.8941 - val_loss: 0.6065 - val_acc: 0.8143\n",
      "Epoch 29/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.2665 - acc: 0.8979 - val_loss: 0.6059 - val_acc: 0.8429\n",
      "Epoch 30/75\n",
      "2380/2380 [==============================] - 13s - loss: 0.2519 - acc: 0.9034 - val_loss: 0.6529 - val_acc: 0.8286\n",
      "Epoch 31/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.2719 - acc: 0.9059 - val_loss: 0.6036 - val_acc: 0.8286\n",
      "Epoch 32/75\n",
      "2380/2380 [==============================] - 13s - loss: 0.2164 - acc: 0.9223 - val_loss: 0.6848 - val_acc: 0.8107\n",
      "Epoch 33/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.2113 - acc: 0.9256 - val_loss: 0.7909 - val_acc: 0.7893\n",
      "Epoch 34/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.2323 - acc: 0.9193 - val_loss: 0.7694 - val_acc: 0.7429\n",
      "Epoch 35/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.1960 - acc: 0.9307 - val_loss: 0.7509 - val_acc: 0.8036\n",
      "Epoch 36/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.2029 - acc: 0.9277 - val_loss: 0.8012 - val_acc: 0.7500\n",
      "Epoch 37/75\n",
      "2380/2380 [==============================] - 13s - loss: 0.2004 - acc: 0.9252 - val_loss: 0.7546 - val_acc: 0.7750\n",
      "Epoch 38/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.1693 - acc: 0.9391 - val_loss: 0.8656 - val_acc: 0.7714\n",
      "Epoch 39/75\n",
      "2380/2380 [==============================] - 13s - loss: 0.1736 - acc: 0.9332 - val_loss: 0.7646 - val_acc: 0.7893\n",
      "Epoch 40/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.1773 - acc: 0.9387 - val_loss: 0.8718 - val_acc: 0.7750\n",
      "Epoch 41/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.1528 - acc: 0.9462 - val_loss: 0.8824 - val_acc: 0.8000\n",
      "Epoch 42/75\n",
      "2380/2380 [==============================] - 13s - loss: 0.1472 - acc: 0.9500 - val_loss: 0.9286 - val_acc: 0.7750\n",
      "Epoch 43/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.1701 - acc: 0.9420 - val_loss: 0.8622 - val_acc: 0.7607\n",
      "Epoch 44/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.1427 - acc: 0.9529 - val_loss: 0.9061 - val_acc: 0.7571\n",
      "Epoch 45/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.1453 - acc: 0.9471 - val_loss: 0.9469 - val_acc: 0.7643\n",
      "Epoch 46/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.1549 - acc: 0.9437 - val_loss: 0.9232 - val_acc: 0.7964\n",
      "Epoch 47/75\n",
      "2380/2380 [==============================] - 13s - loss: 0.1557 - acc: 0.9466 - val_loss: 0.8804 - val_acc: 0.7571\n",
      "Epoch 48/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.1438 - acc: 0.9508 - val_loss: 0.8508 - val_acc: 0.8036\n",
      "Epoch 49/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.1285 - acc: 0.9559 - val_loss: 0.8867 - val_acc: 0.8071\n",
      "Epoch 50/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.1148 - acc: 0.9597 - val_loss: 1.0274 - val_acc: 0.8036\n",
      "Epoch 51/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.1232 - acc: 0.9622 - val_loss: 1.0289 - val_acc: 0.8286\n",
      "Epoch 52/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.1170 - acc: 0.9630 - val_loss: 1.0653 - val_acc: 0.7750\n",
      "Epoch 53/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.1159 - acc: 0.9563 - val_loss: 0.9636 - val_acc: 0.8107\n",
      "Epoch 54/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.1016 - acc: 0.9685 - val_loss: 1.0668 - val_acc: 0.7357\n",
      "Epoch 55/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.1159 - acc: 0.9597 - val_loss: 0.9748 - val_acc: 0.8250\n",
      "Epoch 56/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.1073 - acc: 0.9655 - val_loss: 1.0373 - val_acc: 0.8071\n",
      "Epoch 57/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.0967 - acc: 0.9655 - val_loss: 1.0378 - val_acc: 0.7714\n",
      "Epoch 58/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.1167 - acc: 0.9618 - val_loss: 0.8448 - val_acc: 0.7857\n",
      "Epoch 59/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.0915 - acc: 0.9689 - val_loss: 0.9958 - val_acc: 0.7857\n",
      "Epoch 60/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.0939 - acc: 0.9689 - val_loss: 1.1094 - val_acc: 0.7679\n",
      "Epoch 61/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.1125 - acc: 0.9618 - val_loss: 1.0649 - val_acc: 0.7857\n",
      "Epoch 62/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2380/2380 [==============================] - 12s - loss: 0.1061 - acc: 0.9647 - val_loss: 0.8882 - val_acc: 0.7964\n",
      "Epoch 63/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.1058 - acc: 0.9601 - val_loss: 1.0234 - val_acc: 0.8000\n",
      "Epoch 64/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.0943 - acc: 0.9731 - val_loss: 1.1417 - val_acc: 0.7643\n",
      "Epoch 65/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.0812 - acc: 0.9739 - val_loss: 1.0750 - val_acc: 0.7893\n",
      "Epoch 66/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.0841 - acc: 0.9689 - val_loss: 1.0868 - val_acc: 0.7464\n",
      "Epoch 67/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.0823 - acc: 0.9685 - val_loss: 1.1405 - val_acc: 0.7929\n",
      "Epoch 68/75\n",
      "2380/2380 [==============================] - 13s - loss: 0.0697 - acc: 0.9765 - val_loss: 1.1060 - val_acc: 0.7714\n",
      "Epoch 69/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.0895 - acc: 0.9689 - val_loss: 1.1626 - val_acc: 0.7821\n",
      "Epoch 70/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.0679 - acc: 0.9769 - val_loss: 1.1402 - val_acc: 0.7857\n",
      "Epoch 71/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.0600 - acc: 0.9773 - val_loss: 1.2346 - val_acc: 0.7893\n",
      "Epoch 72/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.1001 - acc: 0.9639 - val_loss: 1.0870 - val_acc: 0.8000\n",
      "Epoch 73/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.0585 - acc: 0.9845 - val_loss: 1.1550 - val_acc: 0.7714\n",
      "Epoch 74/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.0693 - acc: 0.9731 - val_loss: 1.3025 - val_acc: 0.7821\n",
      "Epoch 75/75\n",
      "2380/2380 [==============================] - 12s - loss: 0.0647 - acc: 0.9777 - val_loss: 1.2188 - val_acc: 0.7821\n",
      "Params:  time_steps: 8   n_neurons: 300  n_epoch:  75\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import TensorBoard\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "Preparing data\n",
    "'''\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "'''\n",
    "读入一支股票指定年份的ohlcv数据\n",
    "输入:baseDir,stockCode为字符, startYear,yearNum为整数，\n",
    "输出:dataframe\n",
    "'''\n",
    "def readWSDFile(baseDir, stockCode, startYear, yearNum=1, usecols=None, \n",
    "                names=['date','pre_close','open','high','low','close','change','chg_range',\n",
    "                                               'volume','amount','turn']):\n",
    "    # 解析日期\n",
    "    filename = baseDir+stockCode+'/'+stockCode+'.csv'\n",
    "    print (filename, \"===============\")\n",
    "    dateparse = lambda x: pd.datetime.strptime(x, '%Y/%m/%d').date()\n",
    "    df = pd.read_csv(filename, index_col=0, sep=',', header=None,usecols=usecols,\n",
    "                            skiprows=1, names=names,\n",
    "                           parse_dates=True, date_parser=dateparse)\n",
    "    return df['2005-01-04':'2015-12-31']\n",
    "\n",
    "'''\n",
    "读入一支股票指定年份的技术指标\n",
    "输入:baseDir,stockCode为字符, startYear,yearNum为整数，\n",
    "输出:dataframe\n",
    "'''\n",
    "def readWSDIndexFile(baseDir, stockCode, startYear, yearNum=1):\n",
    "    # 解析日期\n",
    "    dateparse = lambda x: pd.datetime.strptime(x, '%Y/%m/%d').date()\n",
    "\n",
    "    df = 0\n",
    "    for i in range(yearNum):\n",
    "        tempDF = pd.read_csv(baseDir+'I'+stockCode+'/wsd_'+stockCode+'_'+str(startYear+i)+'.csv', index_col=0, sep=',', parse_dates=True, date_parser=dateparse\n",
    "                             # , usecols=usecols\n",
    "                             )\n",
    "        if i==0: df = tempDF\n",
    "        else: df = df.append(tempDF)\n",
    "    return df\n",
    "\n",
    "# 整理好多因子输入，以dataframe返回数据+标签\n",
    "from sklearn import preprocessing\n",
    "def data_prepare(retrace = 0.618):\n",
    "    # prepare data\n",
    "    baseDir = '../data/'\n",
    "    stockCodes = ['000300.SH']\n",
    "    i = 0\n",
    "    startYear = 2005\n",
    "    number =11\n",
    "    usecols = None#[0,5,6]\n",
    "    names = ['date','close','change']\n",
    "    df = readWSDFile(baseDir, stockCodes[i], startYear, number, usecols)\n",
    "    #dfi = readWSDIndexFile(baseDir, stockCodes[i], startYear, number)\n",
    "    allDF = df#pd.concat([df, dfi], axis=1)\n",
    "    sample_num = np.shape(df)[0]\n",
    "    labelDF = pd.Series(np.zeros(sample_num))\n",
    "    print (\"Factors Shape:\", np.shape(df), np.shape(allDF))\n",
    "    \n",
    "    # 求出 trend\n",
    "    price = df['close']\n",
    "    start = 0\n",
    "    while price[start] > price[start+1]:\n",
    "        labelDF[start] = 1 #flat\n",
    "        start +=1\n",
    "    print(\"----- start: \",start)\n",
    "    #find peak, find trough, calculate retracement and label trend accordingly\n",
    "    i = start\n",
    "    while i < sample_num - 1:\n",
    "        cursor = i\n",
    "        while cursor < sample_num - 1 and price[cursor] <= price[cursor+1]:\n",
    "            cursor += 1\n",
    "        peak = cursor\n",
    "        while cursor < sample_num - 1 and price[cursor] >= price[cursor+1]:\n",
    "            cursor += 1\n",
    "        trough = cursor\n",
    "        retracement = (price[peak] - price[trough]) / (price[peak] - price[i])\n",
    "        mark = 1 # flat\n",
    "        if retracement < retrace:\n",
    "            mark = 2 # UP\n",
    "        elif retracement > 1 + retrace:\n",
    "            mark = 0 # DOWN\n",
    "        for k in range(i, cursor+1):\n",
    "            labelDF[k] = mark\n",
    "        i = cursor\n",
    "\n",
    "    print(\"---- Trend Distribution Check --------\")\n",
    "    print(labelDF.value_counts().sort_index())\n",
    "    \n",
    "    # make a deep copy of Price Difference before normalizing\n",
    "    priceDF = allDF['change'].copy(deep=True)\n",
    "    # scikit-learn normalize or: keras.utils.normalize(x)\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    input_data = scaler.fit_transform(allDF)\n",
    "    print (\"input data shape: \", np.shape(input_data)) #  days *  factors\n",
    "    print (\"input label shape: \", np.shape(labelDF))\n",
    "   \n",
    "    return input_data, labelDF, priceDF # train/test data, labels and prices for yield calucluation\n",
    "\n",
    "###### Hyper paramters #########\n",
    "time_steps = 8\n",
    "n_neurons = 300\n",
    "num_classes = 3\n",
    "batch_size = 20 # specify batch size explicitly; no shuffle but successive sequence\n",
    "n_epoch = 75\n",
    "# get training data\n",
    "train_ratio = 0.9\n",
    "#_, dataset, _ = data_prepare()\n",
    "dataset,labels, _ = data_prepare()\n",
    "segment_num = (len(dataset) - time_steps - 1) // batch_size # rollingly use data\n",
    "train_size = int(segment_num * train_ratio)\n",
    "test_size = segment_num - train_size\n",
    "data_dim = 1#np.shape(dataset)[1] #input\n",
    "\n",
    "#divide training/validation dataset\n",
    "#train_x = dataset[0 : train_size * batch_size + time_steps]\n",
    "#test_x = dataset[train_size * batch_size : (train_size + test_size) * batch_size + time_steps]\n",
    "\n",
    "train_x = labels.iloc[0 : train_size * batch_size + time_steps]\n",
    "test_x = labels.iloc[train_size * batch_size : (train_size + test_size) * batch_size + time_steps]\n",
    "#label is just 1 step further after sequence data\n",
    "train_y = labels.iloc[time_steps : train_size * batch_size + time_steps]\n",
    "test_y = labels.iloc[train_size * batch_size + time_steps: (train_size + test_size) * batch_size + time_steps]\n",
    "\n",
    "# construct training data, 1 step forward, keep rolling\n",
    "train_x = np.array(train_x)\n",
    "train_sample = len(train_x) - time_steps\n",
    "b = np.array([[]])\n",
    "for i in range(train_sample):\n",
    "    b = np.append(b, train_x[i : time_steps + i])\n",
    "train_x = b.reshape(train_sample, time_steps, data_dim)\n",
    "print(\"training size: \", train_sample)\n",
    "\n",
    "test_x = np.array(test_x)\n",
    "test_sample = len(test_x) - time_steps\n",
    "b = np.array([[]])\n",
    "for i in range(test_sample):\n",
    "    b = np.append(b, test_x[i : time_steps + i])\n",
    "test_x = b.reshape(test_sample, time_steps, data_dim)\n",
    "print(\"testing size: \", test_sample)\n",
    "\n",
    "train_y = np.array(train_y, dtype=np.int32)\n",
    "test_y = np.array(test_y, dtype=np.int32)\n",
    "\n",
    "\n",
    "# expected input data shape: (batch_size, timesteps, data_dim)\n",
    "model = Sequential()\n",
    "model.add(LSTM(n_neurons, return_sequences=True,stateful=True,\n",
    "               batch_input_shape=(batch_size, time_steps, data_dim)))  # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(n_neurons, return_sequences=True, stateful=True))  # returns a sequence of vectors of dimension 32\n",
    "# add 5 more layers\n",
    "model.add(LSTM(n_neurons, return_sequences=True, stateful=True))\n",
    "\n",
    "model.add(LSTM(n_neurons, stateful=True))  # return a single vector of dimension 32\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', #for integer class, not one hot encoding\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_x, train_y,\n",
    "          batch_size=batch_size, \n",
    "          epochs=n_epoch,\n",
    "          shuffle=False,\n",
    "          validation_data=(test_x, test_y), \n",
    "          callbacks=[TensorBoard(log_dir='./logs/pure_clz', write_graph=True, write_images=True)]\n",
    "         )\n",
    "print (\"Params: \", \"time_steps:\", time_steps, \"  n_neurons:\", n_neurons, \" n_epoch: \", n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " - 9s - loss: 0.2482 - acc: 0.9105 - val_loss: 0.6344 - val_acc: 0.8107\n",
    "    Params:  time_steps: 8   n_neurons: 300  n_epoch:  100\n",
    "    \n",
    "    ## +5 layers\n",
    " - 30s - loss: 1.0736 - acc: 0.4353 - val_loss: 1.0071 - val_acc: 0.5393\n",
    "Params:  time_steps: 8   n_neurons: 300  n_epoch:  50              \n",
    "    \n",
    "    ## +3 layers\n",
    "-- 19s - loss: 1.0764 - acc: 0.4290 - val_loss: 1.0129 - val_acc: 0.5393\n",
    "Params:  time_steps: 8   n_neurons: 300  n_epoch:  75                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/000300.SH/000300.SH.csv ===============\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre_close</th>\n",
       "      <th>open</th>\n",
       "      <th>var1(t-1)</th>\n",
       "      <th>var2(t-1)</th>\n",
       "      <th>var1(t)</th>\n",
       "      <th>var2(t)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-10-08</th>\n",
       "      <td>3202.95</td>\n",
       "      <td>3324.98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-09</th>\n",
       "      <td>3296.48</td>\n",
       "      <td>3302.36</td>\n",
       "      <td>3202.95</td>\n",
       "      <td>3324.98</td>\n",
       "      <td>3296.48</td>\n",
       "      <td>3302.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-12</th>\n",
       "      <td>3340.12</td>\n",
       "      <td>3351.14</td>\n",
       "      <td>3296.48</td>\n",
       "      <td>3302.36</td>\n",
       "      <td>3340.12</td>\n",
       "      <td>3351.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-13</th>\n",
       "      <td>3447.69</td>\n",
       "      <td>3422.48</td>\n",
       "      <td>3340.12</td>\n",
       "      <td>3351.14</td>\n",
       "      <td>3447.69</td>\n",
       "      <td>3422.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-14</th>\n",
       "      <td>3445.04</td>\n",
       "      <td>3431.14</td>\n",
       "      <td>3447.69</td>\n",
       "      <td>3422.48</td>\n",
       "      <td>3445.04</td>\n",
       "      <td>3431.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-15</th>\n",
       "      <td>3406.11</td>\n",
       "      <td>3403.39</td>\n",
       "      <td>3445.04</td>\n",
       "      <td>3431.14</td>\n",
       "      <td>3406.11</td>\n",
       "      <td>3403.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-16</th>\n",
       "      <td>3486.82</td>\n",
       "      <td>3508.52</td>\n",
       "      <td>3406.11</td>\n",
       "      <td>3403.39</td>\n",
       "      <td>3486.82</td>\n",
       "      <td>3508.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-19</th>\n",
       "      <td>3534.07</td>\n",
       "      <td>3548.95</td>\n",
       "      <td>3486.82</td>\n",
       "      <td>3508.52</td>\n",
       "      <td>3534.07</td>\n",
       "      <td>3548.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-20</th>\n",
       "      <td>3534.18</td>\n",
       "      <td>3527.88</td>\n",
       "      <td>3534.07</td>\n",
       "      <td>3548.95</td>\n",
       "      <td>3534.18</td>\n",
       "      <td>3527.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-21</th>\n",
       "      <td>3577.70</td>\n",
       "      <td>3580.84</td>\n",
       "      <td>3534.18</td>\n",
       "      <td>3527.88</td>\n",
       "      <td>3577.70</td>\n",
       "      <td>3580.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-22</th>\n",
       "      <td>3473.25</td>\n",
       "      <td>3453.20</td>\n",
       "      <td>3577.70</td>\n",
       "      <td>3580.84</td>\n",
       "      <td>3473.25</td>\n",
       "      <td>3453.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-23</th>\n",
       "      <td>3524.53</td>\n",
       "      <td>3536.84</td>\n",
       "      <td>3473.25</td>\n",
       "      <td>3453.20</td>\n",
       "      <td>3524.53</td>\n",
       "      <td>3536.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-26</th>\n",
       "      <td>3571.24</td>\n",
       "      <td>3614.70</td>\n",
       "      <td>3524.53</td>\n",
       "      <td>3536.84</td>\n",
       "      <td>3571.24</td>\n",
       "      <td>3614.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-27</th>\n",
       "      <td>3589.26</td>\n",
       "      <td>3569.45</td>\n",
       "      <td>3571.24</td>\n",
       "      <td>3614.70</td>\n",
       "      <td>3589.26</td>\n",
       "      <td>3569.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-28</th>\n",
       "      <td>3592.88</td>\n",
       "      <td>3575.81</td>\n",
       "      <td>3589.26</td>\n",
       "      <td>3569.45</td>\n",
       "      <td>3592.88</td>\n",
       "      <td>3575.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-29</th>\n",
       "      <td>3524.92</td>\n",
       "      <td>3539.98</td>\n",
       "      <td>3592.88</td>\n",
       "      <td>3575.81</td>\n",
       "      <td>3524.92</td>\n",
       "      <td>3539.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-30</th>\n",
       "      <td>3533.31</td>\n",
       "      <td>3530.22</td>\n",
       "      <td>3524.92</td>\n",
       "      <td>3539.98</td>\n",
       "      <td>3533.31</td>\n",
       "      <td>3530.22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pre_close     open  var1(t-1)  var2(t-1)  var1(t)  var2(t)\n",
       "date                                                                  \n",
       "2015-10-08    3202.95  3324.98        NaN        NaN      NaN      NaN\n",
       "2015-10-09    3296.48  3302.36    3202.95    3324.98  3296.48  3302.36\n",
       "2015-10-12    3340.12  3351.14    3296.48    3302.36  3340.12  3351.14\n",
       "2015-10-13    3447.69  3422.48    3340.12    3351.14  3447.69  3422.48\n",
       "2015-10-14    3445.04  3431.14    3447.69    3422.48  3445.04  3431.14\n",
       "2015-10-15    3406.11  3403.39    3445.04    3431.14  3406.11  3403.39\n",
       "2015-10-16    3486.82  3508.52    3406.11    3403.39  3486.82  3508.52\n",
       "2015-10-19    3534.07  3548.95    3486.82    3508.52  3534.07  3548.95\n",
       "2015-10-20    3534.18  3527.88    3534.07    3548.95  3534.18  3527.88\n",
       "2015-10-21    3577.70  3580.84    3534.18    3527.88  3577.70  3580.84\n",
       "2015-10-22    3473.25  3453.20    3577.70    3580.84  3473.25  3453.20\n",
       "2015-10-23    3524.53  3536.84    3473.25    3453.20  3524.53  3536.84\n",
       "2015-10-26    3571.24  3614.70    3524.53    3536.84  3571.24  3614.70\n",
       "2015-10-27    3589.26  3569.45    3571.24    3614.70  3589.26  3569.45\n",
       "2015-10-28    3592.88  3575.81    3589.26    3569.45  3592.88  3575.81\n",
       "2015-10-29    3524.92  3539.98    3592.88    3575.81  3524.92  3539.98\n",
       "2015-10-30    3533.31  3530.22    3524.92    3539.98  3533.31  3530.22"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "import pandas as pd\n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "def readWSDFile(baseDir, stockCode, startYear, yearNum=1, usecols=[0,1,2], \n",
    "                names=['date','pre_close','open']):\n",
    "    # 解析日期\n",
    "    filename = baseDir+stockCode+'/'+stockCode+'.csv'\n",
    "    print (filename, \"===============\")\n",
    "    dateparse = lambda x: pd.datetime.strptime(x, '%Y/%m/%d').date()\n",
    "    df = pd.read_csv(filename, index_col=0, sep=',', header=None,usecols=usecols,\n",
    "                            skiprows=1, names=names,\n",
    "                           parse_dates=True, date_parser=dateparse)\n",
    "    return df['2015-10-04':'2015-10-31']\n",
    "a = readWSDFile('../data/','000300.SH',0)\n",
    "b = series_to_supervised(a)\n",
    "pd.concat([a,b], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
