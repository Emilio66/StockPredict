{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ Let's get the party started! -------------------------------------\n",
      "../data/macro_economy//china_macro_economy_daily.csv\n",
      "../data/macro_economy//OECD-world-economy-daily.csv\n",
      "../data/components//components-top10.csv\n",
      "../data/000300.SH/000300.SH.csv ===============\n",
      "Factors Shape: (2671, 135)\n",
      "---- Trend Distribution Check --------\n",
      "0.0     960\n",
      "1.0     558\n",
      "2.0    1153\n",
      "dtype: int64\n",
      "input data shape:  (2671, 135)\n",
      "input label shape:  (2671,)\n",
      "training size:  2360\n",
      "testing size:  280\n",
      "Train on 2360 samples, validate on 280 samples\n",
      "Epoch 1/100\n",
      "2360/2360 [==============================] - 31s - loss: 0.0639 - val_loss: 0.0441\n",
      "Epoch 2/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0270 - val_loss: 0.0418\n",
      "Epoch 3/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0228 - val_loss: 0.0417\n",
      "Epoch 4/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0202 - val_loss: 0.0416\n",
      "Epoch 5/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0192 - val_loss: 0.0415\n",
      "Epoch 6/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0187 - val_loss: 0.0419\n",
      "Epoch 7/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0183 - val_loss: 0.0421\n",
      "Epoch 8/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0182 - val_loss: 0.0416\n",
      "Epoch 9/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0182 - val_loss: 0.0418\n",
      "Epoch 10/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0174 - val_loss: 0.0417\n",
      "Epoch 11/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0173 - val_loss: 0.0416\n",
      "Epoch 12/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0170 - val_loss: 0.0419\n",
      "Epoch 13/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0170 - val_loss: 0.0422\n",
      "Epoch 14/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0169 - val_loss: 0.0421\n",
      "Epoch 15/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0169 - val_loss: 0.0422\n",
      "Epoch 16/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0169 - val_loss: 0.0424\n",
      "Epoch 17/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0168 - val_loss: 0.0417\n",
      "Epoch 18/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0170 - val_loss: 0.0434\n",
      "Epoch 19/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0170 - val_loss: 0.0424\n",
      "Epoch 20/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0168 - val_loss: 0.0426\n",
      "Epoch 21/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0168 - val_loss: 0.0427\n",
      "Epoch 22/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0172 - val_loss: 0.0431\n",
      "Epoch 23/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0169 - val_loss: 0.0429\n",
      "Epoch 24/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0168 - val_loss: 0.0431\n",
      "Epoch 25/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0168 - val_loss: 0.0432\n",
      "Epoch 26/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0173 - val_loss: 0.0431\n",
      "Epoch 27/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0174 - val_loss: 0.0432\n",
      "Epoch 28/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0173 - val_loss: 0.0431\n",
      "Epoch 29/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0173 - val_loss: 0.0430\n",
      "Epoch 30/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0171 - val_loss: 0.0428\n",
      "Epoch 31/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0173 - val_loss: 0.0437\n",
      "Epoch 32/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0174 - val_loss: 0.0434\n",
      "Epoch 33/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0175 - val_loss: 0.0444\n",
      "Epoch 34/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0175 - val_loss: 0.0436\n",
      "Epoch 35/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0173 - val_loss: 0.0445\n",
      "Epoch 36/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0173 - val_loss: 0.0427\n",
      "Epoch 37/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0173 - val_loss: 0.0426\n",
      "Epoch 38/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0173 - val_loss: 0.0426\n",
      "Epoch 39/100\n",
      "2360/2360 [==============================] - 22s - loss: 0.0172 - val_loss: 0.0423\n",
      "Epoch 40/100\n",
      "2360/2360 [==============================] - 21s - loss: 0.0173 - val_loss: 0.0421\n",
      "Epoch 41/100\n",
      "2360/2360 [==============================] - 21s - loss: 0.0174 - val_loss: 0.0430\n",
      "Epoch 42/100\n",
      "2360/2360 [==============================] - 21s - loss: 0.0174 - val_loss: 0.0459\n",
      "Epoch 43/100\n",
      "2360/2360 [==============================] - 21s - loss: 0.0171 - val_loss: 0.0432\n",
      "Epoch 44/100\n",
      "2360/2360 [==============================] - 21s - loss: 0.0174 - val_loss: 0.0421\n",
      "Epoch 45/100\n",
      "2360/2360 [==============================] - 21s - loss: 0.0175 - val_loss: 0.0448\n",
      "Epoch 46/100\n",
      "2360/2360 [==============================] - 21s - loss: 0.0172 - val_loss: 0.0418\n",
      "Epoch 47/100\n",
      "2360/2360 [==============================] - 21s - loss: 0.0177 - val_loss: 0.0416\n",
      "Epoch 48/100\n",
      "2360/2360 [==============================] - 21s - loss: 0.0176 - val_loss: 0.0439\n",
      "Epoch 49/100\n",
      "2360/2360 [==============================] - 21s - loss: 0.0174 - val_loss: 0.0438\n",
      "Epoch 50/100\n",
      "2360/2360 [==============================] - 21s - loss: 0.0173 - val_loss: 0.0416\n",
      "Epoch 51/100\n",
      "2360/2360 [==============================] - 21s - loss: 0.0174 - val_loss: 0.0425\n",
      "Epoch 52/100\n",
      "2360/2360 [==============================] - 21s - loss: 0.0175 - val_loss: 0.0449\n",
      "Epoch 53/100\n",
      "2360/2360 [==============================] - 21s - loss: 0.0176 - val_loss: 0.0432\n",
      "Epoch 54/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0175 - val_loss: 0.0422\n",
      "Epoch 55/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0179 - val_loss: 0.0442\n",
      "Epoch 56/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0174 - val_loss: 0.0444\n",
      "Epoch 57/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0175 - val_loss: 0.0456\n",
      "Epoch 58/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0173 - val_loss: 0.0460\n",
      "Epoch 59/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0175 - val_loss: 0.0433\n",
      "Epoch 60/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0176 - val_loss: 0.0415\n",
      "Epoch 61/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0175 - val_loss: 0.0426\n",
      "Epoch 62/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0176 - val_loss: 0.0419\n",
      "Epoch 63/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0176 - val_loss: 0.0433\n",
      "Epoch 64/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0177 - val_loss: 0.0420\n",
      "Epoch 65/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0179 - val_loss: 0.0421\n",
      "Epoch 66/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0176 - val_loss: 0.0423\n",
      "Epoch 67/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0175 - val_loss: 0.0411\n",
      "Epoch 68/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0177 - val_loss: 0.0442\n",
      "Epoch 69/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0176 - val_loss: 0.0431\n",
      "Epoch 70/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0177 - val_loss: 0.0439\n",
      "Epoch 71/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0177 - val_loss: 0.0469\n",
      "Epoch 72/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0175 - val_loss: 0.0446\n",
      "Epoch 73/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0175 - val_loss: 0.0416\n",
      "Epoch 74/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0179 - val_loss: 0.0455\n",
      "Epoch 75/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0175 - val_loss: 0.0428\n",
      "Epoch 76/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0173 - val_loss: 0.0417\n",
      "Epoch 77/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0176 - val_loss: 0.0429\n",
      "Epoch 78/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0178 - val_loss: 0.0421\n",
      "Epoch 79/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0177 - val_loss: 0.0453\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2360/2360 [==============================] - 29s - loss: 0.0172 - val_loss: 0.0428\n",
      "Epoch 81/100\n",
      "2360/2360 [==============================] - 30s - loss: 0.0174 - val_loss: 0.0442\n",
      "Epoch 82/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0175 - val_loss: 0.0427\n",
      "Epoch 83/100\n",
      "2360/2360 [==============================] - 30s - loss: 0.0175 - val_loss: 0.0424\n",
      "Epoch 84/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0173 - val_loss: 0.0423\n",
      "Epoch 85/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0176 - val_loss: 0.0417\n",
      "Epoch 86/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0176 - val_loss: 0.0460\n",
      "Epoch 87/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0172 - val_loss: 0.0428\n",
      "Epoch 88/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0178 - val_loss: 0.0454\n",
      "Epoch 89/100\n",
      "2360/2360 [==============================] - 33s - loss: 0.0175 - val_loss: 0.0459\n",
      "Epoch 90/100\n",
      "2360/2360 [==============================] - 39s - loss: 0.0173 - val_loss: 0.0460\n",
      "Epoch 91/100\n",
      "2360/2360 [==============================] - 35s - loss: 0.0170 - val_loss: 0.0427\n",
      "Epoch 92/100\n",
      "2360/2360 [==============================] - 43s - loss: 0.0178 - val_loss: 0.0455\n",
      "Epoch 93/100\n",
      "2360/2360 [==============================] - 37s - loss: 0.0175 - val_loss: 0.0456\n",
      "Epoch 94/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0174 - val_loss: 0.0455\n",
      "Epoch 95/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0173 - val_loss: 0.0438\n",
      "Epoch 96/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0178 - val_loss: 0.0436\n",
      "Epoch 97/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0177 - val_loss: 0.0459\n",
      "Epoch 98/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0173 - val_loss: 0.0444\n",
      "Epoch 99/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0174 - val_loss: 0.0443\n",
      "Epoch 100/100\n",
      "2360/2360 [==============================] - 29s - loss: 0.0174 - val_loss: 0.0430\n",
      "Params:  time_steps: 16  latent_dim: 8  batch_size:  20  n_epoch:  100 layers [32, 128] \n",
      " Time cost:  2905.094607114792\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSqkBRIO+UkoNIBr0lVJqAPn/GYeI+JQxl6MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbec843bcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Construct an AutoEncoder for sequence data based on LSTM\n",
    "'''\n",
    "from keras.layers import Input, LSTM, RepeatVector\n",
    "from keras.models import Model\n",
    "from keras import metrics\n",
    "from sklearn import preprocessing\n",
    "'''\n",
    "Preparing data\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "'''\n",
    "Reading Macro economy factors from CSV\n",
    "Notice: data are distributed monthly, need to be filled for daily usage\n",
    "'''\n",
    "def strip_comma(x):\n",
    "    return float(str(x).replace(',',''))\n",
    "def read_macro_economy(base_dir = '../data/macro_economy/', \n",
    "                       filename = 'china_macro_economy_daily.csv',\n",
    "                       start_date = '2002-01-04', end_date = '2017-11-30',\n",
    "                       names = [i for i in range(31)],\n",
    "                       usecols = None):\n",
    "    filename = base_dir +'/'+ filename\n",
    "    print (filename) #中文读取出问题，所以skip row1\n",
    "    df = pd.read_csv(filename, index_col=0, sep=',', \n",
    "                     skiprows=1, usecols=usecols,\n",
    "                     names = names, parse_dates=True,\n",
    "                     converters = {11: strip_comma, 22: strip_comma}\n",
    "                    )\n",
    "    return df[start_date : end_date]\n",
    "\n",
    "\n",
    "'''\n",
    "Reading World economy factors sponsored by OECD from CSV\n",
    "Notice: data are distributed monthly, need to be filled for daily usage\n",
    "'''\n",
    "def read_world_economy(base_dir = '../data/macro_economy/', \n",
    "                       filename = 'OECD-world-economy-daily.csv',\n",
    "                       start_date = '2002-01-04', end_date = '2017-11-30',\n",
    "                       names = [i for i in range(46)],\n",
    "                       usecols = None):\n",
    "    filename = base_dir +'/'+ filename\n",
    "    print (filename) \n",
    "    df = pd.read_csv(filename, index_col=0, \n",
    "                     skiprows=1, usecols=usecols,parse_dates=True,\n",
    "                     names = names\n",
    "                    )\n",
    "    return df[start_date : end_date]\n",
    "\n",
    "'''\n",
    "Reading Top10 Components CSV\n",
    "Data has been assigned weight according to their ratio in the market\n",
    "\n",
    "# Ref: data calculated from data/generate/FetchingComponentsData.ipynb\n",
    "'''\n",
    "def read_components(base_dir = '../data/components/', \n",
    "                       filename = 'components-top10.csv',\n",
    "                       start_date = '2002-01-04', end_date = '2017-11-30',\n",
    "                       names = [i for i in range(10)]):\n",
    "    filename = base_dir +'/'+ filename\n",
    "    print (filename) \n",
    "    df = pd.read_csv(filename, index_col=0, \n",
    "                     skiprows=1,parse_dates=True,\n",
    "                     names = names\n",
    "                    )\n",
    "    df = df.fillna(0)\n",
    "    return df[start_date : end_date]\n",
    "\n",
    "'''\n",
    "Reading ohlcv transaction data for a stock\n",
    "\n",
    "'''\n",
    "def readWSDFile(baseDir, stockCode, startDate='2005-01-04', endDate= '2015-12-31', usecols=None, \n",
    "                names=['date','pre_close','open','high','low','close','change','chg_range',\n",
    "                                               'volume','amount','turn']):\n",
    "    # 解析日期\n",
    "    filename = baseDir+stockCode+'/'+stockCode+'.csv'\n",
    "    print (filename, \"===============\")\n",
    "    dateparse = lambda x: pd.datetime.strptime(x, '%Y/%m/%d').date()\n",
    "    df = pd.read_csv(filename, index_col=0, sep=',', header=None,usecols=usecols,\n",
    "                            skiprows=1, names=names,\n",
    "                           parse_dates=True, date_parser=dateparse)\n",
    "    df = df.fillna(0)\n",
    "    return df[startDate : endDate]\n",
    "\n",
    "'''\n",
    "Reading Technical indicators of a stock\n",
    "'''\n",
    "def readWSDIndexFile(baseDir, stockCode, startYear, yearNum=1):\n",
    "    # parse date\n",
    "    dateparse = lambda x: pd.datetime.strptime(x, '%Y/%m/%d').date()\n",
    "\n",
    "    df = 0\n",
    "    for i in range(yearNum):\n",
    "        tempDF = pd.read_csv(baseDir+'I'+stockCode+'/wsd_'+stockCode+'_'+str(startYear+i)+'.csv', index_col=0, sep=',', parse_dates=True, date_parser=dateparse\n",
    "                             # , usecols=usecols\n",
    "                             )\n",
    "        if i==0: df = tempDF\n",
    "        else: df = df.append(tempDF)\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "'''\n",
    "Preparing all the useful data\n",
    "'''\n",
    "# 整理好多因子输入，以dataframe返回数据+标签\n",
    "def data_prepare(retrace = 0.618, dtype = 'all', start_date='2005-01-04', end_date= '2015-12-31'):\n",
    "    # prepare data\n",
    "    baseDir = '../data/'\n",
    "    stockCodes = ['000300.SH']\n",
    "    i = 0\n",
    "    startYear = 2005\n",
    "    number =11\n",
    "    print(\"------------------------ Let's get the party started! -------------------------------------\")\n",
    "    ## Load data from CSV\n",
    "    dfm = read_macro_economy(start_date = start_date, end_date = end_date)\n",
    "    dfw = read_world_economy(start_date = start_date, end_date = end_date)\n",
    "    dfc = read_components(start_date = start_date, end_date = end_date)\n",
    "    df = readWSDFile(baseDir, stockCodes[i], start_date, end_date)\n",
    "    dfi = readWSDIndexFile(baseDir, stockCodes[i], startYear, number)\n",
    "    allDF = df\n",
    "    if dtype == 'all':\n",
    "        allDF = pd.concat([df, dfi, dfm, dfw, dfc], axis=1)\n",
    "    elif dtype == 'only_m':\n",
    "        allDF = pd.concat([df, dfm], axis=1) # macro economy \n",
    "    elif dtype == 'only_i':\n",
    "        allDF = pd.concat([df, dfi], axis=1) # technical indicators\n",
    "    elif dtype == 'only_w':\n",
    "        allDF = pd.concat([df, dfw], axis=1) # world economy\n",
    "    elif dtype == 'only_c':\n",
    "        allDF = pd.concat([df, dfc], axis=1) # constitutant stock\n",
    "    elif dtype == 'im':\n",
    "        allDF = pd.concat([df, dfi, dfm], axis=1) \n",
    "    elif dtype == 'ic':\n",
    "        allDF = pd.concat([df, dfi, dfc], axis=1) \n",
    "    else:\n",
    "        pass\n",
    "    allDF = allDF.fillna(0)\n",
    "    sample_num = np.shape(df)[0]\n",
    "    labelDF = pd.Series(np.zeros(sample_num))\n",
    "    print (\"Factors Shape:\", np.shape(allDF))\n",
    "    \n",
    "    # 求出 trend\n",
    "    price = df['close']\n",
    "    start = 0\n",
    "    while price[start] > price[start+1]:\n",
    "        labelDF[start] = 1 #flat\n",
    "        start +=1\n",
    "    \n",
    "    #find peak, find trough, calculate retracement and label trend accordingly\n",
    "    i = start\n",
    "    while i < sample_num - 1:\n",
    "        cursor = i\n",
    "        while cursor < sample_num - 1 and price[cursor] <= price[cursor+1]:\n",
    "            cursor += 1\n",
    "        peak = cursor\n",
    "        while cursor < sample_num - 1 and price[cursor] >= price[cursor+1]:\n",
    "            cursor += 1\n",
    "        trough = cursor\n",
    "        retracement = (price[peak] - price[trough]) / (price[peak] - price[i])\n",
    "        mark = 1 # flat\n",
    "        if retracement < retrace:\n",
    "            mark = 2 # UP\n",
    "        elif retracement > 1 + retrace:\n",
    "            mark = 0 # DOWN\n",
    "        for k in range(i, cursor+1):\n",
    "            labelDF[k] = mark\n",
    "        i = cursor\n",
    "\n",
    "    print(\"---- Trend Distribution Check --------\")\n",
    "    print(labelDF.value_counts().sort_index())\n",
    "    \n",
    "    # make a deep copy of Price Difference before normalizing\n",
    "    priceDF = allDF['change'].copy(deep=True)\n",
    "    # normalize(x)\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    input_data = scaler.fit_transform(allDF)\n",
    "    print (\"input data shape: \", np.shape(input_data)) #  days *  factors\n",
    "    print (\"input label shape: \", np.shape(labelDF))\n",
    "    labels = labelDF.values\n",
    "    #input_data = np.concatenate((input_data, labels.reshape(-1,1)), axis = 1) # historical data as part of the series\n",
    "    return input_data, labels, priceDF, price # train/test data, labels, prices difference, actual price for yield calucluation\n",
    "\n",
    "###### Hyper paramters #########\n",
    "time_steps = 16\n",
    "batch_size = 20 # specify batch size explicitly; no shuffle but successive sequence\n",
    "n_epoch = 100\n",
    "train_ratio = 0.9\n",
    "concate = True\n",
    "t0 = time()\n",
    "dataset,labels, priceDF, price = data_prepare()\n",
    "segment_num = (len(dataset) - time_steps - 1) // batch_size # rollingly use data\n",
    "train_size = int(segment_num * train_ratio)\n",
    "test_size = segment_num - train_size\n",
    "data_dim = np.shape(dataset)[1] #input + historic labels\n",
    "\n",
    "#divide training/validation dataset; numpy array\n",
    "train_x = dataset[0 : train_size * batch_size + time_steps]\n",
    "test_x = dataset[train_size * batch_size : (train_size + test_size) * batch_size + time_steps]\n",
    "\n",
    "#historic label as input time series\n",
    "train_historic_label = np.array(labels[0 : train_size * batch_size + time_steps]).reshape(-1,1)\n",
    "test_historic_label = np.array(labels[train_size * batch_size : (train_size + test_size) * batch_size + time_steps]).reshape(-1,1)\n",
    "\n",
    "#Sliding window: label is just 1 step further after sequence data\n",
    "train_y = labels[time_steps : train_size * batch_size + time_steps]\n",
    "test_y = labels[train_size * batch_size + time_steps: (train_size + test_size) * batch_size + time_steps]\n",
    "\n",
    "# add historical trends or not\n",
    "if concate:\n",
    "    data_dim += 1\n",
    "    train_x = np.concatenate((train_x, train_historic_label), axis = 1)\n",
    "    test_x = np.concatenate((test_x, test_historic_label), axis = 1)\n",
    "\n",
    "train_sample = len(train_x) - time_steps\n",
    "b = np.array([[]])\n",
    "# creating data in a rolling window view \n",
    "for i in range(train_sample):\n",
    "    b = np.append(b, train_x[i : time_steps + i])\n",
    "train_x = b.reshape(train_sample, time_steps, data_dim)\n",
    "print(\"training size: \", train_sample)\n",
    "\n",
    "test_sample = len(test_x) - time_steps\n",
    "b = np.array([[]])\n",
    "for i in range(test_sample):\n",
    "    b = np.append(b, test_x[i : time_steps + i])\n",
    "test_x = b.reshape(test_sample, time_steps, data_dim)\n",
    "print(\"testing size: \", test_sample)\n",
    "\n",
    "train_y = np.array(train_y, dtype=np.int32)\n",
    "test_y = np.array(test_y, dtype=np.int32)\n",
    "\n",
    "############## AutoEncoder MODEL ##########################\n",
    "latent_dim = 8\n",
    "layer1 = 128\n",
    "layer2 = 32\n",
    "layers = [layer2,layer1]\n",
    "inputs = Input(shape=(time_steps, data_dim))\n",
    "encoded = LSTM(units = layer1, return_sequences = True)(inputs)\n",
    "encoded = LSTM(units = layer2, return_sequences = True)(encoded)\n",
    "encoded = LSTM(units = latent_dim, return_sequences = False)(encoded) # most hidden layer, only preseve the last step's output\n",
    "\n",
    "repeated_out = RepeatVector(time_steps)(encoded)  # repeat intermediate output [2D -> 3D]\n",
    "\n",
    "decoded = LSTM(layer2, return_sequences=True)(repeated_out)\n",
    "decoded = LSTM(layer1, return_sequences=True)(decoded)\n",
    "decoded = LSTM(data_dim, return_sequences=True)(decoded)  # output layer as a comparison\n",
    "\n",
    "sequence_autoencoder = Model(inputs, decoded)\n",
    "encoder = Model(inputs, encoded)  ## encoder for dimensionality reduction\n",
    "\n",
    "sequence_autoencoder.compile(optimizer='rmsprop', \n",
    "                             loss='mean_squared_error',\n",
    "                             )\n",
    "history = sequence_autoencoder.fit(x = train_x, y = train_x, \n",
    "                         batch_size=batch_size,\n",
    "                         epochs=n_epoch,\n",
    "                         shuffle=False,\n",
    "                         validation_data = (test_x, test_x))\n",
    "print (\"Params: \", \"time_steps:\", time_steps, \" latent_dim:\", \n",
    "       latent_dim, \" batch_size: \",batch_size, \" n_epoch: \", \n",
    "       n_epoch, 'layers', layers, \"\\n Time cost: \", (time() - t0))\n",
    "\n",
    "# encoder.save('autoencoder-49d-8ts-100ep-24-12-8.h5')\n",
    "# compressed_data = encoder.predict(train_x)\n",
    "\n",
    "# # plot history\n",
    "# plt.plot(history.history['mean_squared_error'], label='train')\n",
    "# plt.plot(history.history['val_mean_squared_error'], label='test')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# # loss\n",
    "plt.plot(history.history['loss'], label='val_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ## drawing compressed data\n",
    "# plt.scatter(compressed_data[:, 0], compressed_data[:, 1],marker='.')\n",
    "# plt.show()\n",
    "\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# ax.scatter(compressed_data[:, 0], compressed_data[:, 1], compressed_data[:,2],marker='o', c= 'r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
