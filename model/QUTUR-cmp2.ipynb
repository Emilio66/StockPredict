{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ Let's get the party started! -------------------------------------\n",
      "../data/macro_economy//china_macro_economy_daily.csv\n",
      "../data/macro_economy//OECD-world-economy-daily.csv\n",
      "../data/components//components-top10.csv\n",
      "../data/000300.SH/000300.SH.csv ===============\n",
      "Factors Shape: (2671, 135)\n",
      "---- Trend Distribution Check --------\n",
      "0.0     960\n",
      "1.0     558\n",
      "2.0    1153\n",
      "dtype: int64\n",
      "input data shape:  (2671, 135)\n",
      "input label shape:  (2671,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/models.py:251: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction model input data shape:  (2663, 8)\n",
      "train sample:  2360 data_dim 8\n",
      "training size:  2360\n",
      "testing size:  280\n",
      "Train on 2360 samples, validate on 280 samples\n",
      "Epoch 1/100\n",
      "2360/2360 [==============================] - 5s - loss: 1.0874 - acc: 0.4475 - val_loss: 1.0243 - val_acc: 0.5143\n",
      "Epoch 2/100\n",
      "2360/2360 [==============================] - 4s - loss: 1.0686 - acc: 0.4309 - val_loss: 1.0315 - val_acc: 0.5143\n",
      "Epoch 3/100\n",
      "2360/2360 [==============================] - 4s - loss: 1.0665 - acc: 0.4407 - val_loss: 1.0272 - val_acc: 0.5143\n",
      "Epoch 4/100\n",
      "2360/2360 [==============================] - 4s - loss: 1.0658 - acc: 0.4263 - val_loss: 1.0384 - val_acc: 0.5250\n",
      "Epoch 5/100\n",
      "2360/2360 [==============================] - 4s - loss: 1.0744 - acc: 0.4267 - val_loss: 1.0386 - val_acc: 0.5143\n",
      "Epoch 6/100\n",
      "2360/2360 [==============================] - 4s - loss: 1.0648 - acc: 0.4538 - val_loss: 1.0181 - val_acc: 0.5143\n",
      "Epoch 7/100\n",
      "2360/2360 [==============================] - 4s - loss: 1.0658 - acc: 0.4305 - val_loss: 1.0170 - val_acc: 0.5143\n",
      "Epoch 8/100\n",
      "2360/2360 [==============================] - 4s - loss: 1.0639 - acc: 0.4301 - val_loss: 1.0204 - val_acc: 0.5143\n",
      "Epoch 9/100\n",
      "2360/2360 [==============================] - 4s - loss: 1.0628 - acc: 0.4169 - val_loss: 1.0259 - val_acc: 0.5143\n",
      "Epoch 10/100\n",
      "2360/2360 [==============================] - 4s - loss: 1.0629 - acc: 0.4195 - val_loss: 1.0261 - val_acc: 0.5143\n",
      "Epoch 11/100\n",
      "2360/2360 [==============================] - 4s - loss: 1.0653 - acc: 0.4229 - val_loss: 1.0289 - val_acc: 0.5143\n",
      "Epoch 12/100\n",
      "2360/2360 [==============================] - 4s - loss: 1.0689 - acc: 0.4174 - val_loss: 1.0308 - val_acc: 0.5143\n",
      "Epoch 13/100\n",
      "2360/2360 [==============================] - 4s - loss: 1.0732 - acc: 0.4182 - val_loss: 1.0282 - val_acc: 0.5143\n",
      "Epoch 14/100\n",
      "2360/2360 [==============================] - 4s - loss: 1.0737 - acc: 0.4178 - val_loss: 1.0337 - val_acc: 0.5143\n",
      "Epoch 15/100\n",
      "2360/2360 [==============================] - 4s - loss: 1.0711 - acc: 0.4203 - val_loss: 1.0277 - val_acc: 0.5143\n",
      "Epoch 16/100\n",
      "2360/2360 [==============================] - 4s - loss: 1.0704 - acc: 0.4225 - val_loss: 1.0199 - val_acc: 0.5143\n",
      "Epoch 17/100\n",
      "2360/2360 [==============================] - 4s - loss: 1.0665 - acc: 0.4297 - val_loss: 1.0177 - val_acc: 0.5143\n",
      "Epoch 18/100\n",
      "2360/2360 [==============================] - 4s - loss: 1.0602 - acc: 0.4360 - val_loss: 1.0134 - val_acc: 0.5143\n",
      "Epoch 19/100\n",
      "2360/2360 [==============================] - 4s - loss: 1.0571 - acc: 0.4483 - val_loss: 1.0092 - val_acc: 0.5107\n",
      "Epoch 20/100\n",
      "2360/2360 [==============================] - 4s - loss: 1.0480 - acc: 0.4492 - val_loss: 1.0119 - val_acc: 0.5143\n",
      "Epoch 21/100\n",
      "2360/2360 [==============================] - 4s - loss: 1.0399 - acc: 0.4911 - val_loss: 1.0116 - val_acc: 0.5143\n",
      "Epoch 22/100\n",
      "2360/2360 [==============================] - 4s - loss: 1.0324 - acc: 0.5017 - val_loss: 0.9836 - val_acc: 0.5214\n",
      "Epoch 23/100\n",
      "2360/2360 [==============================] - 4s - loss: 1.0251 - acc: 0.5131 - val_loss: 0.9481 - val_acc: 0.5250\n",
      "Epoch 24/100\n",
      "2360/2360 [==============================] - 4s - loss: 1.0020 - acc: 0.5398 - val_loss: 0.9307 - val_acc: 0.5679\n",
      "Epoch 25/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.9925 - acc: 0.5479 - val_loss: 1.0538 - val_acc: 0.5071\n",
      "Epoch 26/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.9913 - acc: 0.5525 - val_loss: 0.9363 - val_acc: 0.5643\n",
      "Epoch 27/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.9599 - acc: 0.5852 - val_loss: 0.8513 - val_acc: 0.6286\n",
      "Epoch 28/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.9554 - acc: 0.5818 - val_loss: 0.8354 - val_acc: 0.6214\n",
      "Epoch 29/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.9435 - acc: 0.5852 - val_loss: 0.8361 - val_acc: 0.6429\n",
      "Epoch 30/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.9275 - acc: 0.6000 - val_loss: 0.8477 - val_acc: 0.6214\n",
      "Epoch 31/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.9233 - acc: 0.5979 - val_loss: 0.7971 - val_acc: 0.6536\n",
      "Epoch 32/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.9152 - acc: 0.6085 - val_loss: 0.8514 - val_acc: 0.6250\n",
      "Epoch 33/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.9144 - acc: 0.6055 - val_loss: 0.8206 - val_acc: 0.6536\n",
      "Epoch 34/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.9139 - acc: 0.6114 - val_loss: 0.8798 - val_acc: 0.6143\n",
      "Epoch 35/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.9088 - acc: 0.6153 - val_loss: 0.8484 - val_acc: 0.6286\n",
      "Epoch 36/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.9071 - acc: 0.6068 - val_loss: 0.8402 - val_acc: 0.6250\n",
      "Epoch 37/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.9022 - acc: 0.6064 - val_loss: 0.8117 - val_acc: 0.6357\n",
      "Epoch 38/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8997 - acc: 0.6153 - val_loss: 0.8551 - val_acc: 0.6214\n",
      "Epoch 39/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8917 - acc: 0.6250 - val_loss: 0.8241 - val_acc: 0.6357\n",
      "Epoch 40/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8924 - acc: 0.6144 - val_loss: 0.8228 - val_acc: 0.6464\n",
      "Epoch 41/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8931 - acc: 0.6229 - val_loss: 0.8249 - val_acc: 0.6571\n",
      "Epoch 42/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8683 - acc: 0.6318 - val_loss: 0.8264 - val_acc: 0.6464\n",
      "Epoch 43/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8655 - acc: 0.6335 - val_loss: 0.8466 - val_acc: 0.6286\n",
      "Epoch 44/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8762 - acc: 0.6288 - val_loss: 0.8030 - val_acc: 0.6714\n",
      "Epoch 45/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8874 - acc: 0.6242 - val_loss: 0.8621 - val_acc: 0.6179\n",
      "Epoch 46/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8671 - acc: 0.6339 - val_loss: 0.8249 - val_acc: 0.6429\n",
      "Epoch 47/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8682 - acc: 0.6246 - val_loss: 0.8358 - val_acc: 0.6286\n",
      "Epoch 48/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8683 - acc: 0.6220 - val_loss: 0.8292 - val_acc: 0.6536\n",
      "Epoch 49/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8710 - acc: 0.6199 - val_loss: 0.8164 - val_acc: 0.6500\n",
      "Epoch 50/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8653 - acc: 0.6305 - val_loss: 0.8041 - val_acc: 0.6750\n",
      "Epoch 51/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8715 - acc: 0.6237 - val_loss: 0.8238 - val_acc: 0.6500\n",
      "Epoch 52/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8573 - acc: 0.6364 - val_loss: 0.8131 - val_acc: 0.6536\n",
      "Epoch 53/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8476 - acc: 0.6373 - val_loss: 0.8160 - val_acc: 0.6429\n",
      "Epoch 54/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8561 - acc: 0.6343 - val_loss: 0.8328 - val_acc: 0.6821\n",
      "Epoch 55/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8543 - acc: 0.6373 - val_loss: 0.8060 - val_acc: 0.6607\n",
      "Epoch 56/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8537 - acc: 0.6403 - val_loss: 0.7812 - val_acc: 0.6786\n",
      "Epoch 57/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8637 - acc: 0.6326 - val_loss: 0.8170 - val_acc: 0.6714\n",
      "Epoch 58/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8570 - acc: 0.6339 - val_loss: 0.8043 - val_acc: 0.6679\n",
      "Epoch 59/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8412 - acc: 0.6398 - val_loss: 0.8402 - val_acc: 0.6429\n",
      "Epoch 60/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8430 - acc: 0.6360 - val_loss: 0.7990 - val_acc: 0.6714\n",
      "Epoch 61/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8452 - acc: 0.6343 - val_loss: 0.8356 - val_acc: 0.6357\n",
      "Epoch 62/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8565 - acc: 0.6326 - val_loss: 0.8266 - val_acc: 0.6393\n",
      "Epoch 63/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8493 - acc: 0.6394 - val_loss: 0.8209 - val_acc: 0.6429\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2360/2360 [==============================] - 4s - loss: 0.8526 - acc: 0.6343 - val_loss: 0.8056 - val_acc: 0.6536\n",
      "Epoch 65/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8394 - acc: 0.6441 - val_loss: 0.8342 - val_acc: 0.6357\n",
      "Epoch 66/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8408 - acc: 0.6411 - val_loss: 0.8118 - val_acc: 0.6643\n",
      "Epoch 67/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8399 - acc: 0.6386 - val_loss: 0.8259 - val_acc: 0.6464\n",
      "Epoch 68/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8355 - acc: 0.6407 - val_loss: 0.8682 - val_acc: 0.6143\n",
      "Epoch 69/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8390 - acc: 0.6419 - val_loss: 0.8434 - val_acc: 0.6393\n",
      "Epoch 70/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8435 - acc: 0.6398 - val_loss: 0.8845 - val_acc: 0.6214\n",
      "Epoch 71/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8387 - acc: 0.6475 - val_loss: 0.8577 - val_acc: 0.6357\n",
      "Epoch 72/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8359 - acc: 0.6458 - val_loss: 0.8181 - val_acc: 0.6750\n",
      "Epoch 73/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8272 - acc: 0.6487 - val_loss: 0.8099 - val_acc: 0.6679\n",
      "Epoch 74/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8299 - acc: 0.6466 - val_loss: 0.8139 - val_acc: 0.6679\n",
      "Epoch 75/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8361 - acc: 0.6398 - val_loss: 0.8459 - val_acc: 0.6821\n",
      "Epoch 76/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8400 - acc: 0.6424 - val_loss: 0.8491 - val_acc: 0.6607\n",
      "Epoch 77/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8390 - acc: 0.6419 - val_loss: 0.8424 - val_acc: 0.6607\n",
      "Epoch 78/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8329 - acc: 0.6428 - val_loss: 0.8137 - val_acc: 0.6643\n",
      "Epoch 79/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8425 - acc: 0.6487 - val_loss: 0.8033 - val_acc: 0.6786\n",
      "Epoch 80/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8323 - acc: 0.6483 - val_loss: 0.8207 - val_acc: 0.6536\n",
      "Epoch 81/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8204 - acc: 0.6521 - val_loss: 0.8276 - val_acc: 0.6607\n",
      "Epoch 82/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8336 - acc: 0.6432 - val_loss: 0.8329 - val_acc: 0.6571\n",
      "Epoch 83/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8196 - acc: 0.6470 - val_loss: 0.8322 - val_acc: 0.6429\n",
      "Epoch 84/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8209 - acc: 0.6538 - val_loss: 0.8090 - val_acc: 0.6857\n",
      "Epoch 85/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8308 - acc: 0.6508 - val_loss: 0.7932 - val_acc: 0.6679\n",
      "Epoch 86/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8286 - acc: 0.6492 - val_loss: 0.8208 - val_acc: 0.6571\n",
      "Epoch 87/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8055 - acc: 0.6597 - val_loss: 0.8389 - val_acc: 0.6357\n",
      "Epoch 88/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8093 - acc: 0.6631 - val_loss: 0.8484 - val_acc: 0.6429\n",
      "Epoch 89/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8231 - acc: 0.6555 - val_loss: 0.8374 - val_acc: 0.6464\n",
      "Epoch 90/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8219 - acc: 0.6500 - val_loss: 0.8502 - val_acc: 0.6214\n",
      "Epoch 91/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8167 - acc: 0.6530 - val_loss: 0.8169 - val_acc: 0.6536\n",
      "Epoch 92/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8098 - acc: 0.6564 - val_loss: 0.8359 - val_acc: 0.6286\n",
      "Epoch 93/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8173 - acc: 0.6547 - val_loss: 0.8580 - val_acc: 0.6393\n",
      "Epoch 94/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8126 - acc: 0.6500 - val_loss: 0.8154 - val_acc: 0.6536\n",
      "Epoch 95/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8065 - acc: 0.6525 - val_loss: 0.8252 - val_acc: 0.6536\n",
      "Epoch 96/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8168 - acc: 0.6504 - val_loss: 0.8437 - val_acc: 0.6679\n",
      "Epoch 97/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8007 - acc: 0.6623 - val_loss: 0.8344 - val_acc: 0.6536\n",
      "Epoch 98/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8055 - acc: 0.6564 - val_loss: 0.8611 - val_acc: 0.6500\n",
      "Epoch 99/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8176 - acc: 0.6424 - val_loss: 0.8848 - val_acc: 0.6321\n",
      "Epoch 100/100\n",
      "2360/2360 [==============================] - 4s - loss: 0.8170 - acc: 0.6538 - val_loss: 0.8304 - val_acc: 0.6393\n",
      "Params:  time_steps: 4   n_neurons: 200  batch_size:  20  n_epoch:  100 layers  data_dim  8 5 \n",
      " Time cost:  472.70309591293335\n",
      "Add Dropout between every layer:  0.3\n",
      "Avg. Train ACC =  0.587292373045\n",
      "Avg. Test ACC =  0.614678569968\n",
      "Trading ratio:  0.05693950177935943\n",
      "max_drawdown:  0.0\n",
      "ROR:  9.63074597598\n",
      "mean 0.224178\n",
      "std 0.227666\n",
      "max 0.610489\n",
      "min 0.0294437\n",
      " buy & hold:  0.533577089632\n",
      "sharpe_ratio 0.984678\n",
      "return sum:  5328.68 return ratio:  2.17692621946 Alpha:  1.64334912983\n",
      "prediction model input data shape:  (2663, 8)\n",
      "train sample:  2360 data_dim 8\n",
      "training size:  2360\n",
      "testing size:  280\n",
      "Train on 2360 samples, validate on 280 samples\n",
      "Epoch 1/100\n",
      "2360/2360 [==============================] - 8s - loss: 1.0962 - acc: 0.4182 - val_loss: 1.0396 - val_acc: 0.5214\n",
      "Epoch 2/100\n",
      "2360/2360 [==============================] - 7s - loss: 1.0751 - acc: 0.4292 - val_loss: 1.0361 - val_acc: 0.5214\n",
      "Epoch 3/100\n",
      "2360/2360 [==============================] - 7s - loss: 1.0700 - acc: 0.4199 - val_loss: 1.0342 - val_acc: 0.5214\n",
      "Epoch 4/100\n",
      "2360/2360 [==============================] - 7s - loss: 1.0709 - acc: 0.4178 - val_loss: 1.0301 - val_acc: 0.5214\n",
      "Epoch 5/100\n",
      "2360/2360 [==============================] - 7s - loss: 1.0664 - acc: 0.4449 - val_loss: 1.0231 - val_acc: 0.5214\n",
      "Epoch 6/100\n",
      "2360/2360 [==============================] - 7s - loss: 1.0666 - acc: 0.4492 - val_loss: 1.0167 - val_acc: 0.5214\n",
      "Epoch 7/100\n",
      "2360/2360 [==============================] - 8s - loss: 1.0596 - acc: 0.4703 - val_loss: 0.9723 - val_acc: 0.5286\n",
      "Epoch 8/100\n",
      "2360/2360 [==============================] - 8s - loss: 1.0507 - acc: 0.4890 - val_loss: 0.9660 - val_acc: 0.5429\n",
      "Epoch 9/100\n",
      "2360/2360 [==============================] - 7s - loss: 1.0164 - acc: 0.5280 - val_loss: 0.9085 - val_acc: 0.5929\n",
      "Epoch 10/100\n",
      "2360/2360 [==============================] - 8s - loss: 0.9883 - acc: 0.5572 - val_loss: 0.8713 - val_acc: 0.5786\n",
      "Epoch 11/100\n",
      "2360/2360 [==============================] - 9s - loss: 0.9547 - acc: 0.5792 - val_loss: 0.8496 - val_acc: 0.6214\n",
      "Epoch 12/100\n",
      "2360/2360 [==============================] - 9s - loss: 0.9432 - acc: 0.5818 - val_loss: 0.8295 - val_acc: 0.6286\n",
      "Epoch 13/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.9293 - acc: 0.5856 - val_loss: 0.7927 - val_acc: 0.6607\n",
      "Epoch 14/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.9130 - acc: 0.6068 - val_loss: 0.8180 - val_acc: 0.6393\n",
      "Epoch 15/100\n",
      "2360/2360 [==============================] - 8s - loss: 0.9128 - acc: 0.6008 - val_loss: 0.8461 - val_acc: 0.6107\n",
      "Epoch 16/100\n",
      "2360/2360 [==============================] - 7s - loss: 0.9059 - acc: 0.5992 - val_loss: 0.8109 - val_acc: 0.6464\n",
      "Epoch 17/100\n",
      "2360/2360 [==============================] - 7s - loss: 0.8913 - acc: 0.6191 - val_loss: 0.8212 - val_acc: 0.6750\n",
      "Epoch 18/100\n",
      "2360/2360 [==============================] - 7s - loss: 0.8944 - acc: 0.6178 - val_loss: 0.8275 - val_acc: 0.6250\n",
      "Epoch 19/100\n",
      "2360/2360 [==============================] - 7s - loss: 0.8909 - acc: 0.6081 - val_loss: 0.7966 - val_acc: 0.6607\n",
      "Epoch 20/100\n",
      "2360/2360 [==============================] - 7s - loss: 0.8854 - acc: 0.6153 - val_loss: 0.7901 - val_acc: 0.6607\n",
      "Epoch 21/100\n",
      "2360/2360 [==============================] - 7s - loss: 0.8886 - acc: 0.6097 - val_loss: 0.8130 - val_acc: 0.6250\n",
      "Epoch 22/100\n",
      "2360/2360 [==============================] - 7s - loss: 0.8674 - acc: 0.6292 - val_loss: 0.8001 - val_acc: 0.6357\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2360/2360 [==============================] - 7s - loss: 0.8722 - acc: 0.6203 - val_loss: 0.7967 - val_acc: 0.6429\n",
      "Epoch 24/100\n",
      "2360/2360 [==============================] - 7s - loss: 0.8698 - acc: 0.6254 - val_loss: 0.7624 - val_acc: 0.7000\n",
      "Epoch 25/100\n",
      "2360/2360 [==============================] - 7s - loss: 0.8553 - acc: 0.6381 - val_loss: 0.7887 - val_acc: 0.6429\n",
      "Epoch 26/100\n",
      "2360/2360 [==============================] - 7s - loss: 0.8495 - acc: 0.6369 - val_loss: 0.7573 - val_acc: 0.6893\n",
      "Epoch 27/100\n",
      "2360/2360 [==============================] - 9s - loss: 0.8573 - acc: 0.6326 - val_loss: 0.7823 - val_acc: 0.6679\n",
      "Epoch 28/100\n",
      "2360/2360 [==============================] - 9s - loss: 0.8585 - acc: 0.6356 - val_loss: 0.7830 - val_acc: 0.6714\n",
      "Epoch 29/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.8470 - acc: 0.6309 - val_loss: 0.7667 - val_acc: 0.6821\n",
      "Epoch 30/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.8490 - acc: 0.6318 - val_loss: 0.7580 - val_acc: 0.6786\n",
      "Epoch 31/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.8467 - acc: 0.6398 - val_loss: 0.8055 - val_acc: 0.6750\n",
      "Epoch 32/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.8344 - acc: 0.6428 - val_loss: 0.8220 - val_acc: 0.6607\n",
      "Epoch 33/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.8481 - acc: 0.6284 - val_loss: 0.7876 - val_acc: 0.6893\n",
      "Epoch 34/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.8419 - acc: 0.6356 - val_loss: 0.8086 - val_acc: 0.6714\n",
      "Epoch 35/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.8260 - acc: 0.6407 - val_loss: 0.7680 - val_acc: 0.7000\n",
      "Epoch 36/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.8293 - acc: 0.6356 - val_loss: 0.7765 - val_acc: 0.6643\n",
      "Epoch 37/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.8186 - acc: 0.6496 - val_loss: 0.8196 - val_acc: 0.6714\n",
      "Epoch 38/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.8244 - acc: 0.6458 - val_loss: 0.8231 - val_acc: 0.6607\n",
      "Epoch 39/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.8383 - acc: 0.6398 - val_loss: 0.8102 - val_acc: 0.6321\n",
      "Epoch 40/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.8331 - acc: 0.6415 - val_loss: 0.7821 - val_acc: 0.6786\n",
      "Epoch 41/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.8183 - acc: 0.6466 - val_loss: 0.7831 - val_acc: 0.6607\n",
      "Epoch 42/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.8204 - acc: 0.6445 - val_loss: 0.7919 - val_acc: 0.6786\n",
      "Epoch 43/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.8152 - acc: 0.6424 - val_loss: 0.7916 - val_acc: 0.6929\n",
      "Epoch 44/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.8109 - acc: 0.6542 - val_loss: 0.7916 - val_acc: 0.6750\n",
      "Epoch 45/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.8001 - acc: 0.6606 - val_loss: 0.7904 - val_acc: 0.6893\n",
      "Epoch 46/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.8112 - acc: 0.6564 - val_loss: 0.7873 - val_acc: 0.6750\n",
      "Epoch 47/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.8091 - acc: 0.6589 - val_loss: 0.7895 - val_acc: 0.7000\n",
      "Epoch 48/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.7910 - acc: 0.6610 - val_loss: 0.7738 - val_acc: 0.6929\n",
      "Epoch 49/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.7925 - acc: 0.6593 - val_loss: 0.7642 - val_acc: 0.6821\n",
      "Epoch 50/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.7941 - acc: 0.6602 - val_loss: 0.7757 - val_acc: 0.6857\n",
      "Epoch 51/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.7943 - acc: 0.6627 - val_loss: 0.7798 - val_acc: 0.6786\n",
      "Epoch 52/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.8021 - acc: 0.6619 - val_loss: 0.8215 - val_acc: 0.6821\n",
      "Epoch 53/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.7985 - acc: 0.6602 - val_loss: 0.7994 - val_acc: 0.6821\n",
      "Epoch 54/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.7769 - acc: 0.6674 - val_loss: 0.8206 - val_acc: 0.6643\n",
      "Epoch 55/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.7843 - acc: 0.6758 - val_loss: 0.8198 - val_acc: 0.6714\n",
      "Epoch 56/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.7827 - acc: 0.6674 - val_loss: 0.7922 - val_acc: 0.6643\n",
      "Epoch 57/100\n",
      "2360/2360 [==============================] - 11s - loss: 0.7853 - acc: 0.6661 - val_loss: 0.8080 - val_acc: 0.6643\n",
      "Epoch 58/100\n",
      "2360/2360 [==============================] - 10s - loss: 0.7786 - acc: 0.6661 - val_loss: 0.8363 - val_acc: 0.6357\n",
      "Epoch 59/100\n",
      "2360/2360 [==============================] - 12s - loss: 0.7646 - acc: 0.6720 - val_loss: 0.7994 - val_acc: 0.6500\n",
      "Epoch 60/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7742 - acc: 0.6788 - val_loss: 0.8416 - val_acc: 0.6679\n",
      "Epoch 61/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7856 - acc: 0.6669 - val_loss: 0.7996 - val_acc: 0.6714\n",
      "Epoch 62/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7739 - acc: 0.6742 - val_loss: 0.7872 - val_acc: 0.6607\n",
      "Epoch 63/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7786 - acc: 0.6729 - val_loss: 0.7803 - val_acc: 0.6786\n",
      "Epoch 64/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7751 - acc: 0.6695 - val_loss: 0.7929 - val_acc: 0.6536\n",
      "Epoch 65/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7833 - acc: 0.6631 - val_loss: 0.8051 - val_acc: 0.6643\n",
      "Epoch 66/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7588 - acc: 0.6801 - val_loss: 0.7849 - val_acc: 0.6893\n",
      "Epoch 67/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7774 - acc: 0.6742 - val_loss: 0.8117 - val_acc: 0.6679\n",
      "Epoch 68/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7459 - acc: 0.6903 - val_loss: 0.8767 - val_acc: 0.6250\n",
      "Epoch 69/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7569 - acc: 0.6903 - val_loss: 0.8380 - val_acc: 0.6536\n",
      "Epoch 70/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7419 - acc: 0.6941 - val_loss: 0.7980 - val_acc: 0.6786\n",
      "Epoch 71/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7464 - acc: 0.6801 - val_loss: 0.8228 - val_acc: 0.6393\n",
      "Epoch 72/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7499 - acc: 0.6835 - val_loss: 0.7987 - val_acc: 0.6714\n",
      "Epoch 73/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7449 - acc: 0.6856 - val_loss: 0.7783 - val_acc: 0.6786\n",
      "Epoch 74/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7587 - acc: 0.6809 - val_loss: 0.8479 - val_acc: 0.6214\n",
      "Epoch 75/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7508 - acc: 0.6881 - val_loss: 0.8340 - val_acc: 0.6571\n",
      "Epoch 76/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7576 - acc: 0.6767 - val_loss: 0.8672 - val_acc: 0.6357\n",
      "Epoch 77/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7568 - acc: 0.6856 - val_loss: 0.8132 - val_acc: 0.6607\n",
      "Epoch 78/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7407 - acc: 0.6894 - val_loss: 0.8413 - val_acc: 0.6679\n",
      "Epoch 79/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7341 - acc: 0.6894 - val_loss: 0.8455 - val_acc: 0.6607\n",
      "Epoch 80/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7331 - acc: 0.6877 - val_loss: 0.8720 - val_acc: 0.6357\n",
      "Epoch 81/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7207 - acc: 0.6987 - val_loss: 0.8135 - val_acc: 0.6571\n",
      "Epoch 82/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7344 - acc: 0.6890 - val_loss: 0.7933 - val_acc: 0.6929\n",
      "Epoch 83/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7521 - acc: 0.6809 - val_loss: 0.7991 - val_acc: 0.6679\n",
      "Epoch 84/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7381 - acc: 0.6847 - val_loss: 0.8469 - val_acc: 0.6571\n",
      "Epoch 85/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7536 - acc: 0.6822 - val_loss: 0.8095 - val_acc: 0.6357\n",
      "Epoch 86/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7187 - acc: 0.7017 - val_loss: 0.8324 - val_acc: 0.6500\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2360/2360 [==============================] - 13s - loss: 0.7405 - acc: 0.6797 - val_loss: 0.7805 - val_acc: 0.6643\n",
      "Epoch 88/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7214 - acc: 0.6835 - val_loss: 0.8261 - val_acc: 0.6571\n",
      "Epoch 89/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7238 - acc: 0.6928 - val_loss: 0.8572 - val_acc: 0.6643\n",
      "Epoch 90/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7258 - acc: 0.6915 - val_loss: 0.8327 - val_acc: 0.6500\n",
      "Epoch 91/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7266 - acc: 0.6979 - val_loss: 0.8235 - val_acc: 0.6607\n",
      "Epoch 92/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7300 - acc: 0.6983 - val_loss: 0.8299 - val_acc: 0.6464\n",
      "Epoch 93/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7304 - acc: 0.6890 - val_loss: 0.8325 - val_acc: 0.6679\n",
      "Epoch 94/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7195 - acc: 0.6911 - val_loss: 0.8425 - val_acc: 0.6714\n",
      "Epoch 95/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7165 - acc: 0.7000 - val_loss: 0.8479 - val_acc: 0.6321\n",
      "Epoch 96/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.6981 - acc: 0.7085 - val_loss: 0.8563 - val_acc: 0.6464\n",
      "Epoch 97/100\n",
      "2360/2360 [==============================] - 13s - loss: 0.7176 - acc: 0.6992 - val_loss: 0.8507 - val_acc: 0.6357\n",
      "Params:  time_steps: 8   n_neurons: 200  batch_size:  20  n_epoch:  100 layers  data_dim  8 5 \n",
      " Time cost:  1584.5590171813965\n",
      "Add Dropout between every layer:  0.3\n",
      "Avg. Train ACC =  0.638568059064\n",
      "Avg. Test ACC =  0.649079530312\n",
      "Trading ratio:  0.0711743772241993\n",
      "max_drawdown:  -0.0320320918025\n",
      "ROR:  12.1358445591\n",
      "mean 0.170497\n",
      "std 0.184634\n",
      "max 0.550594\n",
      "min -0.0320321\n",
      " buy & hold:  0.445769603412\n",
      "sharpe_ratio 0.923435\n",
      "return sum:  5290.97 return ratio:  2.14488059381 Alpha:  1.6991109904\n",
      "prediction model input data shape:  (2663, 8)\n",
      "train sample:  2360 data_dim 8\n",
      "training size:  2360\n",
      "testing size:  280\n",
      "Train on 2360 samples, validate on 280 samples\n",
      "Epoch 1/100\n",
      "2360/2360 [==============================] - 25s - loss: 1.1172 - acc: 0.4178 - val_loss: 1.0456 - val_acc: 0.5321\n",
      "Epoch 2/100\n",
      "2360/2360 [==============================] - 23s - loss: 1.0786 - acc: 0.4415 - val_loss: 1.0315 - val_acc: 0.5321\n",
      "Epoch 3/100\n",
      "2360/2360 [==============================] - 23s - loss: 1.0705 - acc: 0.4347 - val_loss: 1.0251 - val_acc: 0.5321\n",
      "Epoch 4/100\n",
      "2360/2360 [==============================] - 23s - loss: 1.0667 - acc: 0.4419 - val_loss: 1.0175 - val_acc: 0.5321\n",
      "Epoch 5/100\n",
      "2360/2360 [==============================] - 23s - loss: 1.0607 - acc: 0.4610 - val_loss: 1.0035 - val_acc: 0.5321\n",
      "Epoch 6/100\n",
      "2360/2360 [==============================] - 23s - loss: 1.0516 - acc: 0.4725 - val_loss: 0.9638 - val_acc: 0.5464\n",
      "Epoch 7/100\n",
      "2360/2360 [==============================] - 23s - loss: 1.0265 - acc: 0.5178 - val_loss: 0.9217 - val_acc: 0.5536\n",
      "Epoch 8/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.9755 - acc: 0.5691 - val_loss: 0.8284 - val_acc: 0.6321\n",
      "Epoch 9/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.9464 - acc: 0.5839 - val_loss: 0.8527 - val_acc: 0.5929\n",
      "Epoch 10/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.9306 - acc: 0.5958 - val_loss: 0.8599 - val_acc: 0.6214\n",
      "Epoch 11/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.9139 - acc: 0.6042 - val_loss: 0.8244 - val_acc: 0.6643\n",
      "Epoch 12/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.9067 - acc: 0.6102 - val_loss: 0.9834 - val_acc: 0.5500\n",
      "Epoch 13/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.9126 - acc: 0.6059 - val_loss: 0.8367 - val_acc: 0.6464\n",
      "Epoch 14/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.9039 - acc: 0.6072 - val_loss: 0.7975 - val_acc: 0.6679\n",
      "Epoch 15/100\n",
      "2360/2360 [==============================] - 19s - loss: 0.8848 - acc: 0.6191 - val_loss: 0.8097 - val_acc: 0.6714\n",
      "Epoch 16/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.8811 - acc: 0.6216 - val_loss: 0.8279 - val_acc: 0.6929\n",
      "Epoch 17/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.8854 - acc: 0.6208 - val_loss: 0.8038 - val_acc: 0.6393\n",
      "Epoch 18/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.8610 - acc: 0.6352 - val_loss: 0.8420 - val_acc: 0.6250\n",
      "Epoch 19/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.8676 - acc: 0.6275 - val_loss: 0.8523 - val_acc: 0.6643\n",
      "Epoch 20/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.8625 - acc: 0.6343 - val_loss: 0.7853 - val_acc: 0.6857\n",
      "Epoch 21/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.8555 - acc: 0.6347 - val_loss: 0.7903 - val_acc: 0.6750\n",
      "Epoch 22/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.8633 - acc: 0.6288 - val_loss: 0.8041 - val_acc: 0.6714\n",
      "Epoch 23/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.8566 - acc: 0.6347 - val_loss: 0.8269 - val_acc: 0.6607\n",
      "Epoch 24/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.8413 - acc: 0.6398 - val_loss: 0.8031 - val_acc: 0.6571\n",
      "Epoch 25/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.8430 - acc: 0.6458 - val_loss: 0.7822 - val_acc: 0.6607\n",
      "Epoch 26/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.8375 - acc: 0.6398 - val_loss: 0.8092 - val_acc: 0.6643\n",
      "Epoch 27/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.8335 - acc: 0.6428 - val_loss: 0.7713 - val_acc: 0.6964\n",
      "Epoch 28/100\n",
      "2360/2360 [==============================] - 21s - loss: 0.8268 - acc: 0.6419 - val_loss: 0.7875 - val_acc: 0.6750\n",
      "Epoch 29/100\n",
      "2360/2360 [==============================] - 21s - loss: 0.8331 - acc: 0.6339 - val_loss: 0.8504 - val_acc: 0.6571\n",
      "Epoch 30/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.8389 - acc: 0.6487 - val_loss: 0.8425 - val_acc: 0.6429\n",
      "Epoch 31/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.8252 - acc: 0.6453 - val_loss: 0.8025 - val_acc: 0.6821\n",
      "Epoch 32/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.8244 - acc: 0.6521 - val_loss: 0.7641 - val_acc: 0.7214\n",
      "Epoch 33/100\n",
      "2360/2360 [==============================] - 21s - loss: 0.8288 - acc: 0.6462 - val_loss: 0.8703 - val_acc: 0.6286\n",
      "Epoch 34/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.8213 - acc: 0.6487 - val_loss: 0.8752 - val_acc: 0.6250\n",
      "Epoch 35/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.8218 - acc: 0.6483 - val_loss: 0.8134 - val_acc: 0.6714\n",
      "Epoch 36/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.8197 - acc: 0.6517 - val_loss: 0.8099 - val_acc: 0.6607\n",
      "Epoch 37/100\n",
      "2360/2360 [==============================] - 24s - loss: 0.8030 - acc: 0.6538 - val_loss: 0.8476 - val_acc: 0.5964\n",
      "Epoch 38/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.8108 - acc: 0.6542 - val_loss: 0.8240 - val_acc: 0.6536\n",
      "Epoch 39/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.8025 - acc: 0.6758 - val_loss: 0.8872 - val_acc: 0.5750\n",
      "Epoch 40/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.8000 - acc: 0.6648 - val_loss: 0.8302 - val_acc: 0.6643\n",
      "Epoch 41/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.8014 - acc: 0.6564 - val_loss: 0.8086 - val_acc: 0.6536\n",
      "Epoch 42/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.8006 - acc: 0.6572 - val_loss: 0.8198 - val_acc: 0.6429\n",
      "Epoch 43/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.7939 - acc: 0.6703 - val_loss: 0.8216 - val_acc: 0.6571\n",
      "Epoch 44/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.7927 - acc: 0.6627 - val_loss: 0.8197 - val_acc: 0.6750\n",
      "Epoch 45/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.8001 - acc: 0.6589 - val_loss: 0.8196 - val_acc: 0.6643\n",
      "Epoch 46/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.7862 - acc: 0.6716 - val_loss: 0.8889 - val_acc: 0.6321\n",
      "Epoch 47/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.7873 - acc: 0.6602 - val_loss: 0.8204 - val_acc: 0.6571\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2360/2360 [==============================] - 23s - loss: 0.7758 - acc: 0.6703 - val_loss: 0.8162 - val_acc: 0.6786\n",
      "Epoch 49/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.7838 - acc: 0.6716 - val_loss: 0.8351 - val_acc: 0.6643\n",
      "Epoch 50/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.7745 - acc: 0.6775 - val_loss: 0.8517 - val_acc: 0.6393\n",
      "Epoch 51/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.7813 - acc: 0.6737 - val_loss: 0.8876 - val_acc: 0.5964\n",
      "Epoch 52/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.7713 - acc: 0.6801 - val_loss: 0.8447 - val_acc: 0.5786\n",
      "Epoch 53/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.7566 - acc: 0.6746 - val_loss: 0.8427 - val_acc: 0.6607\n",
      "Epoch 54/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.7689 - acc: 0.6708 - val_loss: 0.8319 - val_acc: 0.6750\n",
      "Epoch 55/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.7635 - acc: 0.6712 - val_loss: 0.7998 - val_acc: 0.6643\n",
      "Epoch 56/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.7622 - acc: 0.6771 - val_loss: 0.8470 - val_acc: 0.5964\n",
      "Epoch 57/100\n",
      "2360/2360 [==============================] - 21s - loss: 0.7880 - acc: 0.6619 - val_loss: 0.8433 - val_acc: 0.6536\n",
      "Epoch 58/100\n",
      "2360/2360 [==============================] - 19s - loss: 0.7547 - acc: 0.6746 - val_loss: 0.8205 - val_acc: 0.6750\n",
      "Epoch 59/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.7423 - acc: 0.6839 - val_loss: 0.8642 - val_acc: 0.6036\n",
      "Epoch 60/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.7471 - acc: 0.6814 - val_loss: 0.8301 - val_acc: 0.6214\n",
      "Epoch 61/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.7495 - acc: 0.6826 - val_loss: 0.8389 - val_acc: 0.6429\n",
      "Epoch 62/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.7261 - acc: 0.6869 - val_loss: 0.8976 - val_acc: 0.6000\n",
      "Epoch 63/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.7805 - acc: 0.6674 - val_loss: 0.9342 - val_acc: 0.5821\n",
      "Epoch 64/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.7408 - acc: 0.6907 - val_loss: 0.9653 - val_acc: 0.5964\n",
      "Epoch 65/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.7403 - acc: 0.6890 - val_loss: 0.9642 - val_acc: 0.5964\n",
      "Epoch 66/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.7310 - acc: 0.6775 - val_loss: 0.9069 - val_acc: 0.5964\n",
      "Epoch 67/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.7366 - acc: 0.6843 - val_loss: 0.9688 - val_acc: 0.5393\n",
      "Epoch 68/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.7326 - acc: 0.6975 - val_loss: 0.8536 - val_acc: 0.6536\n",
      "Epoch 69/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.7164 - acc: 0.7008 - val_loss: 0.9278 - val_acc: 0.5750\n",
      "Epoch 70/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.7257 - acc: 0.6873 - val_loss: 0.9858 - val_acc: 0.5429\n",
      "Epoch 71/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.7207 - acc: 0.6915 - val_loss: 1.0325 - val_acc: 0.5286\n",
      "Epoch 72/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.7353 - acc: 0.6737 - val_loss: 0.8954 - val_acc: 0.5464\n",
      "Epoch 73/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.7170 - acc: 0.7008 - val_loss: 0.8598 - val_acc: 0.6357\n",
      "Epoch 74/100\n",
      "2360/2360 [==============================] - 20s - loss: 0.7283 - acc: 0.6966 - val_loss: 0.9024 - val_acc: 0.5571\n",
      "Epoch 75/100\n",
      "2360/2360 [==============================] - 19s - loss: 0.7048 - acc: 0.7093 - val_loss: 0.8447 - val_acc: 0.5929\n",
      "Epoch 76/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.7152 - acc: 0.6962 - val_loss: 0.8687 - val_acc: 0.6036\n",
      "Epoch 77/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.7193 - acc: 0.6932 - val_loss: 0.9423 - val_acc: 0.5250\n",
      "Epoch 78/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.7150 - acc: 0.7013 - val_loss: 0.9394 - val_acc: 0.5786\n",
      "Epoch 79/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.6880 - acc: 0.7030 - val_loss: 0.9647 - val_acc: 0.5357\n",
      "Epoch 80/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.6873 - acc: 0.7068 - val_loss: 0.9196 - val_acc: 0.5571\n",
      "Epoch 81/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.7056 - acc: 0.7072 - val_loss: 1.0014 - val_acc: 0.5214\n",
      "Epoch 82/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.6979 - acc: 0.7000 - val_loss: 0.9878 - val_acc: 0.5643\n",
      "Epoch 83/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.6877 - acc: 0.7089 - val_loss: 1.0179 - val_acc: 0.5214\n",
      "Epoch 84/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.6950 - acc: 0.7013 - val_loss: 0.9879 - val_acc: 0.4893\n",
      "Epoch 85/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.6977 - acc: 0.7059 - val_loss: 0.9050 - val_acc: 0.5393\n",
      "Epoch 86/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.6840 - acc: 0.7136 - val_loss: 0.9291 - val_acc: 0.5036\n",
      "Epoch 87/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.6800 - acc: 0.7165 - val_loss: 1.0193 - val_acc: 0.5179\n",
      "Epoch 88/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.6850 - acc: 0.7085 - val_loss: 0.9045 - val_acc: 0.6071\n",
      "Epoch 89/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.6618 - acc: 0.7203 - val_loss: 0.9667 - val_acc: 0.5214\n",
      "Epoch 90/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.6685 - acc: 0.7110 - val_loss: 1.0272 - val_acc: 0.4893\n",
      "Epoch 91/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.6530 - acc: 0.7246 - val_loss: 1.0515 - val_acc: 0.4821\n",
      "Epoch 92/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.6484 - acc: 0.7242 - val_loss: 1.1511 - val_acc: 0.4107\n",
      "Epoch 93/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.6593 - acc: 0.7233 - val_loss: 1.0932 - val_acc: 0.4893\n",
      "Epoch 94/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.6372 - acc: 0.7301 - val_loss: 1.0818 - val_acc: 0.5107\n",
      "Epoch 95/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.6296 - acc: 0.7419 - val_loss: 1.0692 - val_acc: 0.5321\n",
      "Epoch 96/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.6336 - acc: 0.7403 - val_loss: 1.0876 - val_acc: 0.5000\n",
      "Epoch 97/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.6406 - acc: 0.7352 - val_loss: 0.9877 - val_acc: 0.5643\n",
      "Epoch 98/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.6204 - acc: 0.7445 - val_loss: 1.1119 - val_acc: 0.4821\n",
      "Epoch 99/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.6275 - acc: 0.7453 - val_loss: 1.0860 - val_acc: 0.5321\n",
      "Epoch 100/100\n",
      "2360/2360 [==============================] - 23s - loss: 0.6235 - acc: 0.7381 - val_loss: 1.2672 - val_acc: 0.4143\n",
      "Params:  time_steps: 16   n_neurons: 200  batch_size:  20  n_epoch:  100 layers  data_dim  8 5 \n",
      " Time cost:  3855.171511888504\n",
      "Add Dropout between every layer:  0.3\n",
      "Avg. Train ACC =  0.657372882614\n",
      "Avg. Test ACC =  0.599214285101\n",
      "Trading ratio:  0.07829181494661921\n",
      "max_drawdown:  -0.00954495175275\n",
      "ROR:  13.6973588463\n",
      "mean 0.141933\n",
      "std 0.184568\n",
      "max 0.549667\n",
      "min -0.00954495\n",
      " buy & hold:  0.497982337181\n",
      "sharpe_ratio 0.768998\n",
      "return sum:  4737.25 return ratio:  1.95864204677 Alpha:  1.46065970959\n",
      "prediction model input data shape:  (2663, 8)\n",
      "train sample:  2340 data_dim 8\n",
      "training size:  2340\n",
      "testing size:  280\n",
      "Train on 2340 samples, validate on 280 samples\n",
      "Epoch 1/100\n",
      "2340/2340 [==============================] - 46s - loss: 1.1201 - acc: 0.4248 - val_loss: 1.0410 - val_acc: 0.5321\n",
      "Epoch 2/100\n",
      "2340/2340 [==============================] - 44s - loss: 1.0833 - acc: 0.4269 - val_loss: 1.0278 - val_acc: 0.5321\n",
      "Epoch 3/100\n",
      "2340/2340 [==============================] - 44s - loss: 1.0837 - acc: 0.4214 - val_loss: 1.0274 - val_acc: 0.5321\n",
      "Epoch 4/100\n",
      "2340/2340 [==============================] - 43s - loss: 1.0757 - acc: 0.4449 - val_loss: 1.0167 - val_acc: 0.5321\n",
      "Epoch 5/100\n",
      "2340/2340 [==============================] - 44s - loss: 1.0670 - acc: 0.4521 - val_loss: 0.9992 - val_acc: 0.5321\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2340/2340 [==============================] - 44s - loss: 1.0617 - acc: 0.4705 - val_loss: 0.9807 - val_acc: 0.5393\n",
      "Epoch 7/100\n",
      "2340/2340 [==============================] - 44s - loss: 1.0427 - acc: 0.5098 - val_loss: 0.9368 - val_acc: 0.6107\n",
      "Epoch 8/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.9997 - acc: 0.5483 - val_loss: 0.8532 - val_acc: 0.6607\n",
      "Epoch 9/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.9603 - acc: 0.5752 - val_loss: 0.8157 - val_acc: 0.6893\n",
      "Epoch 10/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.9457 - acc: 0.5906 - val_loss: 0.8416 - val_acc: 0.6071\n",
      "Epoch 11/100\n",
      "2340/2340 [==============================] - 43s - loss: 0.9261 - acc: 0.6009 - val_loss: 0.8400 - val_acc: 0.6607\n",
      "Epoch 12/100\n",
      "2340/2340 [==============================] - 43s - loss: 0.9141 - acc: 0.6231 - val_loss: 0.8476 - val_acc: 0.6107\n",
      "Epoch 13/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.9129 - acc: 0.6013 - val_loss: 0.8242 - val_acc: 0.6750\n",
      "Epoch 14/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.8898 - acc: 0.6120 - val_loss: 0.8327 - val_acc: 0.6643\n",
      "Epoch 15/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.8931 - acc: 0.6120 - val_loss: 0.8204 - val_acc: 0.6000\n",
      "Epoch 16/100\n",
      "2340/2340 [==============================] - 43s - loss: 0.8815 - acc: 0.6286 - val_loss: 0.8163 - val_acc: 0.6857\n",
      "Epoch 17/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.8943 - acc: 0.6201 - val_loss: 0.7891 - val_acc: 0.6857\n",
      "Epoch 18/100\n",
      "2340/2340 [==============================] - 39s - loss: 0.8552 - acc: 0.6312 - val_loss: 0.8301 - val_acc: 0.6143\n",
      "Epoch 19/100\n",
      "2340/2340 [==============================] - 38s - loss: 0.8682 - acc: 0.6295 - val_loss: 0.7961 - val_acc: 0.6714\n",
      "Epoch 20/100\n",
      "2340/2340 [==============================] - 38s - loss: 0.8720 - acc: 0.6248 - val_loss: 0.8012 - val_acc: 0.6357\n",
      "Epoch 21/100\n",
      "2340/2340 [==============================] - 38s - loss: 0.8665 - acc: 0.6316 - val_loss: 0.7864 - val_acc: 0.6500\n",
      "Epoch 22/100\n",
      "2340/2340 [==============================] - 38s - loss: 0.8630 - acc: 0.6338 - val_loss: 0.7592 - val_acc: 0.6893\n",
      "Epoch 23/100\n",
      "2340/2340 [==============================] - 38s - loss: 0.8592 - acc: 0.6342 - val_loss: 0.7615 - val_acc: 0.6964\n",
      "Epoch 24/100\n",
      "2340/2340 [==============================] - 38s - loss: 0.8371 - acc: 0.6350 - val_loss: 0.7670 - val_acc: 0.6679\n",
      "Epoch 25/100\n",
      "2340/2340 [==============================] - 38s - loss: 0.8492 - acc: 0.6402 - val_loss: 0.8073 - val_acc: 0.6429\n",
      "Epoch 26/100\n",
      "2340/2340 [==============================] - 38s - loss: 0.8410 - acc: 0.6363 - val_loss: 0.7992 - val_acc: 0.6571\n",
      "Epoch 27/100\n",
      "2340/2340 [==============================] - 38s - loss: 0.8475 - acc: 0.6406 - val_loss: 0.7732 - val_acc: 0.6714\n",
      "Epoch 28/100\n",
      "2340/2340 [==============================] - 38s - loss: 0.8415 - acc: 0.6402 - val_loss: 0.7611 - val_acc: 0.6714\n",
      "Epoch 29/100\n",
      "2340/2340 [==============================] - 43s - loss: 0.8165 - acc: 0.6500 - val_loss: 0.8095 - val_acc: 0.6536\n",
      "Epoch 30/100\n",
      "2340/2340 [==============================] - 43s - loss: 0.8319 - acc: 0.6483 - val_loss: 0.7900 - val_acc: 0.6714\n",
      "Epoch 31/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.8169 - acc: 0.6470 - val_loss: 0.7759 - val_acc: 0.6821\n",
      "Epoch 32/100\n",
      "2340/2340 [==============================] - 43s - loss: 0.8235 - acc: 0.6402 - val_loss: 0.7821 - val_acc: 0.6821\n",
      "Epoch 33/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.8204 - acc: 0.6457 - val_loss: 0.7896 - val_acc: 0.6821\n",
      "Epoch 34/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.8102 - acc: 0.6500 - val_loss: 0.7951 - val_acc: 0.6500\n",
      "Epoch 35/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.8223 - acc: 0.6556 - val_loss: 0.8039 - val_acc: 0.6679\n",
      "Epoch 36/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.8210 - acc: 0.6487 - val_loss: 0.7889 - val_acc: 0.6929\n",
      "Epoch 37/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.8065 - acc: 0.6564 - val_loss: 0.7933 - val_acc: 0.6821\n",
      "Epoch 38/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.8023 - acc: 0.6577 - val_loss: 0.7906 - val_acc: 0.6821\n",
      "Epoch 39/100\n",
      "2340/2340 [==============================] - 43s - loss: 0.7926 - acc: 0.6594 - val_loss: 0.8226 - val_acc: 0.6643\n",
      "Epoch 40/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.7855 - acc: 0.6671 - val_loss: 0.7977 - val_acc: 0.6500\n",
      "Epoch 41/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.7973 - acc: 0.6577 - val_loss: 0.7918 - val_acc: 0.6750\n",
      "Epoch 42/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.7957 - acc: 0.6585 - val_loss: 0.7965 - val_acc: 0.6786\n",
      "Epoch 43/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.7807 - acc: 0.6667 - val_loss: 0.8648 - val_acc: 0.6429\n",
      "Epoch 44/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.7755 - acc: 0.6744 - val_loss: 0.8432 - val_acc: 0.6714\n",
      "Epoch 45/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.7870 - acc: 0.6662 - val_loss: 0.9268 - val_acc: 0.6036\n",
      "Epoch 46/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.7731 - acc: 0.6679 - val_loss: 0.7928 - val_acc: 0.6786\n",
      "Epoch 47/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.7778 - acc: 0.6624 - val_loss: 0.8188 - val_acc: 0.6821\n",
      "Epoch 48/100\n",
      "2340/2340 [==============================] - 38s - loss: 0.7630 - acc: 0.6769 - val_loss: 0.8523 - val_acc: 0.6500\n",
      "Epoch 49/100\n",
      "2340/2340 [==============================] - 24s - loss: 0.7624 - acc: 0.6714 - val_loss: 0.8671 - val_acc: 0.6607\n",
      "Epoch 50/100\n",
      "2340/2340 [==============================] - 24s - loss: 0.7742 - acc: 0.6705 - val_loss: 0.7930 - val_acc: 0.6643\n",
      "Epoch 51/100\n",
      "2340/2340 [==============================] - 27s - loss: 0.7419 - acc: 0.6850 - val_loss: 0.8735 - val_acc: 0.6679\n",
      "Epoch 52/100\n",
      "2340/2340 [==============================] - 36s - loss: 0.7463 - acc: 0.6825 - val_loss: 0.8754 - val_acc: 0.6679\n",
      "Epoch 53/100\n",
      "2340/2340 [==============================] - 38s - loss: 0.7515 - acc: 0.6838 - val_loss: 0.8442 - val_acc: 0.6750\n",
      "Epoch 54/100\n",
      "2340/2340 [==============================] - 37s - loss: 0.7488 - acc: 0.6829 - val_loss: 0.8567 - val_acc: 0.6500\n",
      "Epoch 55/100\n",
      "2340/2340 [==============================] - 37s - loss: 0.7430 - acc: 0.6799 - val_loss: 0.8591 - val_acc: 0.6393\n",
      "Epoch 56/100\n",
      "2340/2340 [==============================] - 37s - loss: 0.7476 - acc: 0.6859 - val_loss: 0.8673 - val_acc: 0.6607\n",
      "Epoch 57/100\n",
      "2340/2340 [==============================] - 37s - loss: 0.7358 - acc: 0.6821 - val_loss: 0.8549 - val_acc: 0.6679\n",
      "Epoch 58/100\n",
      "2340/2340 [==============================] - 37s - loss: 0.7137 - acc: 0.6953 - val_loss: 0.8415 - val_acc: 0.6500\n",
      "Epoch 59/100\n",
      "2340/2340 [==============================] - 37s - loss: 0.7190 - acc: 0.6915 - val_loss: 0.8782 - val_acc: 0.6679\n",
      "Epoch 60/100\n",
      "2340/2340 [==============================] - 36s - loss: 0.6999 - acc: 0.7090 - val_loss: 0.8636 - val_acc: 0.6571\n",
      "Epoch 61/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.6822 - acc: 0.7209 - val_loss: 0.8571 - val_acc: 0.6607\n",
      "Epoch 62/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.6973 - acc: 0.7111 - val_loss: 0.9000 - val_acc: 0.6643\n",
      "Epoch 63/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.6939 - acc: 0.7107 - val_loss: 0.9037 - val_acc: 0.6321\n",
      "Epoch 64/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.6936 - acc: 0.7051 - val_loss: 0.9369 - val_acc: 0.6286\n",
      "Epoch 65/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.6714 - acc: 0.7295 - val_loss: 0.9888 - val_acc: 0.5964\n",
      "Epoch 66/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.6873 - acc: 0.7214 - val_loss: 0.7926 - val_acc: 0.6714\n",
      "Epoch 67/100\n",
      "2340/2340 [==============================] - 45s - loss: 0.7012 - acc: 0.7004 - val_loss: 0.8609 - val_acc: 0.6571\n",
      "Epoch 68/100\n",
      "2340/2340 [==============================] - 43s - loss: 0.6481 - acc: 0.7252 - val_loss: 0.8750 - val_acc: 0.6286\n",
      "Epoch 69/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.6888 - acc: 0.7060 - val_loss: 0.9030 - val_acc: 0.6357\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2340/2340 [==============================] - 44s - loss: 0.6610 - acc: 0.7179 - val_loss: 0.9200 - val_acc: 0.6321\n",
      "Epoch 71/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.6642 - acc: 0.7265 - val_loss: 0.9436 - val_acc: 0.6036\n",
      "Epoch 72/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.6505 - acc: 0.7376 - val_loss: 0.9596 - val_acc: 0.6500\n",
      "Epoch 73/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.6317 - acc: 0.7436 - val_loss: 0.9756 - val_acc: 0.6000\n",
      "Epoch 74/100\n",
      "2340/2340 [==============================] - 43s - loss: 0.6128 - acc: 0.7496 - val_loss: 0.9852 - val_acc: 0.5786\n",
      "Epoch 75/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.6009 - acc: 0.7530 - val_loss: 0.9938 - val_acc: 0.6071\n",
      "Epoch 76/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.5999 - acc: 0.7530 - val_loss: 1.0663 - val_acc: 0.6107\n",
      "Epoch 77/100\n",
      "2340/2340 [==============================] - 43s - loss: 0.5935 - acc: 0.7556 - val_loss: 1.0162 - val_acc: 0.5607\n",
      "Epoch 78/100\n",
      "2340/2340 [==============================] - 43s - loss: 0.5837 - acc: 0.7624 - val_loss: 1.0828 - val_acc: 0.5750\n",
      "Epoch 79/100\n",
      "2340/2340 [==============================] - 43s - loss: 0.5790 - acc: 0.7654 - val_loss: 1.0065 - val_acc: 0.6143\n",
      "Epoch 80/100\n",
      "2340/2340 [==============================] - 43s - loss: 0.5695 - acc: 0.7662 - val_loss: 1.0290 - val_acc: 0.6143\n",
      "Epoch 81/100\n",
      "2340/2340 [==============================] - 44s - loss: 0.5855 - acc: 0.7500 - val_loss: 1.0051 - val_acc: 0.5857\n",
      "Epoch 82/100\n",
      "2340/2340 [==============================] - 43s - loss: 0.5558 - acc: 0.7769 - val_loss: 1.0123 - val_acc: 0.6536\n",
      "Epoch 83/100\n",
      "2340/2340 [==============================] - 42s - loss: 0.5497 - acc: 0.7722 - val_loss: 1.0149 - val_acc: 0.6143\n",
      "Epoch 84/100\n",
      "2340/2340 [==============================] - 36s - loss: 0.5370 - acc: 0.7786 - val_loss: 1.0862 - val_acc: 0.5964\n",
      "Epoch 85/100\n",
      "2340/2340 [==============================] - 37s - loss: 0.5212 - acc: 0.7902 - val_loss: 1.1537 - val_acc: 0.5536\n",
      "Epoch 86/100\n",
      "2340/2340 [==============================] - 38s - loss: 0.5345 - acc: 0.7859 - val_loss: 1.0730 - val_acc: 0.5464\n",
      "Epoch 87/100\n",
      "2340/2340 [==============================] - 38s - loss: 0.5105 - acc: 0.7944 - val_loss: 1.1063 - val_acc: 0.5679\n",
      "Epoch 88/100\n",
      "2340/2340 [==============================] - 38s - loss: 0.5087 - acc: 0.7940 - val_loss: 1.0494 - val_acc: 0.6214\n",
      "Epoch 89/100\n",
      "2340/2340 [==============================] - 37s - loss: 0.4984 - acc: 0.7915 - val_loss: 1.1135 - val_acc: 0.5821\n",
      "Epoch 90/100\n",
      "2340/2340 [==============================] - 38s - loss: 0.4973 - acc: 0.7991 - val_loss: 1.1114 - val_acc: 0.5714\n",
      "Epoch 91/100\n",
      "2340/2340 [==============================] - 38s - loss: 0.5014 - acc: 0.7987 - val_loss: 1.0621 - val_acc: 0.6036\n",
      "Epoch 92/100\n",
      "2340/2340 [==============================] - 37s - loss: 0.4740 - acc: 0.8111 - val_loss: 1.0502 - val_acc: 0.5929\n",
      "Epoch 93/100\n",
      "2340/2340 [==============================] - 43s - loss: 0.4797 - acc: 0.8145 - val_loss: 1.1175 - val_acc: 0.5821\n",
      "Params:  time_steps: 32   n_neurons: 200  batch_size:  20  n_epoch:  100 layers  data_dim  8 5 \n",
      " Time cost:  7728.653725385666\n",
      "Add Dropout between every layer:  0.3\n",
      "Avg. Train ACC =  0.67133995055\n",
      "Avg. Test ACC =  0.63421659266\n",
      "Trading ratio:  0.08540925266903915\n",
      "max_drawdown:  0.0\n",
      "ROR:  15.0643961708\n",
      "mean 0.144008\n",
      "std 0.144712\n",
      "max 0.468198\n",
      "min 0.025218\n",
      " buy & hold:  0.504495990836\n",
      "sharpe_ratio 0.99513\n",
      "return sum:  5551.25 return ratio:  2.27100720013 Alpha:  1.76651120929\n",
      "prediction model input data shape:  (2663, 8)\n",
      "train sample:  2320 data_dim 8\n",
      "training size:  2320\n",
      "testing size:  260\n",
      "Train on 2320 samples, validate on 260 samples\n",
      "Epoch 1/100\n",
      "2320/2320 [==============================] - 87s - loss: 1.1009 - acc: 0.4237 - val_loss: 1.0457 - val_acc: 0.5346\n",
      "Epoch 2/100\n",
      "2320/2320 [==============================] - 85s - loss: 1.0743 - acc: 0.4405 - val_loss: 1.0686 - val_acc: 0.5346\n",
      "Epoch 3/100\n",
      "2320/2320 [==============================] - 84s - loss: 1.0725 - acc: 0.4375 - val_loss: 1.0333 - val_acc: 0.5346\n",
      "Epoch 4/100\n",
      "2320/2320 [==============================] - 84s - loss: 1.0668 - acc: 0.4517 - val_loss: 1.0563 - val_acc: 0.5346\n",
      "Epoch 5/100\n",
      "2320/2320 [==============================] - 84s - loss: 1.0648 - acc: 0.4517 - val_loss: 1.0136 - val_acc: 0.5269\n",
      "Epoch 6/100\n",
      "2320/2320 [==============================] - 84s - loss: 1.0562 - acc: 0.4793 - val_loss: 0.9975 - val_acc: 0.5385\n",
      "Epoch 7/100\n",
      "2320/2320 [==============================] - 84s - loss: 1.0417 - acc: 0.5060 - val_loss: 0.9309 - val_acc: 0.5846\n",
      "Epoch 8/100\n",
      "2320/2320 [==============================] - 84s - loss: 1.0024 - acc: 0.5625 - val_loss: 0.8503 - val_acc: 0.6385\n",
      "Epoch 9/100\n",
      "2320/2320 [==============================] - 84s - loss: 0.9520 - acc: 0.5918 - val_loss: 0.8723 - val_acc: 0.6346\n",
      "Epoch 10/100\n",
      "2320/2320 [==============================] - 84s - loss: 0.9310 - acc: 0.6039 - val_loss: 0.7692 - val_acc: 0.6885\n",
      "Epoch 11/100\n",
      "2320/2320 [==============================] - 84s - loss: 0.9093 - acc: 0.6168 - val_loss: 0.8596 - val_acc: 0.6077\n",
      "Epoch 12/100\n",
      "2320/2320 [==============================] - 85s - loss: 0.9302 - acc: 0.5931 - val_loss: 0.9032 - val_acc: 0.5885\n",
      "Epoch 13/100\n",
      "2320/2320 [==============================] - 84s - loss: 0.8977 - acc: 0.6134 - val_loss: 0.8687 - val_acc: 0.6038\n",
      "Epoch 14/100\n",
      "2320/2320 [==============================] - 84s - loss: 0.8975 - acc: 0.6211 - val_loss: 0.9095 - val_acc: 0.6000\n",
      "Epoch 15/100\n",
      "2320/2320 [==============================] - 73s - loss: 0.8965 - acc: 0.6211 - val_loss: 0.8096 - val_acc: 0.6462\n",
      "Epoch 16/100\n",
      "2320/2320 [==============================] - 73s - loss: 0.8945 - acc: 0.6272 - val_loss: 0.8376 - val_acc: 0.6308\n",
      "Epoch 17/100\n",
      "2320/2320 [==============================] - 73s - loss: 0.8863 - acc: 0.6246 - val_loss: 1.0285 - val_acc: 0.5769\n",
      "Epoch 18/100\n",
      "2320/2320 [==============================] - 72s - loss: 0.8750 - acc: 0.6302 - val_loss: 0.8237 - val_acc: 0.6615\n",
      "Epoch 19/100\n",
      "2320/2320 [==============================] - 73s - loss: 0.8746 - acc: 0.6194 - val_loss: 0.8095 - val_acc: 0.6769\n",
      "Epoch 20/100\n",
      "2320/2320 [==============================] - 85s - loss: 0.8806 - acc: 0.6224 - val_loss: 0.7732 - val_acc: 0.6731\n",
      "Epoch 21/100\n",
      "2320/2320 [==============================] - 85s - loss: 0.8638 - acc: 0.6384 - val_loss: 0.7866 - val_acc: 0.6846\n",
      "Epoch 22/100\n",
      "2320/2320 [==============================] - 85s - loss: 0.8628 - acc: 0.6358 - val_loss: 0.8980 - val_acc: 0.5731\n",
      "Epoch 23/100\n",
      "2320/2320 [==============================] - 84s - loss: 0.8609 - acc: 0.6366 - val_loss: 0.8421 - val_acc: 0.6269\n",
      "Epoch 24/100\n",
      "2320/2320 [==============================] - 84s - loss: 0.8730 - acc: 0.6263 - val_loss: 0.8381 - val_acc: 0.6154\n",
      "Epoch 25/100\n",
      "2320/2320 [==============================] - 85s - loss: 0.8661 - acc: 0.6220 - val_loss: 0.7827 - val_acc: 0.6885\n",
      "Epoch 26/100\n",
      "2320/2320 [==============================] - 86s - loss: 0.8624 - acc: 0.6319 - val_loss: 0.8250 - val_acc: 0.6231\n",
      "Epoch 27/100\n",
      "2320/2320 [==============================] - 85s - loss: 0.8466 - acc: 0.6397 - val_loss: 0.8270 - val_acc: 0.6577\n",
      "Epoch 28/100\n",
      "2320/2320 [==============================] - 85s - loss: 0.8538 - acc: 0.6375 - val_loss: 0.7675 - val_acc: 0.6885\n",
      "Epoch 29/100\n",
      "2320/2320 [==============================] - 85s - loss: 0.8554 - acc: 0.6341 - val_loss: 0.7852 - val_acc: 0.6731\n",
      "Epoch 30/100\n",
      "2320/2320 [==============================] - 85s - loss: 0.8521 - acc: 0.6397 - val_loss: 0.7481 - val_acc: 0.7038\n",
      "Epoch 31/100\n",
      "2320/2320 [==============================] - 85s - loss: 0.8382 - acc: 0.6457 - val_loss: 0.8546 - val_acc: 0.6269\n",
      "Epoch 32/100\n",
      "2320/2320 [==============================] - 85s - loss: 0.8379 - acc: 0.6397 - val_loss: 0.8199 - val_acc: 0.6308\n",
      "Epoch 33/100\n",
      "2320/2320 [==============================] - 85s - loss: 0.8322 - acc: 0.6509 - val_loss: 0.7991 - val_acc: 0.6577\n",
      "Epoch 34/100\n",
      "2320/2320 [==============================] - 84s - loss: 0.8258 - acc: 0.6405 - val_loss: 0.8066 - val_acc: 0.6577\n",
      "Epoch 35/100\n",
      "2320/2320 [==============================] - 83s - loss: 0.8332 - acc: 0.6427 - val_loss: 0.7632 - val_acc: 0.6654\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2320/2320 [==============================] - 71s - loss: 0.8353 - acc: 0.6457 - val_loss: 0.8504 - val_acc: 0.6115\n",
      "Epoch 37/100\n",
      "2320/2320 [==============================] - 74s - loss: 0.8243 - acc: 0.6444 - val_loss: 0.8994 - val_acc: 0.6731\n",
      "Epoch 38/100\n",
      "2320/2320 [==============================] - 73s - loss: 0.8283 - acc: 0.6478 - val_loss: 0.8238 - val_acc: 0.6269\n",
      "Epoch 39/100\n",
      "2320/2320 [==============================] - 72s - loss: 0.8174 - acc: 0.6539 - val_loss: 0.7884 - val_acc: 0.6385\n",
      "Epoch 40/100\n",
      "2320/2320 [==============================] - 76s - loss: 0.8280 - acc: 0.6466 - val_loss: 0.8099 - val_acc: 0.6808\n",
      "Epoch 41/100\n",
      "2320/2320 [==============================] - 85s - loss: 0.8259 - acc: 0.6470 - val_loss: 0.8784 - val_acc: 0.6192\n",
      "Epoch 42/100\n",
      "2320/2320 [==============================] - 85s - loss: 0.8213 - acc: 0.6496 - val_loss: 0.8600 - val_acc: 0.6269\n",
      "Epoch 43/100\n",
      "2320/2320 [==============================] - 85s - loss: 0.8085 - acc: 0.6608 - val_loss: 0.7812 - val_acc: 0.6923\n",
      "Epoch 44/100\n",
      "2320/2320 [==============================] - 84s - loss: 0.8169 - acc: 0.6599 - val_loss: 0.8267 - val_acc: 0.6538\n",
      "Epoch 45/100\n",
      "2320/2320 [==============================] - 84s - loss: 0.8277 - acc: 0.6444 - val_loss: 0.8152 - val_acc: 0.6615\n",
      "Epoch 46/100\n",
      "2320/2320 [==============================] - 85s - loss: 0.8062 - acc: 0.6547 - val_loss: 0.8356 - val_acc: 0.6000\n",
      "Epoch 47/100\n",
      "2320/2320 [==============================] - 85s - loss: 0.7990 - acc: 0.6591 - val_loss: 0.7980 - val_acc: 0.6231\n",
      "Epoch 48/100\n",
      "2320/2320 [==============================] - 80s - loss: 0.7916 - acc: 0.6569 - val_loss: 0.8223 - val_acc: 0.6308\n",
      "Epoch 49/100\n",
      "2320/2320 [==============================] - 46s - loss: 0.7985 - acc: 0.6534 - val_loss: 0.7776 - val_acc: 0.6615\n",
      "Epoch 50/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.7957 - acc: 0.6569 - val_loss: 0.8074 - val_acc: 0.6231\n",
      "Epoch 51/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.7979 - acc: 0.6599 - val_loss: 0.7944 - val_acc: 0.6500\n",
      "Epoch 52/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.7954 - acc: 0.6621 - val_loss: 0.7921 - val_acc: 0.6500\n",
      "Epoch 53/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.7814 - acc: 0.6638 - val_loss: 0.8250 - val_acc: 0.6038\n",
      "Epoch 54/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.7813 - acc: 0.6642 - val_loss: 0.7698 - val_acc: 0.6615\n",
      "Epoch 55/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.7876 - acc: 0.6659 - val_loss: 0.7900 - val_acc: 0.6500\n",
      "Epoch 56/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.7831 - acc: 0.6681 - val_loss: 0.8519 - val_acc: 0.6269\n",
      "Epoch 57/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.7615 - acc: 0.6724 - val_loss: 0.8018 - val_acc: 0.6577\n",
      "Epoch 58/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.7802 - acc: 0.6716 - val_loss: 0.7843 - val_acc: 0.6615\n",
      "Epoch 59/100\n",
      "2320/2320 [==============================] - 46s - loss: 0.7911 - acc: 0.6569 - val_loss: 0.8157 - val_acc: 0.6231\n",
      "Epoch 60/100\n",
      "2320/2320 [==============================] - 46s - loss: 0.7728 - acc: 0.6707 - val_loss: 0.7810 - val_acc: 0.6385\n",
      "Epoch 61/100\n",
      "2320/2320 [==============================] - 46s - loss: 0.7681 - acc: 0.6797 - val_loss: 0.7974 - val_acc: 0.6500\n",
      "Epoch 62/100\n",
      "2320/2320 [==============================] - 46s - loss: 0.7617 - acc: 0.6832 - val_loss: 0.8038 - val_acc: 0.6269\n",
      "Epoch 63/100\n",
      "2320/2320 [==============================] - 46s - loss: 0.7558 - acc: 0.6810 - val_loss: 0.8211 - val_acc: 0.6346\n",
      "Epoch 64/100\n",
      "2320/2320 [==============================] - 46s - loss: 0.7377 - acc: 0.6884 - val_loss: 0.8405 - val_acc: 0.5923\n",
      "Epoch 65/100\n",
      "2320/2320 [==============================] - 48s - loss: 0.7630 - acc: 0.6815 - val_loss: 0.8282 - val_acc: 0.6269\n",
      "Epoch 66/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.7507 - acc: 0.6897 - val_loss: 0.8352 - val_acc: 0.6231\n",
      "Epoch 67/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.7206 - acc: 0.7017 - val_loss: 0.8185 - val_acc: 0.6038\n",
      "Epoch 68/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.7306 - acc: 0.6987 - val_loss: 0.8271 - val_acc: 0.5846\n",
      "Epoch 69/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.7215 - acc: 0.6978 - val_loss: 0.8058 - val_acc: 0.6500\n",
      "Epoch 70/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.7179 - acc: 0.6991 - val_loss: 0.8183 - val_acc: 0.6385\n",
      "Epoch 71/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.7121 - acc: 0.7009 - val_loss: 0.8143 - val_acc: 0.6308\n",
      "Epoch 72/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.7170 - acc: 0.7013 - val_loss: 0.8425 - val_acc: 0.5808\n",
      "Epoch 73/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.7180 - acc: 0.6974 - val_loss: 0.8785 - val_acc: 0.5692\n",
      "Epoch 74/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.7192 - acc: 0.6961 - val_loss: 0.8546 - val_acc: 0.5846\n",
      "Epoch 75/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.7018 - acc: 0.7017 - val_loss: 0.8058 - val_acc: 0.5885\n",
      "Epoch 76/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.7042 - acc: 0.7004 - val_loss: 0.8323 - val_acc: 0.6269\n",
      "Epoch 77/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.6920 - acc: 0.7095 - val_loss: 0.8205 - val_acc: 0.6500\n",
      "Epoch 78/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.7154 - acc: 0.6909 - val_loss: 0.7758 - val_acc: 0.6462\n",
      "Epoch 79/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.7150 - acc: 0.6978 - val_loss: 0.7859 - val_acc: 0.5962\n",
      "Epoch 80/100\n",
      "2320/2320 [==============================] - 46s - loss: 0.6789 - acc: 0.7203 - val_loss: 0.7885 - val_acc: 0.6231\n",
      "Epoch 81/100\n",
      "2320/2320 [==============================] - 46s - loss: 0.6670 - acc: 0.7190 - val_loss: 0.8410 - val_acc: 0.6308\n",
      "Epoch 82/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.6698 - acc: 0.7164 - val_loss: 0.8879 - val_acc: 0.6308\n",
      "Epoch 83/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.6610 - acc: 0.7250 - val_loss: 0.8467 - val_acc: 0.6115\n",
      "Epoch 84/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.6874 - acc: 0.7065 - val_loss: 0.8868 - val_acc: 0.5923\n",
      "Epoch 85/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.6770 - acc: 0.7134 - val_loss: 0.8631 - val_acc: 0.6231\n",
      "Epoch 86/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.6623 - acc: 0.7220 - val_loss: 0.8345 - val_acc: 0.6462\n",
      "Epoch 87/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.6733 - acc: 0.7142 - val_loss: 0.8447 - val_acc: 0.6038\n",
      "Epoch 88/100\n",
      "2320/2320 [==============================] - 48s - loss: 0.6320 - acc: 0.7328 - val_loss: 0.8895 - val_acc: 0.6115\n",
      "Epoch 89/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.6570 - acc: 0.7302 - val_loss: 0.8866 - val_acc: 0.6346\n",
      "Epoch 90/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.6403 - acc: 0.7272 - val_loss: 0.9052 - val_acc: 0.6500\n",
      "Epoch 91/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.6282 - acc: 0.7409 - val_loss: 0.8468 - val_acc: 0.6692\n",
      "Epoch 92/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.6204 - acc: 0.7375 - val_loss: 0.8223 - val_acc: 0.6423\n",
      "Epoch 93/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.6084 - acc: 0.7422 - val_loss: 0.9167 - val_acc: 0.6308\n",
      "Epoch 94/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.5950 - acc: 0.7530 - val_loss: 0.9293 - val_acc: 0.6154\n",
      "Epoch 95/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.5975 - acc: 0.7543 - val_loss: 0.9662 - val_acc: 0.6154\n",
      "Epoch 96/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.5977 - acc: 0.7530 - val_loss: 0.9404 - val_acc: 0.6423\n",
      "Epoch 97/100\n",
      "2320/2320 [==============================] - 47s - loss: 0.6021 - acc: 0.7461 - val_loss: 0.9067 - val_acc: 0.6462\n",
      "Epoch 98/100\n",
      "2320/2320 [==============================] - 46s - loss: 0.6179 - acc: 0.7328 - val_loss: 1.1601 - val_acc: 0.5692\n",
      "Epoch 99/100\n",
      "2320/2320 [==============================] - 46s - loss: 0.5872 - acc: 0.7513 - val_loss: 1.0344 - val_acc: 0.5654\n",
      "Epoch 100/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2320/2320 [==============================] - 46s - loss: 0.5585 - acc: 0.7565 - val_loss: 0.9947 - val_acc: 0.5769\n",
      "Params:  time_steps: 64   n_neurons: 200  batch_size:  20  n_epoch:  100 layers  data_dim  8 5 \n",
      " Time cost:  14160.617578744888\n",
      "Add Dropout between every layer:  0.3\n",
      "Avg. Train ACC =  0.657271551617\n",
      "Avg. Test ACC =  0.625692307456\n",
      "Trading ratio:  0.09230769230769231\n",
      "max_drawdown:  -0.0238929849983\n",
      "ROR:  16.5735010249\n",
      "mean 0.127913\n",
      "std 0.169375\n",
      "max 0.480086\n",
      "min -0.023893\n",
      " buy & hold:  0.494055839685\n",
      "sharpe_ratio 0.755206\n",
      "return sum:  4985.78 return ratio:  1.98435056019 Alpha:  1.4902947205\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "QUTUR Model\n",
    "Quantatative Timing based on Unsupervised Learning and RNN \n",
    "@Author Z.Zhao\n",
    "12/18/2017\n",
    "\n",
    "1. fetching all factors from various datasource\n",
    "2. preprocessing data (alignment, padding, concatatenation, normazlizing)\n",
    "3. reducing dimensionality via LSTM-based AutoEncoder\n",
    "4. feeding compressed sequence data to LSTM (multivariate sequence classifier)\n",
    "5. retrieving Timing signal i.e. Trend label, then execute trading accordingly\n",
    "6. calculating Return of Rate.\n",
    "'''\n",
    "# compare compressed 8d data\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Input, LSTM, RepeatVector\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn import preprocessing\n",
    "from keras.callbacks import EarlyStopping\n",
    "'''\n",
    "Preparing data\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "'''\n",
    "Reading Macro economy factors from CSV\n",
    "Notice: data are distributed monthly, need to be filled for daily usage\n",
    "'''\n",
    "def strip_comma(x):\n",
    "    return float(str(x).replace(',',''))\n",
    "def read_macro_economy(base_dir = '../data/macro_economy/', \n",
    "                       filename = 'china_macro_economy_daily.csv',\n",
    "                       start_date = '2002-01-04', end_date = '2017-11-30',\n",
    "                       names = [i for i in range(31)],\n",
    "                       usecols = None):\n",
    "    filename = base_dir +'/'+ filename\n",
    "    print (filename) #中文读取出问题，所以skip row1\n",
    "    df = pd.read_csv(filename, index_col=0, sep=',', \n",
    "                     skiprows=1, usecols=usecols,\n",
    "                     names = names, parse_dates=True,\n",
    "                     converters = {11: strip_comma, 22: strip_comma}\n",
    "                    )\n",
    "    return df[start_date : end_date]\n",
    "\n",
    "\n",
    "'''\n",
    "Reading World economy factors sponsored by OECD from CSV\n",
    "Notice: data are distributed monthly, need to be filled for daily usage\n",
    "'''\n",
    "def read_world_economy(base_dir = '../data/macro_economy/', \n",
    "                       filename = 'OECD-world-economy-daily.csv',\n",
    "                       start_date = '2002-01-04', end_date = '2017-11-30',\n",
    "                       names = [i for i in range(46)],\n",
    "                       usecols = None):\n",
    "    filename = base_dir +'/'+ filename\n",
    "    print (filename) \n",
    "    df = pd.read_csv(filename, index_col=0, \n",
    "                     skiprows=1, usecols=usecols,parse_dates=True,\n",
    "                     names = names\n",
    "                    )\n",
    "    return df[start_date : end_date]\n",
    "\n",
    "'''\n",
    "Reading Top10 Components CSV\n",
    "Data has been assigned weight according to their ratio in the market\n",
    "\n",
    "# Ref: data calculated from data/generate/FetchingComponentsData.ipynb\n",
    "'''\n",
    "def read_components(base_dir = '../data/components/', \n",
    "                       filename = 'components-top10.csv',\n",
    "                       start_date = '2002-01-04', end_date = '2017-11-30',\n",
    "                       names = [i for i in range(10)]):\n",
    "    filename = base_dir +'/'+ filename\n",
    "    print (filename) \n",
    "    df = pd.read_csv(filename, index_col=0, \n",
    "                     skiprows=1,parse_dates=True,\n",
    "                     names = names\n",
    "                    )\n",
    "    df = df.fillna(0)\n",
    "    return df[start_date : end_date]\n",
    "\n",
    "'''\n",
    "Reading ohlcv transaction data for a stock\n",
    "\n",
    "'''\n",
    "def readWSDFile(baseDir, stockCode, startDate='2005-01-04', endDate= '2015-12-31', usecols=None, \n",
    "                names=['date','pre_close','open','high','low','close','change','chg_range',\n",
    "                                               'volume','amount','turn']):\n",
    "    # 解析日期\n",
    "    filename = baseDir+stockCode+'/'+stockCode+'.csv'\n",
    "    print (filename, \"===============\")\n",
    "    dateparse = lambda x: pd.datetime.strptime(x, '%Y/%m/%d').date()\n",
    "    df = pd.read_csv(filename, index_col=0, sep=',', header=None,usecols=usecols,\n",
    "                            skiprows=1, names=names,\n",
    "                           parse_dates=True, date_parser=dateparse)\n",
    "    df = df.fillna(0)\n",
    "    return df[startDate : endDate]\n",
    "\n",
    "'''\n",
    "Reading Technical indicators of a stock\n",
    "'''\n",
    "def readWSDIndexFile(baseDir, stockCode, startYear, yearNum=1):\n",
    "    # parse date\n",
    "    dateparse = lambda x: pd.datetime.strptime(x, '%Y/%m/%d').date()\n",
    "\n",
    "    df = 0\n",
    "    for i in range(yearNum):\n",
    "        tempDF = pd.read_csv(baseDir+'I'+stockCode+'/wsd_'+stockCode+'_'+str(startYear+i)+'.csv', index_col=0, sep=',', parse_dates=True, date_parser=dateparse\n",
    "                             # , usecols=usecols\n",
    "                             )\n",
    "        if i==0: df = tempDF\n",
    "        else: df = df.append(tempDF)\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "'''\n",
    "Preparing all the useful data\n",
    "'''\n",
    "# 整理好多因子输入，以dataframe返回数据+标签\n",
    "def data_prepare(retrace = 0.618, dtype = 'all', start_date='2005-01-04', end_date= '2015-12-31'):\n",
    "    # prepare data\n",
    "    baseDir = '../data/'\n",
    "    stockCodes = ['000300.SH']\n",
    "    i = 0\n",
    "    startYear = 2005\n",
    "    number =11\n",
    "    print(\"------------------------ Let's get the party started! -------------------------------------\")\n",
    "    ## Load data from CSV\n",
    "    dfm = read_macro_economy(start_date = start_date, end_date = end_date)\n",
    "    dfw = read_world_economy(start_date = start_date, end_date = end_date)\n",
    "    dfc = read_components(start_date = start_date, end_date = end_date)\n",
    "    df = readWSDFile(baseDir, stockCodes[i], start_date, end_date)\n",
    "    dfi = readWSDIndexFile(baseDir, stockCodes[i], startYear, number)\n",
    "    allDF = df\n",
    "    if dtype == 'all':\n",
    "        allDF = pd.concat([df, dfi, dfm, dfw, dfc], axis=1)\n",
    "    elif dtype == 'only_m':\n",
    "        allDF = pd.concat([df, dfm], axis=1) # macro economy \n",
    "    elif dtype == 'only_i':\n",
    "        allDF = pd.concat([df, dfi], axis=1) # technical indicators\n",
    "    elif dtype == 'only_w':\n",
    "        allDF = pd.concat([df, dfw], axis=1) # world economy\n",
    "    elif dtype == 'only_c':\n",
    "        allDF = pd.concat([df, dfc], axis=1) # constitutant stock\n",
    "    elif dtype == 'im':\n",
    "        allDF = pd.concat([df, dfi, dfm], axis=1) \n",
    "    elif dtype == 'ic':\n",
    "        allDF = pd.concat([df, dfi, dfc], axis=1) \n",
    "    else:\n",
    "        pass\n",
    "    allDF = allDF.fillna(0)\n",
    "    sample_num = np.shape(df)[0]\n",
    "    labelDF = pd.Series(np.zeros(sample_num))\n",
    "    print (\"Factors Shape:\", np.shape(allDF))\n",
    "    \n",
    "    # 求出 trend\n",
    "    price = df['close']\n",
    "    start = 0\n",
    "    while price[start] > price[start+1]:\n",
    "        labelDF[start] = 1 #flat\n",
    "        start +=1\n",
    "    \n",
    "    #find peak, find trough, calculate retracement and label trend accordingly\n",
    "    i = start\n",
    "    while i < sample_num - 1:\n",
    "        cursor = i\n",
    "        while cursor < sample_num - 1 and price[cursor] <= price[cursor+1]:\n",
    "            cursor += 1\n",
    "        peak = cursor\n",
    "        while cursor < sample_num - 1 and price[cursor] >= price[cursor+1]:\n",
    "            cursor += 1\n",
    "        trough = cursor\n",
    "        retracement = (price[peak] - price[trough]) / (price[peak] - price[i])\n",
    "        mark = 1 # flat\n",
    "        if retracement < retrace:\n",
    "            mark = 2 # UP\n",
    "        elif retracement > 1 + retrace:\n",
    "            mark = 0 # DOWN\n",
    "        for k in range(i, cursor+1):\n",
    "            labelDF[k] = mark\n",
    "        i = cursor\n",
    "\n",
    "    print(\"---- Trend Distribution Check --------\")\n",
    "    print(labelDF.value_counts().sort_index())\n",
    "    \n",
    "    # make a deep copy of Price Difference before normalizing\n",
    "    priceDF = allDF['change'].copy(deep=True)\n",
    "    # normalize(x)\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    input_data = scaler.fit_transform(allDF)\n",
    "    print (\"input data shape: \", np.shape(input_data)) #  days *  factors\n",
    "    print (\"input label shape: \", np.shape(labelDF))\n",
    "    labels = labelDF.values\n",
    "    #input_data = np.concatenate((input_data, labels.reshape(-1,1)), axis = 1) # historical data as part of the series\n",
    "    return input_data, labels, priceDF, price # train/test data, labels, prices difference, actual price for yield calucluation\n",
    "\n",
    "\n",
    "###### Hyper paramters #########\n",
    "time_steps = 8\n",
    "n_neurons = 200\n",
    "num_classes = 3\n",
    "batch_size = 20 # specify batch size explicitly; no shuffle but successive sequence\n",
    "n_epoch_ae = 40\n",
    "n_epoch = 100\n",
    "train_ratio = 0.9\n",
    "dropout = 0.3\n",
    "concate = False # without using historical labels as input data\n",
    "t0 = time()\n",
    "dataset,labels, priceDF, price = data_prepare()\n",
    "\n",
    "segment_num = (len(dataset) - time_steps - 1) // batch_size # rollingly use data\n",
    "train_size = int(segment_num * train_ratio)\n",
    "test_size = segment_num - train_size\n",
    "data_dim = np.shape(dataset)[1] #input + historic labels\n",
    "'''\n",
    "#divide training/validation dataset; numpy array\n",
    "train_x = dataset[0 : train_size * batch_size + time_steps]\n",
    "test_x = dataset[train_size * batch_size : (train_size + test_size) * batch_size + time_steps]\n",
    "\n",
    "#historic label as input time series\n",
    "train_historic_label = np.array(labels[0 : train_size * batch_size + time_steps]).reshape(-1,1)\n",
    "test_historic_label = np.array(labels[train_size * batch_size : (train_size + test_size) * batch_size + time_steps]).reshape(-1,1)\n",
    "\n",
    "#Sliding window: label is just 1 step further after sequence data\n",
    "train_y = labels[time_steps : train_size * batch_size + time_steps]\n",
    "test_y = labels[train_size * batch_size + time_steps: (train_size + test_size) * batch_size + time_steps]\n",
    "\n",
    "# add historical trends or not\n",
    "if concate:\n",
    "    data_dim += 1\n",
    "    train_x = np.concatenate((train_x, train_historic_label), axis = 1)\n",
    "    test_x = np.concatenate((test_x, test_historic_label), axis = 1)\n",
    "\n",
    "train_sample = len(train_x) - time_steps\n",
    "b = np.array([[]])\n",
    "# creating data in a rolling window view \n",
    "for i in range(train_sample):\n",
    "    b = np.append(b, train_x[i : time_steps + i])\n",
    "train_x = b.reshape(train_sample, time_steps, data_dim)\n",
    "print(\"training size: \", train_sample)\n",
    "\n",
    "test_sample = len(test_x) - time_steps\n",
    "b = np.array([[]])\n",
    "for i in range(test_sample):\n",
    "    b = np.append(b, test_x[i : time_steps + i])\n",
    "test_x = b.reshape(test_sample, time_steps, data_dim)\n",
    "print(\"testing size: \", test_sample)\n",
    "\n",
    "train_y = np.array(train_y, dtype=np.int32)\n",
    "test_y = np.array(test_y, dtype=np.int32)\n",
    "\n",
    "############## AutoEncoder MODEL ##########################\n",
    "latent_dim = 8\n",
    "layer1 = 128\n",
    "layer2 =32\n",
    "layers = [layer2,layer1]\n",
    "inputs = Input(shape=(time_steps, data_dim))\n",
    "encoded = LSTM(units = layer1, return_sequences = True)(inputs)\n",
    "encoded = LSTM(units = layer2, return_sequences = True)(encoded)\n",
    "encoded = LSTM(units = latent_dim, return_sequences = False)(encoded) # most hidden layer, only preseve the last step's output\n",
    "\n",
    "repeated_out = RepeatVector(time_steps)(encoded)  # repeat intermediate output [2D -> 3D]\n",
    "\n",
    "decoded = LSTM(layer2, return_sequences=True)(repeated_out)\n",
    "decoded = LSTM(layer1, return_sequences=True)(decoded)\n",
    "decoded = LSTM(data_dim, return_sequences=True)(decoded)  # output layer as a comparison\n",
    "\n",
    "sequence_autoencoder = Model(inputs, decoded)\n",
    "encoder = Model(inputs, encoded)  ## encoder for dimensionality reduction\n",
    "\n",
    "sequence_autoencoder.compile(optimizer='rmsprop', \n",
    "                             loss='mean_squared_error',\n",
    "                             )\n",
    "history = sequence_autoencoder.fit(x = train_x, y = train_x, \n",
    "                         batch_size=batch_size,\n",
    "                         epochs=n_epoch_ae,\n",
    "                         shuffle=False,\n",
    "                         validation_data = (test_x, test_x))\n",
    "print (\"Params: \", \"time_steps:\", time_steps, \" latent_dim:\", \n",
    "       latent_dim, \" batch_size: \",batch_size, \" n_epoch: \", \n",
    "       n_epoch, 'layers', layers, \"\\n Time cost: \", (time() - t0))\n",
    "\n",
    "encoder.save('autoencoder-135d-8ts-40ep-128-32-8.h5')\n",
    "# compressed_data = encoder.predict(dataset)\n",
    "\n",
    "'''\n",
    "## load model from file\n",
    "encoder = load_model('autoencoder-135d-8ts-40ep-128-32-8.h5')\n",
    "\n",
    "\n",
    "# rolling window\n",
    "b = np.array([[]])\n",
    "n_sample = len(dataset) - time_steps\n",
    "for i in range(n_sample):\n",
    "    b = np.append(b, dataset[i : time_steps + i])\n",
    "dataset = b.reshape(n_sample, time_steps, data_dim)\n",
    "\n",
    "compressed_data = encoder.predict(dataset)\n",
    "dataset = compressed_data\n",
    "\n",
    "################################################################\n",
    "########### Prediction Model ###################################\n",
    "# divide training/validation dataset \n",
    "\n",
    "for time_steps in [4, 8, 16, 32, 64]:\n",
    "    segment_num = (len(dataset) - time_steps - 1) // batch_size # rollingly use data\n",
    "    train_size = int(segment_num * train_ratio)\n",
    "    test_size = segment_num - train_size\n",
    "    data_dim = np.shape(dataset)[1]\n",
    "    print (\"prediction model input data shape: \", np.shape(dataset)) \n",
    "\n",
    "    #divide training/validation dataset; numpy array\n",
    "    train_x = dataset[0 : train_size * batch_size + time_steps]\n",
    "    test_x = dataset[train_size * batch_size : (train_size + test_size) * batch_size + time_steps]\n",
    "    # rolling window\n",
    "    train_sample = len(train_x) - time_steps\n",
    "    print (\"train sample: \", train_sample, \"data_dim\",data_dim)\n",
    "    b = np.array([[]])\n",
    "    for i in range(train_sample):\n",
    "        b = np.append(b, train_x[i : time_steps + i])\n",
    "    train_x = b.reshape(train_sample, time_steps, data_dim)\n",
    "    print(\"training size: \", train_sample)\n",
    "\n",
    "    test_sample = len(test_x) - time_steps\n",
    "    b = np.array([[]])\n",
    "    for i in range(test_sample):\n",
    "        b = np.append(b, test_x[i : time_steps + i])\n",
    "    test_x = b.reshape(test_sample, time_steps, data_dim)\n",
    "    print(\"testing size: \", test_sample)\n",
    "\n",
    "    #Sliding window: label is just 1 step further after sequence data\n",
    "    train_y = labels[time_steps : train_size * batch_size + time_steps]\n",
    "    test_y = labels[train_size * batch_size + time_steps: (train_size + test_size) * batch_size + time_steps]\n",
    "    train_y = np.array(train_y, dtype=np.int32)\n",
    "    test_y = np.array(test_y, dtype=np.int32)\n",
    "\n",
    "    layers = 5\n",
    "    early_stop = EarlyStopping(monitor='val_loss',patience = 70, mode='min') #when it stops deacreasing, stop\n",
    "    # expected input data shape: (batch_size, timesteps, data_dim)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_neurons, return_sequences=True,stateful=True,dropout = dropout,\n",
    "                   batch_input_shape=(batch_size, time_steps, data_dim)))  # returns a sequence of vectors of dimension 32\n",
    "    model.add(LSTM(n_neurons, return_sequences=True,dropout = dropout, stateful=True))  # returns a sequence of vectors of dimension 32\n",
    "    model.add(LSTM(n_neurons, return_sequences=True,dropout = dropout, stateful=True)) \n",
    "    model.add(LSTM(n_neurons, dropout = dropout,stateful=True))  # return a single vector of dimension 32\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', #for integer class, not one hot encoding\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])   \n",
    "\n",
    "    history = model.fit(train_x, train_y,\n",
    "              batch_size=batch_size, \n",
    "              epochs=n_epoch,\n",
    "              shuffle=False,\n",
    "              validation_data=(test_x, test_y),\n",
    "              callbacks = [early_stop]         \n",
    "                       )\n",
    "\n",
    "    print (\"Params: \", \"time_steps:\", time_steps, \"  n_neurons:\", n_neurons, \n",
    "           \" batch_size: \",batch_size, \" n_epoch: \", n_epoch, 'layers',\" data_dim \",data_dim,\n",
    "           layers, \"\\n Time cost: \", (time() - t0))\n",
    "    print(\"Add Dropout between every layer: \",dropout)\n",
    "    print('Avg. Train ACC = ', np.average(history.history['acc']))\n",
    "    print('Avg. Test ACC = ', np.average(history.history['val_acc']))\n",
    "    predictions = np.argmax(model.predict(test_x, batch_size=batch_size), axis = 1)\n",
    "    '''\n",
    "    # plot history\n",
    "    plt.plot(history.history['acc'], label='train_acc')\n",
    "    plt.plot(history.history['val_acc'], label='test_acc')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # loss\n",
    "    plt.plot(history.history['loss'], label='train_loss')\n",
    "    plt.plot(history.history['val_loss'], label='test_loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # prediction check & visualization\n",
    "    #model.save('classifier.h5')\n",
    "    \n",
    "    plt.plot(predictions, label = 'prediction')\n",
    "    plt.plot(test_y, label = 'label')\n",
    "    plt.legend()\n",
    "    plt.show()'''\n",
    "    '''\n",
    "    T + 1 trading rules.\n",
    "    Trading stratigy:\n",
    "    1. initial capital: reward = 1 share of 1st day's stock\n",
    "    2. hold = True, empty = False \n",
    "    3. if hold:\n",
    "            if down:\n",
    "                reward += price[x]\n",
    "                empty = True\n",
    "                hold = False\n",
    "                x += 1\n",
    "       if empty:\n",
    "           if up:\n",
    "               reward -= price[x]\n",
    "               hold = True\n",
    "               empty = False\n",
    "               x += 1\n",
    "        x += 1\n",
    "    '''\n",
    "    # real price\n",
    "    #price.plot()\n",
    "\n",
    "    # points of transactions \n",
    "    test_price = price.iloc[train_size * batch_size + time_steps: (train_size + test_size) * batch_size + time_steps]\n",
    "    test_priceDF = priceDF[train_size * batch_size + time_steps: (train_size + test_size) * batch_size + time_steps]\n",
    "    sells = []\n",
    "    buys = []\n",
    "    reward = test_price[0]\n",
    "    ror = []\n",
    "    hold = True\n",
    "    empty = False\n",
    "    buys.append(reward)\n",
    "    sells.append(2000)\n",
    "    i = 1\n",
    "    while i < len(predictions):\n",
    "        if hold:\n",
    "            if predictions[i] == 0: # Down trend\n",
    "                reward += test_price[i] # selling\n",
    "                sells.append(test_price[i] + 75) # for ploting\n",
    "                empty = True\n",
    "                hold = False\n",
    "                i += 1\n",
    "                buys.append(2000)\n",
    "        if empty:\n",
    "            if predictions[i] == 2: # UP trend\n",
    "                reward -= price[i]  # buying\n",
    "                buys.append(test_price[i] - 75) \n",
    "                hold = True\n",
    "                empty = False\n",
    "                i += 1\n",
    "                sells.append(2000) \n",
    "        buys.append(2000) # fill blank\n",
    "        sells.append(2000) \n",
    "        i += 1\n",
    "        ror.append(reward / test_price[0])\n",
    "\n",
    "    #empty repository at the last day\n",
    "    if hold:\n",
    "        sells.append(test_price[-1])\n",
    "        buys.append(2000)\n",
    "        reward += test_price[-1] # selling\n",
    "        ror.append(reward / test_price[0] - 1)\n",
    "    '''\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(14, 8)\n",
    "    x = np.array([i for i in range(len(sells))])\n",
    "    s = np.random.rand(*x.shape) * 100\n",
    "    plt.scatter(x, sells, c=\"r\", s= s, alpha=0.7, label = \"trade signal\",marker=\"v\")\n",
    "    plt.scatter(x, buys, c=\"y\", s= s, alpha=0.7, label = \"trade signal\",marker=\"^\")\n",
    "    plt.plot(test_price.values, label = \"close\", linewidth=3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.plot(ror, label = \"returns\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    '''\n",
    "    '''\n",
    "    Calculate Max Drawdown\n",
    "    max drawdown = (Px - Py) / Px \n",
    "    (x for buy, y for sell)\n",
    "    '''\n",
    "    trade = []\n",
    "    for i in range(len(buys)):\n",
    "        if buys[i] != 2000:\n",
    "            trade.append(-buys[i])\n",
    "        elif sells[i] != 2000:\n",
    "            trade.append(sells[i])\n",
    "    print(\"Trading ratio: \", len(trade) / len(sells))\n",
    "    max_drawdown = 0. #negative number\n",
    "    i = 0\n",
    "    while i in range(len(trade) - 1):\n",
    "        if (trade[i+1] + trade[i]) / -trade[i] < max_drawdown:\n",
    "            max_drawdown = (trade[i+1] + trade[i]) / -trade[i]\n",
    "            #print ('max_drawdown: ', max_drawdown, \" buy: \", trade[i], \" sell: \",trade[i+1])\n",
    "        i += 2\n",
    "    print ('max_drawdown: ', max_drawdown)\n",
    "\n",
    "    '''\n",
    "    Calculate Sharpe Ratio\n",
    "    ratio = return_of_year.mean / return_of_year.standard_deviation\n",
    "    (average returns) / volatility\n",
    "    '''\n",
    "    returns = []\n",
    "    i = 0\n",
    "    while i in range(len(trade) - 1):\n",
    "        returns.append((trade[i+1] + trade[i]) / -trade[i])\n",
    "        i += 2\n",
    "    df = pd.DataFrame(data=returns, dtype=np.float32)\n",
    "    sharpe_ratio = df.mean() / df.std()\n",
    "    print('ROR: ', ror[-1])\n",
    "    print('mean', df.mean().values[0])\n",
    "    print('std', df.std().values[0])\n",
    "    print('max', df.max().values[0])\n",
    "    print('min', df.min().values[0])\n",
    "    print(\" buy & hold: \", (test_price[-1] - test_price[0]) / test_price[0])\n",
    "    print('sharpe_ratio', sharpe_ratio.values[0])\n",
    "\n",
    "    return_sum = np.sum(trade)\n",
    "    print(\"return sum: \", return_sum, \"return ratio: \", return_sum / test_price[0], \n",
    "          'Alpha: ', (return_sum - (test_price[-1] - test_price[0])) / test_price[0])\n",
    "\n",
    "####### Generation Ability Testing #################\n",
    "# print('####### Generation Ability Testing #################')\n",
    "# # load testing data\n",
    "# dataset,labels, priceDF, price = data_prepare(startDate='2015-11-20', endDate='2016-11-18')\n",
    "# # adjust input of AutoEncoder\n",
    "# data_dim = np.shape(dataset)[1]\n",
    "# b = np.array([[]])\n",
    "# n_sample = len(dataset) - time_steps\n",
    "# for i in range(n_sample):\n",
    "#     b = np.append(b, dataset[i : time_steps + i])\n",
    "# dataset = b.reshape(n_sample, time_steps, data_dim)\n",
    "\n",
    "# '''APPLY AutoEncoder ''' \n",
    "# compressed_data = encoder.predict(dataset)       \n",
    "\n",
    "# segment_num = (len(dataset) - time_steps - 1) // batch_size # rollingly use data\n",
    "# data_dim = np.shape(dataset)[1]\n",
    "\n",
    "# test_x = dataset[0 : segment_num * batch_size + time_steps] # entire dataset as testing\n",
    "# test_sample = len(test_x) - time_steps\n",
    "# b = np.array([[]])\n",
    "# for i in range(test_sample):\n",
    "#     b = np.append(b, test_x[i : time_steps + i])\n",
    "# test_x = b.reshape(test_sample, time_steps, data_dim)\n",
    "# print(\"testing size: \", test_sample)\n",
    "\n",
    "# #Sliding window: label is just 1 step further after sequence data\n",
    "# test_y = labels[time_steps: segment_num * batch_size + time_steps]\n",
    "# test_y = np.array(test_y, dtype=np.int32)\n",
    "\n",
    "# '''APPLY LSTM Classifier'''\n",
    "# predictions = np.argmax(model.predict(test_x, batch_size=batch_size), axis = 1)\n",
    "\n",
    "# plt.subplot(412)\n",
    "# plt.plot(predictions, label = 'prediction')\n",
    "# plt.plot(test_y, label = 'label')\n",
    "# plt.show()\n",
    "\n",
    "# # points of transactions \n",
    "# test_price = price.iloc[time_steps: segment_num * batch_size + time_steps]\n",
    "# test_priceDF = priceDF[time_steps: segment_num * batch_size + time_steps]\n",
    "# deals = []\n",
    "# returns = 0.\n",
    "# ror = []\n",
    "# for i in range(len(predictions)):\n",
    "#     if predictions[i] > 0:\n",
    "#         returns += test_priceDF[i]\n",
    "#         deals.append(test_price[i])\n",
    "#     else:\n",
    "#         deals.append(2000) # means down trend will occur\n",
    "#     ror.append(returns)\n",
    "\n",
    "# fig = plt.gcf()\n",
    "# fig.set_size_inches(14, 8)\n",
    "# x = np.array([i for i in range(len(deals))])\n",
    "# s = np.random.rand(*x.shape) * 100\n",
    "# plt.scatter(x, deals, c=\"r\", s= s, alpha=0.7, label = \"trade signal\",marker=\"v\")\n",
    "# plt.plot(test_price.values, label = \"close\", linewidth=3)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# plt.plot(ror, label = \"returns\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "########## can use `test_on_batch` to output metrics in prediction\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
