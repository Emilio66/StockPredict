{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ Let's get the party started! -------------------------------------\n",
      "../data/macro_economy//china_macro_economy_daily.csv\n",
      "../data/macro_economy//OECD-world-economy-daily.csv\n",
      "../data/components//components-top10.csv\n",
      "../data/000300.SH/000300.SH.csv ===============\n",
      "Factors Shape: (2671, 135)\n",
      "---- Trend Distribution Check --------\n",
      "0.0     960\n",
      "1.0     558\n",
      "2.0    1153\n",
      "dtype: int64\n",
      "input data shape:  (2671, 135)\n",
      "input label shape:  (2671,)\n",
      "training size:  2380\n",
      "testing size:  280\n",
      "Train on 2380 samples, validate on 280 samples\n",
      "Epoch 1/100\n",
      "2380/2380 [==============================] - 21s - loss: 0.3341 - val_loss: 0.3525\n",
      "Epoch 2/100\n",
      "2380/2380 [==============================] - 21s - loss: 0.3298 - val_loss: 0.3483\n",
      "Epoch 3/100\n",
      "2380/2380 [==============================] - 23s - loss: 0.3256 - val_loss: 0.3441\n",
      "Epoch 4/100\n",
      "2380/2380 [==============================] - 20s - loss: 0.3213 - val_loss: 0.3400\n",
      "Epoch 5/100\n",
      "2380/2380 [==============================] - 20s - loss: 0.3172 - val_loss: 0.3359\n",
      "Epoch 6/100\n",
      "2380/2380 [==============================] - 20s - loss: 0.3130 - val_loss: 0.3317\n",
      "Epoch 7/100\n",
      "2380/2380 [==============================] - 18s - loss: 0.3088 - val_loss: 0.3276\n",
      "Epoch 8/100\n",
      "2380/2380 [==============================] - 17s - loss: 0.3045 - val_loss: 0.3234\n",
      "Epoch 9/100\n",
      "2380/2380 [==============================] - 16s - loss: 0.3003 - val_loss: 0.3192\n",
      "Epoch 10/100\n",
      "2380/2380 [==============================] - 17s - loss: 0.2960 - val_loss: 0.3149\n",
      "Epoch 11/100\n",
      "2380/2380 [==============================] - 17s - loss: 0.2916 - val_loss: 0.3106\n",
      "Epoch 12/100\n",
      "2380/2380 [==============================] - 17s - loss: 0.2871 - val_loss: 0.3062\n",
      "Epoch 13/100\n",
      "2380/2380 [==============================] - 17s - loss: 0.2826 - val_loss: 0.3017\n",
      "Epoch 14/100\n",
      "2380/2380 [==============================] - 17s - loss: 0.2780 - val_loss: 0.2971\n",
      "Epoch 15/100\n",
      "2380/2380 [==============================] - 17s - loss: 0.2733 - val_loss: 0.2925\n",
      "Epoch 16/100\n",
      "2380/2380 [==============================] - 17s - loss: 0.2685 - val_loss: 0.2877\n",
      "Epoch 17/100\n",
      "2380/2380 [==============================] - 17s - loss: 0.2636 - val_loss: 0.2829\n",
      "Epoch 18/100\n",
      "2380/2380 [==============================] - 17s - loss: 0.2586 - val_loss: 0.2779\n",
      "Epoch 19/100\n",
      "2380/2380 [==============================] - 14s - loss: 0.2535 - val_loss: 0.2729\n",
      "Epoch 20/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.2484 - val_loss: 0.2678\n",
      "Epoch 21/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.2432 - val_loss: 0.2627\n",
      "Epoch 22/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.2379 - val_loss: 0.2575\n",
      "Epoch 23/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.2327 - val_loss: 0.2523\n",
      "Epoch 24/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.2274 - val_loss: 0.2471\n",
      "Epoch 25/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.2221 - val_loss: 0.2419\n",
      "Epoch 26/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.2169 - val_loss: 0.2368\n",
      "Epoch 27/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.2118 - val_loss: 0.2318\n",
      "Epoch 28/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.2067 - val_loss: 0.2269\n",
      "Epoch 29/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.2018 - val_loss: 0.2221\n",
      "Epoch 30/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1971 - val_loss: 0.2175\n",
      "Epoch 31/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1925 - val_loss: 0.2130\n",
      "Epoch 32/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1881 - val_loss: 0.2087\n",
      "Epoch 33/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1839 - val_loss: 0.2046\n",
      "Epoch 34/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1798 - val_loss: 0.2007\n",
      "Epoch 35/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1760 - val_loss: 0.1970\n",
      "Epoch 36/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1724 - val_loss: 0.1935\n",
      "Epoch 37/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1689 - val_loss: 0.1901\n",
      "Epoch 38/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1657 - val_loss: 0.1870\n",
      "Epoch 39/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1626 - val_loss: 0.1839\n",
      "Epoch 40/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1597 - val_loss: 0.1811\n",
      "Epoch 41/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1569 - val_loss: 0.1784\n",
      "Epoch 42/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1543 - val_loss: 0.1759\n",
      "Epoch 43/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1518 - val_loss: 0.1734\n",
      "Epoch 44/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1495 - val_loss: 0.1711\n",
      "Epoch 45/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1472 - val_loss: 0.1690\n",
      "Epoch 46/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1451 - val_loss: 0.1669\n",
      "Epoch 47/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.1431 - val_loss: 0.1650\n",
      "Epoch 48/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1412 - val_loss: 0.1631\n",
      "Epoch 49/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1394 - val_loss: 0.1613\n",
      "Epoch 50/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1377 - val_loss: 0.1596\n",
      "Epoch 51/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1361 - val_loss: 0.1580\n",
      "Epoch 52/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1345 - val_loss: 0.1565\n",
      "Epoch 53/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1330 - val_loss: 0.1550\n",
      "Epoch 54/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1316 - val_loss: 0.1536\n",
      "Epoch 55/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1302 - val_loss: 0.1523\n",
      "Epoch 56/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1289 - val_loss: 0.1510\n",
      "Epoch 57/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1276 - val_loss: 0.1498\n",
      "Epoch 58/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1264 - val_loss: 0.1486\n",
      "Epoch 59/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1252 - val_loss: 0.1475\n",
      "Epoch 60/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1241 - val_loss: 0.1464\n",
      "Epoch 61/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1230 - val_loss: 0.1454\n",
      "Epoch 62/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1220 - val_loss: 0.1444\n",
      "Epoch 63/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1210 - val_loss: 0.1434\n",
      "Epoch 64/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1200 - val_loss: 0.1425\n",
      "Epoch 65/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1191 - val_loss: 0.1416\n",
      "Epoch 66/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1182 - val_loss: 0.1408\n",
      "Epoch 67/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1173 - val_loss: 0.1400\n",
      "Epoch 68/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1165 - val_loss: 0.1392\n",
      "Epoch 69/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1157 - val_loss: 0.1384\n",
      "Epoch 70/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1149 - val_loss: 0.1377\n",
      "Epoch 71/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1141 - val_loss: 0.1370\n",
      "Epoch 72/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1134 - val_loss: 0.1363\n",
      "Epoch 73/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1127 - val_loss: 0.1356\n",
      "Epoch 74/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1120 - val_loss: 0.1350\n",
      "Epoch 75/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1113 - val_loss: 0.1344\n",
      "Epoch 76/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1106 - val_loss: 0.1338\n",
      "Epoch 77/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1100 - val_loss: 0.1332\n",
      "Epoch 78/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1094 - val_loss: 0.1327\n",
      "Epoch 79/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1088 - val_loss: 0.1322\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2380/2380 [==============================] - 12s - loss: 0.1082 - val_loss: 0.1317\n",
      "Epoch 81/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1077 - val_loss: 0.1312\n",
      "Epoch 82/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1071 - val_loss: 0.1307\n",
      "Epoch 83/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1066 - val_loss: 0.1302\n",
      "Epoch 84/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1061 - val_loss: 0.1298\n",
      "Epoch 85/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1056 - val_loss: 0.1293\n",
      "Epoch 86/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.1051 - val_loss: 0.1289\n",
      "Epoch 87/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1047 - val_loss: 0.1285\n",
      "Epoch 88/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1042 - val_loss: 0.1281\n",
      "Epoch 89/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1038 - val_loss: 0.1278\n",
      "Epoch 90/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1034 - val_loss: 0.1274\n",
      "Epoch 91/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1030 - val_loss: 0.1270\n",
      "Epoch 92/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1026 - val_loss: 0.1267\n",
      "Epoch 93/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1022 - val_loss: 0.1264\n",
      "Epoch 94/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1018 - val_loss: 0.1260\n",
      "Epoch 95/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1014 - val_loss: 0.1257\n",
      "Epoch 96/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1011 - val_loss: 0.1254\n",
      "Epoch 97/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1007 - val_loss: 0.1251\n",
      "Epoch 98/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1004 - val_loss: 0.1249\n",
      "Epoch 99/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.1000 - val_loss: 0.1246\n",
      "Epoch 100/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.0997 - val_loss: 0.1243\n",
      "Params:  time_steps: 8  latent_dim: 8  batch_size:  20  n_epoch:   Optimizer:  sgd 100  layers  [32, 128] \n",
      " Time cost:  1402.89732837677\n",
      "Train on 2380 samples, validate on 280 samples\n",
      "Epoch 1/100\n",
      "2380/2380 [==============================] - 14s - loss: 0.0914 - val_loss: 0.1097\n",
      "Epoch 2/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.0840 - val_loss: 0.1031\n",
      "Epoch 3/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.0791 - val_loss: 0.0984\n",
      "Epoch 4/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.0753 - val_loss: 0.0947\n",
      "Epoch 5/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.0722 - val_loss: 0.0917\n",
      "Epoch 6/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.0697 - val_loss: 0.0891\n",
      "Epoch 7/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.0675 - val_loss: 0.0868\n",
      "Epoch 8/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.0657 - val_loss: 0.0849\n",
      "Epoch 9/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.0641 - val_loss: 0.0833\n",
      "Epoch 10/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.0627 - val_loss: 0.0818\n",
      "Epoch 11/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.0615 - val_loss: 0.0804\n",
      "Epoch 12/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.0603 - val_loss: 0.0792\n",
      "Epoch 13/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.0593 - val_loss: 0.0781\n",
      "Epoch 14/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.0583 - val_loss: 0.0771\n",
      "Epoch 15/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.0574 - val_loss: 0.0761\n",
      "Epoch 16/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.0565 - val_loss: 0.0752\n",
      "Epoch 17/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.0557 - val_loss: 0.0743\n",
      "Epoch 18/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.0549 - val_loss: 0.0736\n",
      "Epoch 19/100\n",
      "2380/2380 [==============================] - 12s - loss: 0.0541 - val_loss: 0.0729\n",
      "Epoch 20/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.0529 - val_loss: 0.0721\n",
      "Epoch 21/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.0515 - val_loss: 0.0713\n",
      "Epoch 22/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.0502 - val_loss: 0.0705\n",
      "Epoch 23/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.0488 - val_loss: 0.0698\n",
      "Epoch 24/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.0475 - val_loss: 0.0694\n",
      "Epoch 25/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.0462 - val_loss: 0.0693\n",
      "Epoch 26/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.0449 - val_loss: 0.0694\n",
      "Epoch 27/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.0436 - val_loss: 0.0697\n",
      "Epoch 28/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.0424 - val_loss: 0.0698\n",
      "Epoch 29/100\n",
      "2380/2380 [==============================] - 13s - loss: 0.0413 - val_loss: 0.0696\n",
      "Epoch 30/100\n",
      "2380/2380 [==============================] - 9s - loss: 0.0402 - val_loss: 0.0693\n",
      "Epoch 31/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0393 - val_loss: 0.0689\n",
      "Epoch 32/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0384 - val_loss: 0.0684\n",
      "Epoch 33/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0376 - val_loss: 0.0680\n",
      "Epoch 34/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0369 - val_loss: 0.0677\n",
      "Epoch 35/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0362 - val_loss: 0.0674\n",
      "Epoch 36/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0356 - val_loss: 0.0671\n",
      "Epoch 37/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0351 - val_loss: 0.0668\n",
      "Epoch 38/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0345 - val_loss: 0.0666\n",
      "Epoch 39/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0341 - val_loss: 0.0664\n",
      "Epoch 40/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0337 - val_loss: 0.0662\n",
      "Epoch 41/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0333 - val_loss: 0.0661\n",
      "Epoch 42/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0329 - val_loss: 0.0659\n",
      "Epoch 43/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0325 - val_loss: 0.0656\n",
      "Epoch 44/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0322 - val_loss: 0.0655\n",
      "Epoch 45/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0319 - val_loss: 0.0653\n",
      "Epoch 46/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0316 - val_loss: 0.0652\n",
      "Epoch 47/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0313 - val_loss: 0.0651\n",
      "Epoch 48/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0310 - val_loss: 0.0650\n",
      "Epoch 49/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0307 - val_loss: 0.0649\n",
      "Epoch 50/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0304 - val_loss: 0.0648\n",
      "Epoch 51/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0301 - val_loss: 0.0647\n",
      "Epoch 52/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0299 - val_loss: 0.0649\n",
      "Epoch 53/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0297 - val_loss: 0.0648\n",
      "Epoch 54/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0295 - val_loss: 0.0648\n",
      "Epoch 55/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0292 - val_loss: 0.0646\n",
      "Epoch 56/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0289 - val_loss: 0.0645\n",
      "Epoch 57/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0287 - val_loss: 0.0644\n",
      "Epoch 58/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0284 - val_loss: 0.0642\n",
      "Epoch 59/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0282 - val_loss: 0.0641\n",
      "Epoch 60/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0279 - val_loss: 0.0639\n",
      "Epoch 61/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0277 - val_loss: 0.0638\n",
      "Epoch 62/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0275 - val_loss: 0.0637\n",
      "Epoch 63/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0273 - val_loss: 0.0636\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2380/2380 [==============================] - 8s - loss: 0.0271 - val_loss: 0.0635\n",
      "Epoch 65/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0268 - val_loss: 0.0631\n",
      "Epoch 66/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0266 - val_loss: 0.0632\n",
      "Epoch 67/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0266 - val_loss: 0.0631\n",
      "Epoch 68/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0263 - val_loss: 0.0629\n",
      "Epoch 69/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0261 - val_loss: 0.0628\n",
      "Epoch 70/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0259 - val_loss: 0.0625\n",
      "Epoch 71/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0258 - val_loss: 0.0624\n",
      "Epoch 72/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0256 - val_loss: 0.0622\n",
      "Epoch 73/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0251 - val_loss: 0.0620\n",
      "Epoch 74/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0251 - val_loss: 0.0619\n",
      "Epoch 75/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0249 - val_loss: 0.0618\n",
      "Epoch 76/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0247 - val_loss: 0.0616\n",
      "Epoch 77/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0246 - val_loss: 0.0615\n",
      "Epoch 78/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0244 - val_loss: 0.0614\n",
      "Epoch 79/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0242 - val_loss: 0.0613\n",
      "Epoch 80/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0243 - val_loss: 0.0612\n",
      "Epoch 81/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0240 - val_loss: 0.0608\n",
      "Epoch 82/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0238 - val_loss: 0.0610\n",
      "Epoch 83/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0236 - val_loss: 0.0607\n",
      "Epoch 84/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0234 - val_loss: 0.0607\n",
      "Epoch 85/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0232 - val_loss: 0.0607\n",
      "Epoch 86/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0232 - val_loss: 0.0606\n",
      "Epoch 87/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0230 - val_loss: 0.0605\n",
      "Epoch 88/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0229 - val_loss: 0.0608\n",
      "Epoch 89/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0229 - val_loss: 0.0610\n",
      "Epoch 90/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0227 - val_loss: 0.0606\n",
      "Epoch 91/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0226 - val_loss: 0.0607\n",
      "Epoch 92/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0224 - val_loss: 0.0601\n",
      "Epoch 93/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0224 - val_loss: 0.0607\n",
      "Epoch 94/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0223 - val_loss: 0.0606\n",
      "Epoch 95/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0224 - val_loss: 0.0599\n",
      "Epoch 96/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0221 - val_loss: 0.0603\n",
      "Epoch 97/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0221 - val_loss: 0.0595\n",
      "Epoch 98/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0218 - val_loss: 0.0599\n",
      "Epoch 99/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0218 - val_loss: 0.0593\n",
      "Epoch 100/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0217 - val_loss: 0.0598\n",
      "Params:  time_steps: 8  latent_dim: 8  batch_size:  20  n_epoch:   Optimizer:  adadelta 100  layers  [32, 128] \n",
      " Time cost:  2417.532465696335\n",
      "Train on 2380 samples, validate on 280 samples\n",
      "Epoch 1/100\n",
      "2380/2380 [==============================] - 10s - loss: 0.0462 - val_loss: 0.0668\n",
      "Epoch 2/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0551 - val_loss: 0.0607\n",
      "Epoch 3/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0538 - val_loss: 0.0623\n",
      "Epoch 4/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0536 - val_loss: 0.0621\n",
      "Epoch 5/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0538 - val_loss: 0.0657\n",
      "Epoch 6/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0527 - val_loss: 0.0655\n",
      "Epoch 7/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0543 - val_loss: 0.0672\n",
      "Epoch 8/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0540 - val_loss: 0.0673\n",
      "Epoch 9/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0540 - val_loss: 0.0682\n",
      "Epoch 10/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0540 - val_loss: 0.0687\n",
      "Epoch 11/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0538 - val_loss: 0.0689\n",
      "Epoch 12/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0536 - val_loss: 0.0678\n",
      "Epoch 13/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0529 - val_loss: 0.0688\n",
      "Epoch 14/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0473 - val_loss: 0.0673\n",
      "Epoch 15/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0487 - val_loss: 0.0704\n",
      "Epoch 16/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0531 - val_loss: 0.0716\n",
      "Epoch 17/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0530 - val_loss: 0.0718\n",
      "Epoch 18/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0533 - val_loss: 0.0728\n",
      "Epoch 19/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0537 - val_loss: 0.0727\n",
      "Epoch 20/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0536 - val_loss: 0.0729\n",
      "Epoch 21/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0535 - val_loss: 0.0732\n",
      "Epoch 22/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0535 - val_loss: 0.0734\n",
      "Epoch 23/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0534 - val_loss: 0.0736\n",
      "Epoch 24/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0534 - val_loss: 0.0743\n",
      "Epoch 25/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0532 - val_loss: 0.0744\n",
      "Epoch 26/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0532 - val_loss: 0.0746\n",
      "Epoch 27/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0530 - val_loss: 0.0744\n",
      "Epoch 28/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0538 - val_loss: 0.0745\n",
      "Epoch 29/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0531 - val_loss: 0.0740\n",
      "Epoch 30/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0524 - val_loss: 0.0750\n",
      "Epoch 31/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0523 - val_loss: 0.0760\n",
      "Epoch 32/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0523 - val_loss: 0.0762\n",
      "Epoch 33/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0524 - val_loss: 0.0762\n",
      "Epoch 34/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0522 - val_loss: 0.0765\n",
      "Epoch 35/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0523 - val_loss: 0.0768\n",
      "Epoch 36/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0517 - val_loss: 0.0768\n",
      "Epoch 37/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0521 - val_loss: 0.0772\n",
      "Epoch 38/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0516 - val_loss: 0.0771\n",
      "Epoch 39/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0517 - val_loss: 0.0773\n",
      "Epoch 40/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0509 - val_loss: 0.0727\n",
      "Epoch 41/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0502 - val_loss: 0.0777\n",
      "Epoch 42/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0493 - val_loss: 0.0762\n",
      "Epoch 43/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0451 - val_loss: 0.0747\n",
      "Epoch 44/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0498 - val_loss: 0.0751\n",
      "Epoch 45/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0511 - val_loss: 0.0770\n",
      "Epoch 46/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0516 - val_loss: 0.0773\n",
      "Epoch 47/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0516 - val_loss: 0.0773\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2380/2380 [==============================] - 8s - loss: 0.0515 - val_loss: 0.0772\n",
      "Epoch 49/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0514 - val_loss: 0.0772\n",
      "Epoch 50/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0514 - val_loss: 0.0774\n",
      "Epoch 51/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0513 - val_loss: 0.0773\n",
      "Epoch 52/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0517 - val_loss: 0.0784\n",
      "Epoch 53/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0514 - val_loss: 0.0783\n",
      "Epoch 54/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0512 - val_loss: 0.0777\n",
      "Epoch 55/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0515 - val_loss: 0.0784\n",
      "Epoch 56/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0513 - val_loss: 0.0782\n",
      "Epoch 57/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0514 - val_loss: 0.0783\n",
      "Epoch 58/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0513 - val_loss: 0.0782\n",
      "Epoch 59/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0512 - val_loss: 0.0779\n",
      "Epoch 60/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0514 - val_loss: 0.0785\n",
      "Epoch 61/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0512 - val_loss: 0.0784\n",
      "Epoch 62/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0512 - val_loss: 0.0782\n",
      "Epoch 63/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0512 - val_loss: 0.0780\n",
      "Epoch 64/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0514 - val_loss: 0.0791\n",
      "Epoch 65/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0511 - val_loss: 0.0789\n",
      "Epoch 66/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0511 - val_loss: 0.0785\n",
      "Epoch 67/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0511 - val_loss: 0.0778\n",
      "Epoch 68/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0514 - val_loss: 0.0798\n",
      "Epoch 69/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0510 - val_loss: 0.0794\n",
      "Epoch 70/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0510 - val_loss: 0.0793\n",
      "Epoch 71/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0508 - val_loss: 0.0775\n",
      "Epoch 72/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0514 - val_loss: 0.0800\n",
      "Epoch 73/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0509 - val_loss: 0.0800\n",
      "Epoch 74/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0509 - val_loss: 0.0801\n",
      "Epoch 75/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0508 - val_loss: 0.0797\n",
      "Epoch 76/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0509 - val_loss: 0.0800\n",
      "Epoch 77/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0509 - val_loss: 0.0794\n",
      "Epoch 78/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0510 - val_loss: 0.0810\n",
      "Epoch 79/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0508 - val_loss: 0.0806\n",
      "Epoch 80/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0508 - val_loss: 0.0806\n",
      "Epoch 81/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0507 - val_loss: 0.0794\n",
      "Epoch 82/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0509 - val_loss: 0.0816\n",
      "Epoch 83/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0507 - val_loss: 0.0810ss: \n",
      "Epoch 84/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0507 - val_loss: 0.0800\n",
      "Epoch 85/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0509 - val_loss: 0.0817\n",
      "Epoch 86/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0507 - val_loss: 0.0810\n",
      "Epoch 87/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0508 - val_loss: 0.0812\n",
      "Epoch 88/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0508 - val_loss: 0.0807\n",
      "Epoch 89/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0509 - val_loss: 0.0820\n",
      "Epoch 90/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0508 - val_loss: 0.0813\n",
      "Epoch 91/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0508 - val_loss: 0.0813\n",
      "Epoch 92/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0508 - val_loss: 0.0821\n",
      "Epoch 93/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0506 - val_loss: 0.0814\n",
      "Epoch 94/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0507 - val_loss: 0.0820\n",
      "Epoch 95/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0507 - val_loss: 0.0813\n",
      "Epoch 96/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0508 - val_loss: 0.0818\n",
      "Epoch 97/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0507 - val_loss: 0.0811\n",
      "Epoch 98/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0506 - val_loss: 0.0813\n",
      "Epoch 99/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0506 - val_loss: 0.0810\n",
      "Epoch 100/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0506 - val_loss: 0.0813\n",
      "Params:  time_steps: 8  latent_dim: 8  batch_size:  20  n_epoch:   Optimizer:  adam 100  layers  [32, 128] \n",
      " Time cost:  3302.4126880168915\n",
      "Train on 2380 samples, validate on 280 samples\n",
      "Epoch 1/100\n",
      "2380/2380 [==============================] - 10s - loss: 0.0411 - val_loss: 0.0674\n",
      "Epoch 2/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0410 - val_loss: 0.0644\n",
      "Epoch 3/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0400 - val_loss: 0.0580\n",
      "Epoch 4/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0346 - val_loss: 0.0541\n",
      "Epoch 5/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0308 - val_loss: 0.0523\n",
      "Epoch 6/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0291 - val_loss: 0.0512\n",
      "Epoch 7/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0278 - val_loss: 0.0501\n",
      "Epoch 8/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0263 - val_loss: 0.0495\n",
      "Epoch 9/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0250 - val_loss: 0.0511\n",
      "Epoch 10/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0237 - val_loss: 0.0489\n",
      "Epoch 11/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0240 - val_loss: 0.0487\n",
      "Epoch 12/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0227 - val_loss: 0.0484\n",
      "Epoch 13/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0229 - val_loss: 0.0479\n",
      "Epoch 14/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0225 - val_loss: 0.0478\n",
      "Epoch 15/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0214 - val_loss: 0.0476\n",
      "Epoch 16/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0208 - val_loss: 0.0469\n",
      "Epoch 17/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0203 - val_loss: 0.0463\n",
      "Epoch 18/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0198 - val_loss: 0.0454\n",
      "Epoch 19/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0193 - val_loss: 0.0455\n",
      "Epoch 20/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0189 - val_loss: 0.0450\n",
      "Epoch 21/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0189 - val_loss: 0.0449\n",
      "Epoch 22/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0182 - val_loss: 0.0440\n",
      "Epoch 23/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0181 - val_loss: 0.0450\n",
      "Epoch 24/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0173 - val_loss: 0.0438\n",
      "Epoch 25/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0177 - val_loss: 0.0450\n",
      "Epoch 26/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0170 - val_loss: 0.0443\n",
      "Epoch 27/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0166 - val_loss: 0.0449\n",
      "Epoch 28/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0163 - val_loss: 0.0452\n",
      "Epoch 29/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0163 - val_loss: 0.0443\n",
      "Epoch 30/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0161 - val_loss: 0.0456\n",
      "Epoch 31/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0161 - val_loss: 0.0480\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2380/2380 [==============================] - 8s - loss: 0.0159 - val_loss: 0.0455\n",
      "Epoch 33/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0156 - val_loss: 0.0468\n",
      "Epoch 34/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0159 - val_loss: 0.0484\n",
      "Epoch 35/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0152 - val_loss: 0.0450\n",
      "Epoch 36/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0152 - val_loss: 0.0516\n",
      "Epoch 37/100\n",
      "2380/2380 [==============================] - 9s - loss: 0.0148 - val_loss: 0.0469\n",
      "Epoch 38/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0148 - val_loss: 0.0473\n",
      "Epoch 39/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0147 - val_loss: 0.0522\n",
      "Epoch 40/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0149 - val_loss: 0.0481\n",
      "Epoch 41/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0145 - val_loss: 0.0527\n",
      "Epoch 42/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0143 - val_loss: 0.0510\n",
      "Epoch 43/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0149 - val_loss: 0.0553\n",
      "Epoch 44/100\n",
      "2380/2380 [==============================] - 9s - loss: 0.0142 - val_loss: 0.0545\n",
      "Epoch 45/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0140 - val_loss: 0.0538\n",
      "Epoch 46/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0142 - val_loss: 0.0581\n",
      "Epoch 47/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0140 - val_loss: 0.0491\n",
      "Epoch 48/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0137 - val_loss: 0.0532\n",
      "Epoch 49/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0137 - val_loss: 0.0504\n",
      "Epoch 50/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0139 - val_loss: 0.0568\n",
      "Epoch 51/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0138 - val_loss: 0.0558\n",
      "Epoch 52/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0135 - val_loss: 0.0509\n",
      "Epoch 53/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0135 - val_loss: 0.0585\n",
      "Epoch 54/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0134 - val_loss: 0.0501\n",
      "Epoch 55/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0132 - val_loss: 0.0561\n",
      "Epoch 56/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0133 - val_loss: 0.0532\n",
      "Epoch 57/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0132 - val_loss: 0.0582\n",
      "Epoch 58/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0131 - val_loss: 0.0518\n",
      "Epoch 59/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0131 - val_loss: 0.0551\n",
      "Epoch 60/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0130 - val_loss: 0.0567\n",
      "Epoch 61/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0132 - val_loss: 0.0618\n",
      "Epoch 62/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0130 - val_loss: 0.0652\n",
      "Epoch 63/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0129 - val_loss: 0.0596\n",
      "Epoch 64/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0128 - val_loss: 0.0540\n",
      "Epoch 65/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0129 - val_loss: 0.0547\n",
      "Epoch 66/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0128 - val_loss: 0.0582\n",
      "Epoch 67/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0128 - val_loss: 0.0569\n",
      "Epoch 68/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0128 - val_loss: 0.0580\n",
      "Epoch 69/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0126 - val_loss: 0.0713\n",
      "Epoch 70/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0126 - val_loss: 0.0618\n",
      "Epoch 71/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0127 - val_loss: 0.0700\n",
      "Epoch 72/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0125 - val_loss: 0.0642\n",
      "Epoch 73/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0125 - val_loss: 0.0605\n",
      "Epoch 74/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0125 - val_loss: 0.0558\n",
      "Epoch 75/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0124 - val_loss: 0.0531\n",
      "Epoch 76/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0124 - val_loss: 0.0600\n",
      "Epoch 77/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0124 - val_loss: 0.0643\n",
      "Epoch 78/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0124 - val_loss: 0.0599\n",
      "Epoch 79/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0124 - val_loss: 0.0628\n",
      "Epoch 80/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0123 - val_loss: 0.0609\n",
      "Epoch 81/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0123 - val_loss: 0.0585\n",
      "Epoch 82/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0122 - val_loss: 0.0495\n",
      "Epoch 83/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0122 - val_loss: 0.0609\n",
      "Epoch 84/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0122 - val_loss: 0.0572\n",
      "Epoch 85/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0122 - val_loss: 0.0657\n",
      "Epoch 86/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0121 - val_loss: 0.0619\n",
      "Epoch 87/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0121 - val_loss: 0.0516\n",
      "Epoch 88/100\n",
      "2380/2380 [==============================] - 8s - loss: 0.0121 - val_loss: 0.0518\n",
      "Epoch 89/100\n",
      "2380/2380 [==============================] - 4s - loss: 0.0120 - val_loss: 0.0495\n",
      "Epoch 90/100\n",
      "2380/2380 [==============================] - 4s - loss: 0.0120 - val_loss: 0.0487\n",
      "Epoch 91/100\n",
      "2380/2380 [==============================] - 4s - loss: 0.0120 - val_loss: 0.0520\n",
      "Epoch 92/100\n",
      "2380/2380 [==============================] - 4s - loss: 0.0119 - val_loss: 0.0512\n",
      "Epoch 93/100\n",
      "2380/2380 [==============================] - 4s - loss: 0.0120 - val_loss: 0.0490\n",
      "Epoch 94/100\n",
      "2380/2380 [==============================] - 4s - loss: 0.0120 - val_loss: 0.0457\n",
      "Epoch 95/100\n",
      "2380/2380 [==============================] - 4s - loss: 0.0119 - val_loss: 0.0496\n",
      "Epoch 96/100\n",
      "2380/2380 [==============================] - 4s - loss: 0.0119 - val_loss: 0.0501\n",
      "Epoch 97/100\n",
      "2380/2380 [==============================] - 4s - loss: 0.0119 - val_loss: 0.0512\n",
      "Epoch 98/100\n",
      "2380/2380 [==============================] - 4s - loss: 0.0118 - val_loss: 0.0475\n",
      "Epoch 99/100\n",
      "2380/2380 [==============================] - 4s - loss: 0.0118 - val_loss: 0.0495\n",
      "Epoch 100/100\n",
      "2380/2380 [==============================] - 4s - loss: 0.0118 - val_loss: 0.0547\n",
      "Params:  time_steps: 8  latent_dim: 8  batch_size:  20  n_epoch:   Optimizer:  rmsprop 100  layers  [32, 128] \n",
      " Time cost:  4135.6606023311615\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa714b4c550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Construct an AutoEncoder for sequence data based on LSTM\n",
    "'''\n",
    "from keras.layers import Input, LSTM, RepeatVector\n",
    "from keras.models import Model\n",
    "from keras import metrics\n",
    "from sklearn import preprocessing\n",
    "'''\n",
    "Preparing data\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "'''\n",
    "Reading Macro economy factors from CSV\n",
    "Notice: data are distributed monthly, need to be filled for daily usage\n",
    "'''\n",
    "def strip_comma(x):\n",
    "    return float(str(x).replace(',',''))\n",
    "def read_macro_economy(base_dir = '../data/macro_economy/', \n",
    "                       filename = 'china_macro_economy_daily.csv',\n",
    "                       start_date = '2002-01-04', end_date = '2017-11-30',\n",
    "                       names = [i for i in range(31)],\n",
    "                       usecols = None):\n",
    "    filename = base_dir +'/'+ filename\n",
    "    print (filename) #中文读取出问题，所以skip row1\n",
    "    df = pd.read_csv(filename, index_col=0, sep=',', \n",
    "                     skiprows=1, usecols=usecols,\n",
    "                     names = names, parse_dates=True,\n",
    "                     converters = {11: strip_comma, 22: strip_comma}\n",
    "                    )\n",
    "    return df[start_date : end_date]\n",
    "\n",
    "\n",
    "'''\n",
    "Reading World economy factors sponsored by OECD from CSV\n",
    "Notice: data are distributed monthly, need to be filled for daily usage\n",
    "'''\n",
    "def read_world_economy(base_dir = '../data/macro_economy/', \n",
    "                       filename = 'OECD-world-economy-daily.csv',\n",
    "                       start_date = '2002-01-04', end_date = '2017-11-30',\n",
    "                       names = [i for i in range(46)],\n",
    "                       usecols = None):\n",
    "    filename = base_dir +'/'+ filename\n",
    "    print (filename) \n",
    "    df = pd.read_csv(filename, index_col=0, \n",
    "                     skiprows=1, usecols=usecols,parse_dates=True,\n",
    "                     names = names\n",
    "                    )\n",
    "    return df[start_date : end_date]\n",
    "\n",
    "'''\n",
    "Reading Top10 Components CSV\n",
    "Data has been assigned weight according to their ratio in the market\n",
    "\n",
    "# Ref: data calculated from data/generate/FetchingComponentsData.ipynb\n",
    "'''\n",
    "def read_components(base_dir = '../data/components/', \n",
    "                       filename = 'components-top10.csv',\n",
    "                       start_date = '2002-01-04', end_date = '2017-11-30',\n",
    "                       names = [i for i in range(10)]):\n",
    "    filename = base_dir +'/'+ filename\n",
    "    print (filename) \n",
    "    df = pd.read_csv(filename, index_col=0, \n",
    "                     skiprows=1,parse_dates=True,\n",
    "                     names = names\n",
    "                    )\n",
    "    df = df.fillna(0)\n",
    "    return df[start_date : end_date]\n",
    "\n",
    "'''\n",
    "Reading ohlcv transaction data for a stock\n",
    "\n",
    "'''\n",
    "def readWSDFile(baseDir, stockCode, startDate='2005-01-04', endDate= '2015-12-31', usecols=None, \n",
    "                names=['date','pre_close','open','high','low','close','change','chg_range',\n",
    "                                               'volume','amount','turn']):\n",
    "    # 解析日期\n",
    "    filename = baseDir+stockCode+'/'+stockCode+'.csv'\n",
    "    print (filename, \"===============\")\n",
    "    dateparse = lambda x: pd.datetime.strptime(x, '%Y/%m/%d').date()\n",
    "    df = pd.read_csv(filename, index_col=0, sep=',', header=None,usecols=usecols,\n",
    "                            skiprows=1, names=names,\n",
    "                           parse_dates=True, date_parser=dateparse)\n",
    "    df = df.fillna(0)\n",
    "    return df[startDate : endDate]\n",
    "\n",
    "'''\n",
    "Reading Technical indicators of a stock\n",
    "'''\n",
    "def readWSDIndexFile(baseDir, stockCode, startYear, yearNum=1):\n",
    "    # parse date\n",
    "    dateparse = lambda x: pd.datetime.strptime(x, '%Y/%m/%d').date()\n",
    "\n",
    "    df = 0\n",
    "    for i in range(yearNum):\n",
    "        tempDF = pd.read_csv(baseDir+'I'+stockCode+'/wsd_'+stockCode+'_'+str(startYear+i)+'.csv', index_col=0, sep=',', parse_dates=True, date_parser=dateparse\n",
    "                             # , usecols=usecols\n",
    "                             )\n",
    "        if i==0: df = tempDF\n",
    "        else: df = df.append(tempDF)\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "'''\n",
    "Preparing all the useful data\n",
    "'''\n",
    "# 整理好多因子输入，以dataframe返回数据+标签\n",
    "def data_prepare(retrace = 0.618, dtype = 'all', start_date='2005-01-04', end_date= '2015-12-31'):\n",
    "    # prepare data\n",
    "    baseDir = '../data/'\n",
    "    stockCodes = ['000300.SH']\n",
    "    i = 0\n",
    "    startYear = 2005\n",
    "    number =11\n",
    "    print(\"------------------------ Let's get the party started! -------------------------------------\")\n",
    "    ## Load data from CSV\n",
    "    dfm = read_macro_economy(start_date = start_date, end_date = end_date)\n",
    "    dfw = read_world_economy(start_date = start_date, end_date = end_date)\n",
    "    dfc = read_components(start_date = start_date, end_date = end_date)\n",
    "    df = readWSDFile(baseDir, stockCodes[i], start_date, end_date)\n",
    "    dfi = readWSDIndexFile(baseDir, stockCodes[i], startYear, number)\n",
    "    allDF = df\n",
    "    if dtype == 'all':\n",
    "        allDF = pd.concat([df, dfi, dfm, dfw, dfc], axis=1)\n",
    "    elif dtype == 'only_m':\n",
    "        allDF = pd.concat([df, dfm], axis=1) # macro economy \n",
    "    elif dtype == 'only_i':\n",
    "        allDF = pd.concat([df, dfi], axis=1) # technical indicators\n",
    "    elif dtype == 'only_w':\n",
    "        allDF = pd.concat([df, dfw], axis=1) # world economy\n",
    "    elif dtype == 'only_c':\n",
    "        allDF = pd.concat([df, dfc], axis=1) # constitutant stock\n",
    "    elif dtype == 'im':\n",
    "        allDF = pd.concat([df, dfi, dfm], axis=1) \n",
    "    elif dtype == 'ic':\n",
    "        allDF = pd.concat([df, dfi, dfc], axis=1) \n",
    "    else:\n",
    "        pass\n",
    "    allDF = allDF.fillna(0)\n",
    "    sample_num = np.shape(df)[0]\n",
    "    labelDF = pd.Series(np.zeros(sample_num))\n",
    "    print (\"Factors Shape:\", np.shape(allDF))\n",
    "    \n",
    "    # 求出 trend\n",
    "    price = df['close']\n",
    "    start = 0\n",
    "    while price[start] > price[start+1]:\n",
    "        labelDF[start] = 1 #flat\n",
    "        start +=1\n",
    "    \n",
    "    #find peak, find trough, calculate retracement and label trend accordingly\n",
    "    i = start\n",
    "    while i < sample_num - 1:\n",
    "        cursor = i\n",
    "        while cursor < sample_num - 1 and price[cursor] <= price[cursor+1]:\n",
    "            cursor += 1\n",
    "        peak = cursor\n",
    "        while cursor < sample_num - 1 and price[cursor] >= price[cursor+1]:\n",
    "            cursor += 1\n",
    "        trough = cursor\n",
    "        retracement = (price[peak] - price[trough]) / (price[peak] - price[i])\n",
    "        mark = 1 # flat\n",
    "        if retracement < retrace:\n",
    "            mark = 2 # UP\n",
    "        elif retracement > 1 + retrace:\n",
    "            mark = 0 # DOWN\n",
    "        for k in range(i, cursor+1):\n",
    "            labelDF[k] = mark\n",
    "        i = cursor\n",
    "\n",
    "    print(\"---- Trend Distribution Check --------\")\n",
    "    print(labelDF.value_counts().sort_index())\n",
    "    \n",
    "    # make a deep copy of Price Difference before normalizing\n",
    "    priceDF = allDF['change'].copy(deep=True)\n",
    "    # normalize(x)\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    input_data = scaler.fit_transform(allDF)\n",
    "    print (\"input data shape: \", np.shape(input_data)) #  days *  factors\n",
    "    print (\"input label shape: \", np.shape(labelDF))\n",
    "    labels = labelDF.values\n",
    "    #input_data = np.concatenate((input_data, labels.reshape(-1,1)), axis = 1) # historical data as part of the series\n",
    "    return input_data, labels, priceDF, price # train/test data, labels, prices difference, actual price for yield calucluation\n",
    "\n",
    "###### Hyper paramters #########\n",
    "time_steps = 8\n",
    "batch_size = 20 # specify batch size explicitly; no shuffle but successive sequence\n",
    "n_epoch = 100\n",
    "train_ratio = 0.9\n",
    "concate = True\n",
    "t0 = time()\n",
    "dataset,labels, priceDF, price = data_prepare()\n",
    "segment_num = (len(dataset) - time_steps - 1) // batch_size # rollingly use data\n",
    "train_size = int(segment_num * train_ratio)\n",
    "test_size = segment_num - train_size\n",
    "data_dim = np.shape(dataset)[1] #input + historic labels\n",
    "\n",
    "#divide training/validation dataset; numpy array\n",
    "train_x = dataset[0 : train_size * batch_size + time_steps]\n",
    "test_x = dataset[train_size * batch_size : (train_size + test_size) * batch_size + time_steps]\n",
    "\n",
    "#historic label as input time series\n",
    "train_historic_label = np.array(labels[0 : train_size * batch_size + time_steps]).reshape(-1,1)\n",
    "test_historic_label = np.array(labels[train_size * batch_size : (train_size + test_size) * batch_size + time_steps]).reshape(-1,1)\n",
    "\n",
    "#Sliding window: label is just 1 step further after sequence data\n",
    "train_y = labels[time_steps : train_size * batch_size + time_steps]\n",
    "test_y = labels[train_size * batch_size + time_steps: (train_size + test_size) * batch_size + time_steps]\n",
    "\n",
    "# add historical trends or not\n",
    "if concate:\n",
    "    data_dim += 1\n",
    "    train_x = np.concatenate((train_x, train_historic_label), axis = 1)\n",
    "    test_x = np.concatenate((test_x, test_historic_label), axis = 1)\n",
    "\n",
    "train_sample = len(train_x) - time_steps\n",
    "b = np.array([[]])\n",
    "# creating data in a rolling window view \n",
    "for i in range(train_sample):\n",
    "    b = np.append(b, train_x[i : time_steps + i])\n",
    "train_x = b.reshape(train_sample, time_steps, data_dim)\n",
    "print(\"training size: \", train_sample)\n",
    "\n",
    "test_sample = len(test_x) - time_steps\n",
    "b = np.array([[]])\n",
    "for i in range(test_sample):\n",
    "    b = np.append(b, test_x[i : time_steps + i])\n",
    "test_x = b.reshape(test_sample, time_steps, data_dim)\n",
    "print(\"testing size: \", test_sample)\n",
    "\n",
    "train_y = np.array(train_y, dtype=np.int32)\n",
    "test_y = np.array(test_y, dtype=np.int32)\n",
    "\n",
    "############## AutoEncoder MODEL ##########################\n",
    "latent_dim = 8\n",
    "layer1 = 128\n",
    "layer2 = 32\n",
    "layers = [layer2,layer1]\n",
    "inputs = Input(shape=(time_steps, data_dim))\n",
    "encoded = LSTM(units = layer1, return_sequences = True)(inputs)\n",
    "encoded = LSTM(units = layer2, return_sequences = True)(encoded)\n",
    "encoded = LSTM(units = latent_dim, return_sequences = False)(encoded) # most hidden layer, only preseve the last step's output\n",
    "\n",
    "repeated_out = RepeatVector(time_steps)(encoded)  # repeat intermediate output [2D -> 3D]\n",
    "\n",
    "decoded = LSTM(layer2, return_sequences=True)(repeated_out)\n",
    "decoded = LSTM(layer1, return_sequences=True)(decoded)\n",
    "decoded = LSTM(data_dim, return_sequences=True)(decoded)  # output layer as a comparison\n",
    "\n",
    "sequence_autoencoder = Model(inputs, decoded)\n",
    "encoder = Model(inputs, encoded)  ## encoder for dimensionality reduction\n",
    "\n",
    "for opt in ['sgd', 'adadelta', 'adam', 'rmsprop']:\n",
    "    sequence_autoencoder.compile(optimizer=opt, \n",
    "                                 loss='mean_squared_error',\n",
    "                                 )\n",
    "    history = sequence_autoencoder.fit(x = train_x, y = train_x, \n",
    "                             batch_size=batch_size,\n",
    "                             epochs=n_epoch,\n",
    "                             shuffle=False,\n",
    "                             validation_data = (test_x, test_x))\n",
    "    print (\"Params: \", \"time_steps:\", time_steps, \" latent_dim:\", \n",
    "           latent_dim, \" batch_size: \",batch_size, \" n_epoch: \", ' Optimizer: ', opt,\n",
    "           n_epoch, ' layers ', layers, \"\\n Time cost: \", (time() - t0))\n",
    "\n",
    "# encoder.save('autoencoder-49d-8ts-100ep-24-12-8.h5')\n",
    "# compressed_data = encoder.predict(train_x)\n",
    "\n",
    "# # plot history\n",
    "# plt.plot(history.history['mean_squared_error'], label='train')\n",
    "# plt.plot(history.history['val_mean_squared_error'], label='test')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# # loss\n",
    "plt.plot(history.history['loss'], label='val_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ## drawing compressed data\n",
    "# plt.scatter(compressed_data[:, 0], compressed_data[:, 1],marker='.')\n",
    "# plt.show()\n",
    "\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# ax.scatter(compressed_data[:, 0], compressed_data[:, 1], compressed_data[:,2],marker='o', c= 'r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
